#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹ (ST-GPR)

æœ¬æ¨¡å—å®ç°äº†åŸºäºPyTorchå’ŒGPyTorchçš„æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹ï¼Œç”¨äºæ¤è¢«å¥åº·æŒ‡æ•°å»ºæ¨¡ã€‚
æ¨¡å‹ç»“æ„å°†ç©ºé—´ç»´åº¦ã€æ—¶é—´ç»´åº¦å’Œç‰¹å¾ç»´åº¦æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ ¸å‡½æ•°ç»“æ„ä¸­ï¼š
(SpatialSimilarityKernel + MaternKernel) * RBFKernel(æ—¶é—´)

ç‰¹ç‚¹:
- SpatialSimilarityKernel: è‡ªå®šä¹‰æ ¸å‡½æ•°ï¼Œåˆ©ç”¨ç‰¹å¾ç›¸ä¼¼æ€§è¿›è¡Œé¢„æµ‹
- MaternKernel: å¤„ç†ç©ºé—´ç›¸å…³æ€§ï¼ˆç»çº¬åº¦ï¼‰
- RBFKernel: å¤„ç†æ—¶é—´ç›¸å…³æ€§ï¼ˆå¹´ä»½ï¼‰
- ç¨€ç–å˜åˆ†é«˜æ–¯è¿‡ç¨‹: é€šè¿‡inducing pointsæé«˜å¤§æ•°æ®é›†çš„è®¡ç®—æ•ˆç‡
"""

# è®¾ç½®OMP_NUM_THREADSç¯å¢ƒå˜é‡ï¼Œé¿å…Windowsä¸ŠKMeanså†…å­˜æ³„æ¼é—®é¢˜
# è¿™å¿…é¡»åœ¨å¯¼å…¥sklearnä¹‹å‰è¿›è¡Œè®¾ç½®
import os
import sys
import platform
if platform.system() == 'Windows' and 'OMP_NUM_THREADS' not in os.environ:
    os.environ['OMP_NUM_THREADS'] = '1'
    print(f"åœ¨stgpr.pyä¸­è®¾ç½®OMP_NUM_THREADS=1ï¼Œé¿å…Windowsä¸ŠKMeanså†…å­˜æ³„æ¼é—®é¢˜")

import time
import traceback
import numpy as np
import pandas as pd
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import StratifiedShuffleSplit

# æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦å¯ç”¨
HAS_HYPEROPT = False
HAS_GPYTORCH = False
HAS_GEOSHAPLEY = False

# æ·»åŠ hyperoptç›¸å…³å¯¼å…¥
try:
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
    HAS_HYPEROPT = True
except ImportError:
    pass

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from .stgpr_config import get_config, RANDOM_SEED
    CONFIG = get_config()
except ImportError:
    RANDOM_SEED = 42
    CONFIG = {
        'model': {'num_inducing_points': 500, 'batch_size': 200, 'num_iterations': 1000, 'use_lbfgs': True},
        'optimizer': {'adam_learning_rate': 0.01, 'gradient_clip_norm': 10.0},
        'kernel': {'lengthscale_lower_bound': 1e-3, 'variance_lower_bound': 1e-5}
    }

# æ£€æŸ¥PyTorchå’ŒGPyTorchä¾èµ–
try:
    import torch
    import gpytorch
    from gpytorch.models import ApproximateGP
    from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy
    from gpytorch.means import ConstantMean
    from gpytorch.kernels import RBFKernel, MaternKernel, ScaleKernel
    HAS_GPYTORCH = True
except ImportError:
    pass

# æ£€æŸ¥GeoShapleyåº“
try:
    from geoshapley import GeoShapleyExplainer
    HAS_GEOSHAPLEY = True
except ImportError:
    pass

# ä»coreæ¨¡å—å¯¼å…¥ensure_dir_existså‡½æ•°
from .core import ensure_dir_exists

# ä»stgpr_modelæ¨¡å—å¯¼å…¥STGPRModelå’ŒSpatialSimilarityKernel
from .stgpr_model import STGPRModel, SpatialSimilarityKernel

def select_inducing_points_spatiotemporal(X_train, n_inducing_points, h3_col='h3_index', year_col='year', random_state=42, feature_columns=None, use_kmeans_fallback=True, return_indices=False):
    """
    é€‰æ‹©è¯±å¯¼ç‚¹ï¼Œé»˜è®¤ä½¿ç”¨KMeansèšç±»ï¼Œå¯é€‰æ—¶ç©ºåˆ†å±‚é‡‡æ ·
    
    å‚æ•°:
    X_train: è®­ç»ƒæ•°æ®ï¼ŒåŒ…å«ç‰¹å¾å’Œå¯èƒ½çš„h3_indexã€yearåˆ—
    n_inducing_points: éœ€è¦çš„è¯±å¯¼ç‚¹æ•°é‡
    h3_col: H3ç½‘æ ¼ç´¢å¼•åˆ—å
    year_col: å¹´ä»½åˆ—å
    random_state: éšæœºç§å­
    feature_columns: è¦è¿”å›çš„ç‰¹å¾åˆ—åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨ç¡®å®šï¼‰
    use_kmeans_fallback: æ˜¯å¦ä½¿ç”¨KMeansï¼ˆé»˜è®¤Trueï¼‰ã€‚å¦‚æœä¸ºFalseï¼Œåˆ™å°è¯•æ—¶ç©ºåˆ†å±‚é‡‡æ ·
    return_indices: æ˜¯å¦è¿”å›ç´¢å¼•è€Œä¸æ˜¯æ•°æ®
    
    è¿”å›:
    inducing_points: é€‰ä¸­çš„è¯±å¯¼ç‚¹æ•°ç»„
    """
    np.random.seed(random_state)
    
    # å†³å®šæ˜¯å¦ä½¿ç”¨æ—¶ç©ºåˆ†å±‚é‡‡æ ·
    # é»˜è®¤ä½¿ç”¨KMeansï¼Œå› ä¸ºå®ƒåœ¨ç‰¹å¾ç©ºé—´ä¸­é€‰æ‹©æ›´æœ‰ä»£è¡¨æ€§çš„ç‚¹
    use_spatiotemporal = False
    if not use_kmeans_fallback:
        # åªæœ‰åœ¨æ˜ç¡®ä¸ä½¿ç”¨KMeanså›é€€æ—¶æ‰ä½¿ç”¨æ—¶ç©ºåˆ†å±‚
        use_spatiotemporal = True
        print(f"  å°è¯•æ—¶ç©ºåˆ†å±‚é‡‡æ ·é€‰æ‹©è¯±å¯¼ç‚¹...")
    else:
        print(f"  ä½¿ç”¨KMeansèšç±»é€‰æ‹©è¯±å¯¼ç‚¹ï¼ˆåœ¨ç‰¹å¾ç©ºé—´ä¸­é€‰æ‹©ä»£è¡¨æ€§ç‚¹ï¼‰")
    
    # å¦‚æœæ˜¯DataFrameä¸”æœ‰æ—¶ç©ºä¿¡æ¯ï¼Œå°è¯•ä½¿ç”¨æ—¶ç©ºåˆ†å±‚
    if use_spatiotemporal and isinstance(X_train, pd.DataFrame) and h3_col in X_train.columns and year_col in X_train.columns:
        print(f"  ä½¿ç”¨æ—¶ç©ºåˆ†å±‚é‡‡æ ·é€‰æ‹©{n_inducing_points}ä¸ªè¯±å¯¼ç‚¹...")
        
        # è·å–å”¯ä¸€çš„ç©ºé—´å’Œæ—¶é—´å€¼
        unique_h3 = X_train[h3_col].unique()
        unique_years = X_train[year_col].unique()
        
        n_h3 = len(unique_h3)
        n_years = len(unique_years)
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦åº”è¯¥é€‰æ‹©å¤šå°‘ä¸ªå€¼
        # æ”¹è¿›çš„ç­–ç•¥ï¼šç¡®ä¿æ›´å¥½çš„è¦†ç›–ç‡
        # 1. å¯¹äºæ—¶é—´ç»´åº¦ï¼Œè‡³å°‘è¦†ç›–50%çš„å¹´ä»½
        min_years = max(n_years // 2, 5)  # è‡³å°‘5å¹´æˆ–ä¸€åŠå¹´ä»½
        # 2. å¯¹äºç©ºé—´ç»´åº¦ï¼Œæ ¹æ®è¯±å¯¼ç‚¹æ•°é‡åŠ¨æ€è°ƒæ•´
        
        if n_inducing_points >= n_h3 * n_years * 0.5:
            # å¦‚æœè¯±å¯¼ç‚¹å¾ˆå¤šï¼Œä½¿ç”¨å¤§éƒ¨åˆ†ç½‘æ ¼å’Œå¹´ä»½
            n_h3_select = min(int(n_h3 * 0.8), n_h3)
            n_years_select = min(int(n_years * 0.8), n_years)
        elif n_inducing_points >= 1000:
            # ä¸­ç­‰æ•°é‡çš„è¯±å¯¼ç‚¹
            n_h3_select = min(int(np.sqrt(n_inducing_points / min_years)), n_h3)
            n_years_select = min(max(min_years, int(n_inducing_points / n_h3_select)), n_years)
        else:
            # è¾ƒå°‘çš„è¯±å¯¼ç‚¹ï¼Œä¼˜å…ˆä¿è¯æ—¶é—´è¦†ç›–
            n_years_select = min(min_years, n_years)
            n_h3_select = min(max(int(n_inducing_points / n_years_select), 10), n_h3)
        
        # ç¡®ä¿åˆç†çš„èŒƒå›´
        n_h3_select = max(min(n_h3_select, n_h3), min(10, n_h3))
        n_years_select = max(min(n_years_select, n_years), min(5, n_years))
        
        # éšæœºé€‰æ‹©H3ç½‘æ ¼å’Œå¹´ä»½
        selected_h3 = np.random.choice(unique_h3, size=n_h3_select, replace=False)
        selected_years = np.random.choice(unique_years, size=n_years_select, replace=False)
        
        # åˆ›å»ºç½‘æ ¼ç»„åˆ
        inducing_indices = []
        for h3 in selected_h3:
            for year in selected_years:
                mask = (X_train[h3_col] == h3) & (X_train[year_col] == year)
                indices = X_train.index[mask].tolist()
                if indices:
                    # ä»æ¯ä¸ªç½‘æ ¼-å¹´ä»½ç»„åˆä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç‚¹
                    inducing_indices.append(np.random.choice(indices))
        
        # å¦‚æœè¯±å¯¼ç‚¹ä¸å¤Ÿï¼Œè¡¥å……ä¸€äº›éšæœºç‚¹
        if len(inducing_indices) < n_inducing_points:
            remaining = n_inducing_points - len(inducing_indices)
            all_indices = list(set(range(len(X_train))) - set(inducing_indices))
            additional = np.random.choice(all_indices, size=min(remaining, len(all_indices)), replace=False)
            inducing_indices.extend(additional)
        
        # å¦‚æœè¯±å¯¼ç‚¹å¤ªå¤šï¼Œéšæœºé€‰æ‹©å­é›†
        if len(inducing_indices) > n_inducing_points:
            inducing_indices = np.random.choice(inducing_indices, size=n_inducing_points, replace=False)
        
        # è·å–è¯±å¯¼ç‚¹çš„ç‰¹å¾å€¼
        if isinstance(X_train, pd.DataFrame):
            if feature_columns is not None:
                # å¦‚æœæŒ‡å®šäº†ç‰¹å¾åˆ—ï¼Œä½¿ç”¨æŒ‡å®šçš„åˆ—
                inducing_points = X_train.iloc[inducing_indices][feature_columns].values
            else:
                # å¦åˆ™è‡ªåŠ¨ç¡®å®šï¼šåªè·å–æ•°å€¼åˆ—ï¼Œæ’é™¤h3_indexå’Œå…¶ä»–éç‰¹å¾åˆ—
                numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
                # æ’é™¤h3_indexå³ä½¿å®ƒæ˜¯æ•°å€¼å‹ï¼ˆè™½ç„¶ç°åœ¨æ˜¯å­—ç¬¦ä¸²ï¼‰
                # åŒæ—¶æ’é™¤å¯èƒ½çš„ç›®æ ‡å˜é‡ï¼ˆVHIï¼‰
                exclude_cols = [h3_col, 'h3_index', 'original_h3_index', 'VHI']
                numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
                inducing_points = X_train.iloc[inducing_indices][numeric_cols].values
        else:
            inducing_points = X_train.iloc[inducing_indices].values
            
        print(f"    é€‰æ‹©äº†{n_h3_select}ä¸ªH3ç½‘æ ¼ Ã— {n_years_select}ä¸ªå¹´ä»½")
        print(f"    æœ€ç»ˆå¾—åˆ°{len(inducing_points)}ä¸ªè¯±å¯¼ç‚¹")
        
        if return_indices:
            return inducing_indices
        else:
            return inducing_points.astype(np.float32)
    else:
        # ä½¿ç”¨KMeansæ–¹æ³•é€‰æ‹©è¯±å¯¼ç‚¹
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if isinstance(X_train, pd.DataFrame):
            if feature_columns is not None:
                # å¦‚æœæŒ‡å®šäº†ç‰¹å¾åˆ—ï¼Œä½¿ç”¨æŒ‡å®šçš„åˆ—
                X_array = X_train[feature_columns].values
            else:
                # åªä½¿ç”¨æ•°å€¼åˆ—ï¼Œæ’é™¤éç‰¹å¾åˆ—
                numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
                exclude_cols = [h3_col, 'h3_index', 'original_h3_index', 'VHI']
                numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
                X_array = X_train[numeric_cols].values
        else:
            X_array = X_train
            
        if len(X_array) > n_inducing_points:
            # å¼ºåˆ¶è®¾ç½®OMP_NUM_THREADS=1ä»¥é¿å…å†…å­˜æ³„æ¼
            if platform.system() == 'Windows':
                old_value = os.environ.get('OMP_NUM_THREADS', None)
                os.environ['OMP_NUM_THREADS'] = '1'
            
            kmeans = KMeans(n_clusters=n_inducing_points, random_state=random_state, n_init=10)
            kmeans.fit(X_array)
            inducing_points = kmeans.cluster_centers_
            
            if platform.system() == 'Windows' and 'old_value' in locals():
                if old_value is not None:
                    os.environ['OMP_NUM_THREADS'] = old_value
                    
            print(f"    ä½¿ç”¨KMeansèšç±»é€‰æ‹©äº†{len(inducing_points)}ä¸ªè¯±å¯¼ç‚¹")
        else:
            inducing_points = X_array.copy()
    
    if return_indices:
        # KMeansä¸è¿”å›ç´¢å¼•ï¼Œè¿”å›Noneè¡¨ç¤ºéœ€è¦ä½¿ç”¨èšç±»ä¸­å¿ƒ
        return None
    else:
        return inducing_points.astype(np.float32)

def create_stgpr_model(X_train, y_train, num_inducing_points=None, batch_size=None, device=None,
                      spatial_variance=None, temporal_variance=None, feature_variance=None,
                      spatial_lengthscale=None, temporal_lengthscale=None, feature_lengthscale=None,
                      scaler=None, X_train_full=None):
    """
    åˆ›å»ºST-GPRæ¨¡å‹å®ä¾‹
    
    å‚æ•°:
    X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µ
    y_train: è®­ç»ƒç›®æ ‡å˜é‡
    num_inducing_points: è¯±å¯¼ç‚¹æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
    spatial_variance: ç©ºé—´æ ¸å‡½æ•°çš„æ–¹å·®å‚æ•°
    temporal_variance: æ—¶é—´æ ¸å‡½æ•°çš„æ–¹å·®å‚æ•°
    feature_variance: ç‰¹å¾æ ¸å‡½æ•°çš„æ–¹å·®å‚æ•°
    spatial_lengthscale: ç©ºé—´æ ¸å‡½æ•°çš„é•¿åº¦å°ºåº¦å‚æ•°
    temporal_lengthscale: æ—¶é—´æ ¸å‡½æ•°çš„é•¿åº¦å°ºåº¦å‚æ•°
    feature_lengthscale: ç‰¹å¾æ ¸å‡½æ•°çš„é•¿åº¦å°ºåº¦å‚æ•°
    scaler: å·²åºŸå¼ƒï¼Œä¿ç•™ä»…ä¸ºå…¼å®¹æ€§
    X_train_full: åŒ…å«å®Œæ•´ä¿¡æ¯çš„åŸå§‹DataFrameï¼ˆç”¨äºè¯±å¯¼ç‚¹é€‰æ‹©ï¼ŒåŒ…å«h3_indexå’Œyearï¼‰
    
    è¿”å›:
    tuple: (model, X_train_tensor, y_train_tensor, device)
    """
    # æ£€æŸ¥num_inducing_pointså‚æ•°
    if num_inducing_points is None:
        num_inducing_points = CONFIG['model']['num_inducing_points']
    
    # ä¿å­˜åŸå§‹DataFrameç”¨äºè¯±å¯¼ç‚¹é€‰æ‹©
    # ä¼˜å…ˆä½¿ç”¨X_train_fullï¼ˆåŒ…å«h3_indexå’Œyearï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨X_train
    X_train_original = X_train_full if X_train_full is not None else (X_train if isinstance(X_train, pd.DataFrame) else None)
        
    # ç¡®ä¿è¾“å…¥æ•°æ®æ˜¯æµ®ç‚¹ç±»å‹
    if isinstance(X_train, pd.DataFrame):
        X_train_np = X_train.select_dtypes(include=[np.number]).values.astype(np.float32)
    else:
        X_train_np = np.asarray(X_train).astype(np.float32)
    
    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
    X_train_np_used = X_train_np
    
    # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†pandas Seriesç±»å‹çš„y_train
    if isinstance(y_train, pd.Series):
        y_train_np = np.asarray(y_train.values).reshape(-1).astype(np.float32)
    else:
        y_train_np = np.asarray(y_train).reshape(-1).astype(np.float32)
    
    # ä»è®­ç»ƒæ•°æ®ä¸­é€‰æ‹©è¯±å¯¼ç‚¹
    if X_train_np_used.shape[0] > num_inducing_points:
        # å¦‚æœæœ‰åŸå§‹DataFrameï¼Œè°ƒç”¨è¯±å¯¼ç‚¹é€‰æ‹©å‡½æ•°
        if X_train_original is not None:
            Z_indices = select_inducing_points_spatiotemporal(
                X_train_original, 
                num_inducing_points,
                h3_col='h3_index',
                year_col='year', 
                random_state=RANDOM_SEED,
                return_indices=True  # è¿”å›ç´¢å¼•
            )
            # ä»åŸå§‹æ•°æ®ä¸­æå–è¯±å¯¼ç‚¹
            if Z_indices is not None and len(Z_indices) > 0:
                # ä½¿ç”¨æ—¶ç©ºåˆ†å±‚é‡‡æ ·çš„ç´¢å¼•
                Z_np = X_train_np_used[Z_indices]
            else:
                # ä½¿ç”¨KMeansï¼ˆè¿™æ˜¯é»˜è®¤è¡Œä¸ºï¼Œä¸æ˜¯è­¦å‘Šï¼‰
                kmeans = KMeans(n_clusters=num_inducing_points, random_state=RANDOM_SEED, n_init=10)
                kmeans.fit(X_train_np_used)
                Z_np = kmeans.cluster_centers_.astype(np.float32)
        else:
            # ç›´æ¥ä½¿ç”¨KMeans
            print(f"  ä½¿ç”¨KMeansèšç±»é€‰æ‹©è¯±å¯¼ç‚¹")
            # å¼ºåˆ¶è®¾ç½®OMP_NUM_THREADS=1ä»¥å½»åº•è§£å†³å†…å­˜æ³„æ¼é—®é¢˜
            if platform.system() == 'Windows':
                old_value = os.environ.get('OMP_NUM_THREADS', None)
                os.environ['OMP_NUM_THREADS'] = '1'
            
            kmeans = KMeans(n_clusters=num_inducing_points, random_state=RANDOM_SEED, n_init=10)
            kmeans.fit(X_train_np_used)  # åœ¨åŸå§‹æ•°æ®ä¸Šè¿›è¡Œèšç±»
            Z_np = kmeans.cluster_centers_.astype(np.float32)
    else:
        Z_np = X_train_np_used.copy()
        num_inducing_points = X_train_np_used.shape[0]
    
    # è·å–ç‰¹å¾ç»´åº¦
    input_dim = X_train_np.shape[1]
    
    # ç¡®å®šç©ºé—´ã€æ—¶é—´å’Œç‰¹å¾çš„ç´¢å¼•
    spatial_dims = list(range(2))
    temporal_dims = [input_dim - 1]
    feature_dims = list(set(range(input_dim)) - set(spatial_dims) - set(temporal_dims))
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.from_numpy(X_train_np_used)  # ä½¿ç”¨åŸå§‹æ•°æ®
    y_train_tensor = torch.from_numpy(y_train_np)
    inducing_points = torch.from_numpy(Z_np)
    
    # æ£€æµ‹è®¾å¤‡
    if device is None:
        device = torch.device('cpu')
    
    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
    X_train_tensor = X_train_tensor.to(device)
    y_train_tensor = y_train_tensor.to(device)
    inducing_points = inducing_points.to(device)
    
    # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹æ•°æ®çš„æ–¹å·®è®¡ç®—sigma_valuesï¼ˆç”¨äºæ ¸å‡½æ•°çš„æ–¹å·®å½’ä¸€åŒ–ï¼‰
    sigma_values = torch.tensor(np.var(X_train_np_used[:, feature_dims], axis=0) + 1e-5, dtype=torch.float32).to(device)
    
    # åˆ›å»ºç‰¹å¾æƒé‡
    feature_weights = torch.ones(len(feature_dims), device=device)
    
    # ä»é…ç½®ä¸­è·å–æ ¸å‡½æ•°å‚æ•°
    kernel_config = CONFIG['kernel']
    
    # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é…ç½®ä¸­çš„é»˜è®¤å€¼
    spatial_variance = spatial_variance if spatial_variance is not None else kernel_config.get('spatial_variance_init', 1.0)
    temporal_variance = temporal_variance if temporal_variance is not None else kernel_config.get('temporal_variance_init', 1.0)
    feature_variance = feature_variance if feature_variance is not None else kernel_config.get('feature_variance_init', 1.0)
    spatial_lengthscale = spatial_lengthscale if spatial_lengthscale is not None else kernel_config.get('spatial_lengthscale_init', 1.0)
    temporal_lengthscale = temporal_lengthscale if temporal_lengthscale is not None else kernel_config.get('temporal_lengthscale_init', 1.0)
    feature_lengthscale = feature_lengthscale if feature_lengthscale is not None else kernel_config.get('feature_lengthscale_init', 1.0)
    
    # åˆ›å»ºæ¨¡å‹
    model = STGPRModel(
        inducing_points=inducing_points,
        input_dim=input_dim,
        spatial_dims=spatial_dims,
        temporal_dims=temporal_dims,
        feature_dims=feature_dims,
        spatial_variance=spatial_variance,
        temporal_variance=temporal_variance,
        feature_variance=feature_variance,
        spatial_lengthscale=spatial_lengthscale,
        temporal_lengthscale=temporal_lengthscale,
        feature_lengthscale=feature_lengthscale,
        feature_weights=feature_weights,
        sigma_values=sigma_values
    )
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
    model = model.to(device)
    
    # åº”ç”¨å‚æ•°çº¦æŸ
    lengthscale_lower_bound = kernel_config.get('lengthscale_lower_bound', 1e-3)
    variance_lower_bound = kernel_config.get('variance_lower_bound', 1e-5)
    
    with torch.no_grad():
        for name, param in model.named_parameters():
            if 'lengthscale' in name:
                param.data.clamp_(min=lengthscale_lower_bound)
            elif 'outputscale' in name or 'raw_outputscale' in name:
                param.data.clamp_(min=variance_lower_bound)
    
    return model, X_train_tensor, y_train_tensor, device

def load_stgpr_model(model_path, device=None):
    """
    ä»ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ä¸­åŠ è½½STGPRæ¨¡å‹
    
    å‚æ•°:
    model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
    
    è¿”å›:
    dict: åŒ…å«åŠ è½½çš„æ¨¡å‹å’Œç›¸å…³å…ƒæ•°æ®çš„å­—å…¸
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # ç¡®å®šè®¾å¤‡
    if device is None:
        device = torch.device('cpu')
    
    # åŠ è½½æ¨¡å‹æ•°æ®
    checkpoint = torch.load(model_path, map_location=device)
    
    # æå–æ¨¡å‹å…ƒæ•°æ®
    feature_names = checkpoint.get('feature_names', None)
    scaler = checkpoint.get('scaler', None)
    metrics = checkpoint.get('metrics', {})
    
    # å¦‚æœæ²¡æœ‰ç‰¹å¾åç§°åˆ—è¡¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
    if feature_names is None:
        # å°è¯•æ¨æ–­ç‰¹å¾ç»´åº¦
        for name, param in checkpoint['model_state_dict'].items():
            if 'lengthscale' in name and len(param.shape) > 0:
                n_features = param.shape[0]
                feature_names = [f'feature_{i}' for i in range(n_features)]
                break
    
    # ç¡®å®šè¾“å…¥ç»´åº¦
    input_dim = len(feature_names) if feature_names else 19  # é»˜è®¤å€¼ï¼šlat, lon, 16ä¸ªç‰¹å¾, year
    
    # åˆ›å»ºè¯±å¯¼ç‚¹
    inducing_points = None
    for name, param in checkpoint['model_state_dict'].items():
        if 'inducing_points' in name:
            inducing_points = param
            break
    
    if inducing_points is None:
        num_inducing = CONFIG['model']['num_inducing_points']
        inducing_points = torch.randn(num_inducing, input_dim, device=device)
    
    # ç¡®å®šç©ºé—´ã€æ—¶é—´å’Œç‰¹å¾çš„ç´¢å¼•
    spatial_dims = list(range(2))
    temporal_dims = [input_dim - 1]
    feature_dims = list(set(range(input_dim)) - set(spatial_dims) - set(temporal_dims))
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = STGPRModel(
        inducing_points=inducing_points,
        input_dim=input_dim,
        spatial_dims=spatial_dims,
        temporal_dims=temporal_dims,
        feature_dims=feature_dims
    )
    
    # åˆ›å»ºä¼¼ç„¶å‡½æ•°
    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    
    # åŠ è½½å‚æ•°
    model.load_state_dict(checkpoint['model_state_dict'])
    likelihood.load_state_dict(checkpoint['likelihood_state_dict'])
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    likelihood = likelihood.to(device)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    likelihood.eval()
    
    return {
        'model': model,
        'likelihood': likelihood,
        'feature_names': feature_names,
        'scaler': scaler,
        'metrics': metrics,
        'device': device
    }

def predict_with_stgpr(model_dict, X_new, return_variance=False):
    """
    ä½¿ç”¨åŠ è½½çš„STGPRæ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    å‚æ•°:
    model_dict: ç”±load_stgpr_modelå‡½æ•°è¿”å›çš„æ¨¡å‹å­—å…¸
    X_new: æ–°çš„ç‰¹å¾æ•°æ®ï¼ŒDataFrameæˆ–numpyæ•°ç»„
    return_variance: æ˜¯å¦è¿”å›é¢„æµ‹æ–¹å·®
    
    è¿”å›:
    mean: é¢„æµ‹å‡å€¼ï¼Œè¿”å›ä¸€ç»´numpyæ•°ç»„ï¼Œä¸GeoShapleyExplaineræœŸæœ›çš„æ ¼å¼ä¸€è‡´
    variance (å¯é€‰): é¢„æµ‹æ–¹å·®ï¼Œè¿”å›ä¸€ç»´numpyæ•°ç»„
    """
    model = model_dict['model']
    likelihood = model_dict['likelihood']
    device = model_dict['device']
    
    # é¢„å¤„ç†è¾“å…¥æ•°æ®
    if isinstance(X_new, pd.DataFrame):
        X_new_np = X_new.values
    else:
        X_new_np = np.asarray(X_new)
    
    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
    X_new_used = X_new_np
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_new_tensor = torch.tensor(X_new_used, dtype=torch.float32).to(device)
    
    # è¿›è¡Œé¢„æµ‹
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        if return_variance:
            # è·å–å®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒ
            pred_dist = likelihood(model(X_new_tensor))
            mean = pred_dist.mean.cpu().numpy()
            variance = pred_dist.variance.cpu().numpy()
            
            # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„ï¼ŒGeoShapleyæœŸæœ›ä¸€ç»´è¾“å‡º
            if len(mean.shape) > 1:
                mean = mean.flatten()
            if len(variance.shape) > 1:
                variance = variance.flatten()
                
            return mean, variance
        else:
            # åªè¿”å›å‡å€¼é¢„æµ‹
            pred_dist = model(X_new_tensor)
            mean = pred_dist.mean.cpu().numpy()
            
            # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„ï¼ŒGeoShapleyæœŸæœ›ä¸€ç»´è¾“å‡º
            if len(mean.shape) > 1:
                mean = mean.flatten()
                
            return mean

# hyperoptä¼˜åŒ–ç›¸å…³å‡½æ•°
def optimize_stgpr_hyperparameters(X_train, y_train, feature_names, num_inducing_points=50, 
                                  spatial_dims=[0, 1], temporal_dims=[-1], max_evals=10, device=None,
                                  scaler=None, X_train_full=None):
    """
    ä½¿ç”¨hyperoptä¼˜åŒ–STGPRæ ¸å‡½æ•°çš„è¶…å‚æ•°
    
    å‚æ•°:
    X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µï¼ˆå¯ä»¥æ˜¯DataFrameæˆ–numpyæ•°ç»„ï¼‰
    y_train: è®­ç»ƒç›®æ ‡å˜é‡
    feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    num_inducing_points: è¯±å¯¼ç‚¹æ•°é‡
    spatial_dims: ç©ºé—´ç»´åº¦ç´¢å¼•åˆ—è¡¨
    temporal_dims: æ—¶é—´ç»´åº¦ç´¢å¼•åˆ—è¡¨
    max_evals: hyperoptæœ€å¤§è¯„ä¼°æ¬¡æ•°
    device: è®¡ç®—è®¾å¤‡
    scaler: å·²åºŸå¼ƒï¼Œä¿ç•™ä»…ä¸ºå…¼å®¹æ€§
    X_train_full: åŒ…å«å®Œæ•´ä¿¡æ¯çš„åŸå§‹DataFrameï¼ˆç”¨äºè¯±å¯¼ç‚¹é€‰æ‹©ï¼ŒåŒ…å«h3_indexå’Œyearï¼‰
    
    è¿”å›:
    dict: åŒ…å«æœ€ä¼˜å‚æ•°å’Œè®­ç»ƒå¥½çš„æ¨¡å‹çš„å­—å…¸
    """
    if not HAS_HYPEROPT:
        return None
    
    # ä¿å­˜åŸå§‹DataFrameç”¨äºè¯±å¯¼ç‚¹é€‰æ‹©
    # ä¼˜å…ˆä½¿ç”¨X_train_fullï¼ˆåŒ…å«h3_indexå’Œyearï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨X_train
    X_train_original = X_train_full if X_train_full is not None else (X_train if isinstance(X_train, pd.DataFrame) else None)
    
    # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„
    if isinstance(X_train, pd.DataFrame):
        X_train_np = X_train.select_dtypes(include=[np.number]).values.astype(np.float32)
    else:
        X_train_np = np.asarray(X_train).astype(np.float32)
    
    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
    X_train_np_used = X_train_np
    
    # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†pandas Seriesç±»å‹çš„y_train
    if isinstance(y_train, pd.Series):
        y_train_np = np.asarray(y_train.values).reshape(-1).astype(np.float32)
    else:
        y_train_np = np.asarray(y_train).reshape(-1).astype(np.float32)
    
    # ç¡®å®šè®¾å¤‡
    if device is None:
        device = torch.device('cpu')
    
    # ç¡®å®šç‰¹å¾ç»´åº¦
    input_dim = X_train_np.shape[1]
    
    # å¦‚æœfeature_dimsä¸ºNoneï¼Œè‡ªåŠ¨è®¡ç®—ï¼ˆé™¤äº†ç©ºé—´å’Œæ—¶é—´ç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦ï¼‰
    all_dims = set(range(input_dim))
    spatial_temporal_dims = set(spatial_dims + temporal_dims)
    feature_dims = list(all_dims - spatial_temporal_dims)
    
    # ç¡®ä¿num_inducing_pointsä¸ä¸ºNone
    if num_inducing_points is None:
        num_inducing_points = min(500, X_train_np.shape[0] // 10)  # é»˜è®¤ä½¿ç”¨æ•°æ®ç‚¹æ•°é‡çš„10%ä½œä¸ºè¯±å¯¼ç‚¹æ•°é‡
    
    # ä»è®­ç»ƒæ•°æ®ä¸­é€‰æ‹©è¯±å¯¼ç‚¹
    if X_train_np_used.shape[0] > num_inducing_points:
        # å¦‚æœæœ‰åŸå§‹DataFrameï¼Œè°ƒç”¨è¯±å¯¼ç‚¹é€‰æ‹©å‡½æ•°
        if X_train_original is not None:
            Z_indices = select_inducing_points_spatiotemporal(
                X_train_original, 
                num_inducing_points,
                h3_col='h3_index',
                year_col='year', 
                random_state=RANDOM_SEED,
                return_indices=True  # è¿”å›ç´¢å¼•
            )
            # ä»åŸå§‹æ•°æ®ä¸­æå–è¯±å¯¼ç‚¹
            if Z_indices is not None and len(Z_indices) > 0:
                # ä½¿ç”¨æ—¶ç©ºåˆ†å±‚é‡‡æ ·çš„ç´¢å¼•
                Z_np = X_train_np_used[Z_indices]
            else:
                # ä½¿ç”¨KMeansï¼ˆè¿™æ˜¯é»˜è®¤è¡Œä¸ºï¼Œä¸æ˜¯è­¦å‘Šï¼‰
                kmeans = KMeans(n_clusters=num_inducing_points, random_state=RANDOM_SEED, n_init=10)
                kmeans.fit(X_train_np_used)
                Z_np = kmeans.cluster_centers_.astype(np.float32)
        else:
            # ç›´æ¥ä½¿ç”¨KMeans
            print(f"  ä½¿ç”¨KMeansèšç±»é€‰æ‹©è¯±å¯¼ç‚¹")
            # å¼ºåˆ¶è®¾ç½®OMP_NUM_THREADS=1ä»¥å½»åº•è§£å†³å†…å­˜æ³„æ¼é—®é¢˜
            if platform.system() == 'Windows':
                old_value = os.environ.get('OMP_NUM_THREADS', None)
                os.environ['OMP_NUM_THREADS'] = '1'
            
            kmeans = KMeans(n_clusters=num_inducing_points, random_state=RANDOM_SEED, n_init=10)
            kmeans.fit(X_train_np_used)  # åœ¨åŸå§‹æ•°æ®ä¸Šè¿›è¡Œèšç±»
            Z_np = kmeans.cluster_centers_.astype(np.float32)
    else:
        Z_np = X_train_np_used.copy()
        num_inducing_points = X_train_np_used.shape[0]
    
    # è·å–ç‰¹å¾ç»´åº¦
    input_dim = X_train_np.shape[1]
    
    # ç¡®å®šç©ºé—´ã€æ—¶é—´å’Œç‰¹å¾çš„ç´¢å¼•
    spatial_dims = list(range(2))
    temporal_dims = [input_dim - 1]
    feature_dims = list(set(range(input_dim)) - set(spatial_dims) - set(temporal_dims))
    
    # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹æ•°æ®çš„æ–¹å·®ï¼ˆç”¨äºæ ¸å‡½æ•°çš„æ–¹å·®å½’ä¸€åŒ–ï¼‰
    sigma_values = torch.tensor(np.var(X_train_np_used[:, feature_dims], axis=0) + 1e-5, dtype=torch.float32)
    
    # ä»é…ç½®ä¸­è·å–æ ¸å‡½æ•°å‚æ•°
    kernel_config = CONFIG['kernel']
    
    # å®šä¹‰å‚æ•°ç©ºé—´
    space = {
        # æ–¹å·®å‚æ•°
        'spatial_variance': hp.loguniform('spatial_variance', np.log(0.01), np.log(5.0)),
        'temporal_variance': hp.loguniform('temporal_variance', np.log(0.01), np.log(5.0)),
        'feature_variance': hp.loguniform('feature_variance', np.log(0.01), np.log(5.0)),
        
        # é•¿åº¦å°ºåº¦
        'spatial_lengthscale': hp.loguniform('spatial_lengthscale', np.log(0.1), np.log(5.0)),
        'temporal_lengthscale': hp.loguniform('temporal_lengthscale', np.log(0.5), np.log(5.0)),
        'feature_lengthscale': hp.loguniform('feature_lengthscale', np.log(0.1), np.log(5.0)),
        
        # MatÃ©rnKernelå‚æ•°nu
        'nu': hp.choice('nu', [0.5, 1.5, 2.5]),
        
        # å¤åˆæ ¸æƒé‡w
        'w': hp.uniform('w', 0.05, 0.2)
    }
    
    # ç‰¹å¾æƒé‡
    for i in range(len(feature_dims)):
        space[f'p{i}'] = hp.uniform(f'p{i}', 0.1, 5)
    
    # æ·»åŠ è¯„ä¼°è®¡æ•°å™¨
    eval_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    # ç›®æ ‡å‡½æ•°
    def objective(params):
        eval_count[0] += 1
        try:
            # å¤„ç†å‚æ•°
            cleaned_params = {}
            p_function_values = [1.0] * len(feature_dims)
            
            for k, v in params.items():
                if k.startswith('p') and k[1:].isdigit():
                    idx = int(k[1:])
                    if idx < len(feature_dims):
                        p_function_values[idx] = float(v)
                elif k == 'nu':
                    # ç‰¹æ®Šå¤„ç†nuå‚æ•°ï¼Œç¡®ä¿å®ƒæ˜¯æœ‰æ•ˆçš„é€‰æ‹©å€¼
                    if isinstance(v, (int, float)):
                        # å¦‚æœæ˜¯æ•°å€¼ï¼Œæ˜ å°„åˆ°æœ€è¿‘çš„æœ‰æ•ˆå€¼
                        valid_nu_values = [0.5, 1.5, 2.5]
                        cleaned_params[k] = min(valid_nu_values, key=lambda x: abs(x - float(v)))
                    else:
                        # å¦‚æœæ˜¯ç´¢å¼•ï¼Œç›´æ¥ä½¿ç”¨
                        valid_nu_values = [0.5, 1.5, 2.5]
                        nu_idx = int(v) if isinstance(v, (int, float)) else 1
                        cleaned_params[k] = valid_nu_values[min(nu_idx, len(valid_nu_values)-1)]
                else:
                    cleaned_params[k] = float(v)
            
            # å°†PyTorchå¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            X_train_tensor = torch.tensor(X_train_np_used, dtype=torch.float32).to(device)  # ä½¿ç”¨åŸå§‹æ•°æ®
            y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).to(device)
            inducing_points = torch.tensor(Z_np, dtype=torch.float32).to(device)
            torch_sigma_values = sigma_values.clone().detach().to(device)
            feature_weights = torch.tensor(p_function_values, dtype=torch.float32).to(device)
            
            # åˆ›å»ºæ¨¡å‹
            model = STGPRModel(
                inducing_points=inducing_points,
                input_dim=input_dim,
                spatial_dims=spatial_dims,
                temporal_dims=temporal_dims,
                feature_dims=feature_dims,
                spatial_variance=cleaned_params.get('spatial_variance', 1.0),
                temporal_variance=cleaned_params.get('temporal_variance', 1.0),
                feature_variance=cleaned_params.get('feature_variance', 1.0),
                spatial_lengthscale=cleaned_params.get('spatial_lengthscale', 1.0),
                temporal_lengthscale=cleaned_params.get('temporal_lengthscale', 1.0),
                feature_lengthscale=cleaned_params.get('feature_lengthscale', 1.0),
                feature_weights=feature_weights,
                sigma_values=torch_sigma_values,
                nu=cleaned_params.get('nu', 2.5),
                w=cleaned_params.get('w', 0.1)
            ).to(device)
            
            # æ¨¡å‹å‚æ•°çº¦æŸ
            with torch.no_grad():
                for name, param in model.named_parameters():
                    if 'lengthscale' in name:
                        param.data.clamp_(min=kernel_config.get('lengthscale_lower_bound', 1e-3))
                    elif 'outputscale' in name or 'raw_outputscale' in name:
                        param.data.clamp_(min=kernel_config.get('variance_lower_bound', 1e-5))
            
            # åˆ›å»ºä¼¼ç„¶å‡½æ•°å’ŒæŸå¤±å‡½æ•°
            likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
            mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(y_train_np), combine_terms=True)
            
            # åˆ›å»ºä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨
            optimizer = torch.optim.Adam([
                {'params': model.parameters()},
                {'params': likelihood.parameters()}
            ], lr=0.01)
            
            # ç®€åŒ–è®­ç»ƒ - é’ˆå¯¹ä¸åŒæ•°æ®è§„æ¨¡ä¼˜åŒ–
            if X_train_np.shape[0] > 150000:  # res7çº§åˆ«çš„å¤§æ•°æ®é›†
                max_iter = 3  # æå°‘çš„è¿­ä»£æ¬¡æ•°ï¼Œåªä¸ºå¿«é€Ÿè¯„ä¼°
                batch_size = min(500, len(X_train_tensor))  # æ›´å¤§çš„æ‰¹æ¬¡
            elif X_train_np.shape[0] > 50000:  # res6çº§åˆ«
                max_iter = 5
                batch_size = min(300, len(X_train_tensor))
            elif X_train_np.shape[0] > 10000:  # ä¸­ç­‰æ•°æ®é›†
                max_iter = 8
                batch_size = min(200, len(X_train_tensor))
            else:  # å°æ•°æ®é›†
                max_iter = 10
                batch_size = min(100, len(X_train_tensor))
            
            train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            model.train()
            likelihood.train()
            
            # è®­ç»ƒå¾ªç¯
            for i in range(max_iter):
                epoch_loss = 0.0
                num_batches = 0
                
                for X_batch, y_batch in train_loader:
                    optimizer.zero_grad()
                    output = model(X_batch)
                    loss = -mll(output, y_batch)
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                    num_batches += 1
            
            # è¯„ä¼°æ€§èƒ½
            model.eval()
            likelihood.eval()
            
            with torch.no_grad(), gpytorch.settings.fast_pred_var():
                f_preds = model(X_train_tensor)
                mean_pred = f_preds.mean.cpu().numpy()
            
            # è®¡ç®—RMSEä½œä¸ºæŸå¤±
            loss_value = np.sqrt(mean_squared_error(y_train_np, mean_pred))
            r2_value = r2_score(y_train_np, mean_pred)
            
            # ç¡®å®šè¿­ä»£æ¬¡æ•°
            iterations = min(40, CONFIG['model']['num_iterations']) if X_train_np.shape[0] > 100000 else min(200, CONFIG['model']['num_iterations'])
            
            # æ›´æ–°è¿›åº¦æ˜¾ç¤º
            progress = eval_count[0] / max_evals * 100
            print(f"\r  è¿›åº¦: {progress:.0f}% [{eval_count[0]}/{max_evals}] | å½“å‰RMSE: {loss_value:.6f} | å½“å‰RÂ²: {r2_value:.4f}", end="", flush=True)
            
            return {
                'loss': loss_value,
                'status': STATUS_OK,
                'model': model,
                'likelihood': likelihood,
                'rmse': loss_value,
                'r2': r2_value,
                'iterations': iterations,
                'params': params
            }
        except Exception as e:
            # æ›´æ–°è¿›åº¦æ˜¾ç¤ºï¼ˆå¤±è´¥æƒ…å†µï¼‰
            progress = eval_count[0] / max_evals * 100
            print(f"\r  è¿›åº¦: {progress:.0f}% [{eval_count[0]}/{max_evals}] | è¯„ä¼°å¤±è´¥: {str(e)[:30]}...", end="", flush=True)
            return {'loss': 1e10, 'status': 'fail', 'exception': str(e)}
    
    try:
        # è®¾ç½®éšæœºç§å­
        np.random.seed(RANDOM_SEED)
        
        # ä½¿ç”¨hyperoptä¼˜åŒ–
        trials = Trials()
        
        # ç¦ç”¨hyperoptçš„é»˜è®¤è¿›åº¦æ¡ï¼Œä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ˜¾ç¤º
        import logging
        logging.getLogger('hyperopt').setLevel(logging.WARNING)
        
        # è‡ªå®šä¹‰è¿›åº¦æ˜¾ç¤º
        print(f"ğŸ” å¼€å§‹è´å¶æ–¯ä¼˜åŒ– (å…±{max_evals}æ¬¡è¯„ä¼°)...")
        
        best = fmin(
            fn=lambda params: objective(params)['loss'],
            space=space,
            algo=tpe.suggest,
            max_evals=max_evals,
            trials=trials,
            show_progressbar=False  # ç¦ç”¨é»˜è®¤è¿›åº¦æ¡
        )
        
        # æ¢è¡Œä»¥ç»“æŸè¿›åº¦æ˜¾ç¤º
        print()  # æ¢è¡Œ
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°åˆ›å»ºæ¨¡å‹
        final_params = {}
        for k, v in best.items():
            if k == 'nu':
                # ç‰¹æ®Šå¤„ç†nuå‚æ•°
                valid_nu_values = [0.5, 1.5, 2.5]
                if isinstance(v, (int, float)):
                    final_params[k] = min(valid_nu_values, key=lambda x: abs(x - float(v)))
                else:
                    nu_idx = int(v) if isinstance(v, (int, float)) else 1
                    final_params[k] = valid_nu_values[min(nu_idx, len(valid_nu_values)-1)]
            else:
                final_params[k] = float(v)
        
        best_model_result = objective(final_params)
        
        # æ·»åŠ æœ€ä½³å‚æ•°åˆ°ç»“æœ
        if best_model_result['status'] == STATUS_OK:
            best_model_result['best_params'] = final_params
            return best_model_result
        
    except Exception as e:
        print(f"ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    # å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å‚æ•°
    default_params = {
        'spatial_variance': kernel_config.get('spatial_variance_init', 1.0),
        'temporal_variance': kernel_config.get('temporal_variance_init', 1.0),
        'feature_variance': kernel_config.get('feature_variance_init', 1.0),
        'spatial_lengthscale': kernel_config.get('spatial_lengthscale_init', 1.0),
        'temporal_lengthscale': kernel_config.get('temporal_lengthscale_init', 1.0),
        'feature_lengthscale': kernel_config.get('feature_lengthscale_init', 1.0),
        'nu': 2.5,
        'w': kernel_config.get('w_init', 0.1)
    }
    
    # å°†é»˜è®¤æƒé‡æ·»åŠ åˆ°å‚æ•°ä¸­
    for i in range(len(feature_dims)):
        default_params[f'p{i}'] = 1.0
    
    # ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹
    result = objective(default_params)
    if result['status'] == STATUS_OK:
        result['best_params'] = default_params
        return result
    
    return None

def train_stgpr_model(model, likelihood, X_train_tensor, y_train_tensor, num_iterations=None, 
                     use_lbfgs=None, batch_size=None, callback=None, device=None):
    """
    è®­ç»ƒSTGPRæ¨¡å‹
    
    å‚æ•°:
    model: STGPRæ¨¡å‹å®ä¾‹
    likelihood: ä¼¼ç„¶å‡½æ•°
    X_train_tensor: è®­ç»ƒç‰¹å¾å¼ é‡
    y_train_tensor: è®­ç»ƒç›®æ ‡å¼ é‡
    num_iterations: è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    use_lbfgs: æ˜¯å¦ä½¿ç”¨L-BFGSä¼˜åŒ–å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    callback: è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦å’Œä¸­é—´ç»“æœ
    device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
    model: è®­ç»ƒå¥½çš„æ¨¡å‹
    likelihood: è®­ç»ƒå¥½çš„ä¼¼ç„¶å‡½æ•°
    metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_iterations is None:
        num_iterations = CONFIG['model']['num_iterations']
    if use_lbfgs is None:
        use_lbfgs = CONFIG['model'].get('use_lbfgs', True)
    if batch_size is None:
        batch_size = CONFIG['model']['batch_size']
    
    # è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œè¿­ä»£æ¬¡æ•°
    if X_train_tensor.shape[0] > 100000:  # å¤§æ•°æ®é›†ï¼Œé™ä½è®¡ç®—æˆæœ¬
        batch_size = min(batch_size, 500)
        num_iterations = min(num_iterations, 50)
    
    # è®¾ç½®è®­ç»ƒæ¨¡å¼
    model.train()
    likelihood.train()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=X_train_tensor.size(0), combine_terms=True)
    
    if use_lbfgs:
        # ä½¿ç”¨L-BFGSä¼˜åŒ–å™¨ï¼ˆé€‚ç”¨äºå°æ•°æ®é›†å’Œä¸­ç­‰æ•°æ®é›†ï¼‰
        optimizer = torch.optim.LBFGS(
            [{'params': model.parameters()}, {'params': likelihood.parameters()}],
            line_search_fn="strong_wolfe",
            max_iter=5
        )
        
        # è®­ç»ƒå¾ªç¯
        for i in range(num_iterations):
            # å®šä¹‰é—­åŒ…å‡½æ•°ä»¥è®¡ç®—æŸå¤±
            def closure():
                optimizer.zero_grad()
                output = model(X_train_tensor)
                loss = -mll(output, y_train_tensor)
                loss.backward()
                return loss
            
            # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
            loss = optimizer.step(closure)
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if callback is not None and i % max(1, num_iterations // 10) == 0:
                callback(i, num_iterations, loss.item())
    else:
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰
        optimizer = torch.optim.Adam(
            [{'params': model.parameters()}, {'params': likelihood.parameters()}],
            lr=CONFIG['optimizer'].get('adam_learning_rate', 0.01)
        )
        
        # æ¢¯åº¦è£å‰ªå€¼
        grad_clip_norm = CONFIG['optimizer'].get('gradient_clip_norm', 10.0)
        
        # è®­ç»ƒå¾ªç¯
        for i in range(num_iterations):
            epoch_loss = 0.0
            num_batches = 0
            
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                output = model(X_batch)
                loss = -mll(output, y_batch)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                if grad_clip_norm > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                    torch.nn.utils.clip_grad_norm_(likelihood.parameters(), grad_clip_norm)
                
                optimizer.step()
                epoch_loss += loss.item()
                num_batches += 1
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if callback is not None and i % max(1, num_iterations // 10) == 0:
                avg_loss = epoch_loss / max(1, num_batches)
                callback(i, num_iterations, avg_loss)
    
    # è®¡ç®—è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡
    model.eval()
    likelihood.eval()
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        # åˆ†æ‰¹é¢„æµ‹ï¼Œé¿å…å†…å­˜æº¢å‡º
        all_means = []
        all_targets = []
        
        for X_batch, y_batch in train_loader:
            pred_dist = likelihood(model(X_batch))
            means = pred_dist.mean.cpu().numpy()
            targets = y_batch.cpu().numpy()
            
            all_means.append(means)
            all_targets.append(targets)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹
        train_preds = np.concatenate(all_means)
        train_targets = np.concatenate(all_targets)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    train_rmse = np.sqrt(mean_squared_error(train_targets, train_preds))
    train_r2 = r2_score(train_targets, train_preds)
    
    metrics = {
        'train_rmse': float(train_rmse),
        'train_r2': float(train_r2),
        'num_iterations': num_iterations
    }
    
    return model, likelihood, metrics

def evaluate_stgpr_model(model, likelihood, X_test_tensor, y_test_tensor, batch_size=None):
    """
    è¯„ä¼°STGPRæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
    
    å‚æ•°:
    model: è®­ç»ƒå¥½çš„STGPRæ¨¡å‹
    likelihood: è®­ç»ƒå¥½çš„ä¼¼ç„¶å‡½æ•°
    X_test_tensor: æµ‹è¯•ç‰¹å¾å¼ é‡
    y_test_tensor: æµ‹è¯•ç›®æ ‡å¼ é‡
    batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    
    è¿”å›:
    metrics: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    predictions: é¢„æµ‹ç»“æœ
    """
    # è·å–æ‰¹å¤„ç†å¤§å°
    if batch_size is None:
        batch_size = CONFIG['model']['batch_size']
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    likelihood.eval()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ†æ‰¹é¢„æµ‹ï¼Œé¿å…å†…å­˜æº¢å‡º
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        all_means = []
        all_variances = []
        all_targets = []
        
        for X_batch, y_batch in test_loader:
            pred_dist = likelihood(model(X_batch))
            means = pred_dist.mean.cpu().numpy()
            variances = pred_dist.variance.cpu().numpy()
            targets = y_batch.cpu().numpy()
            
            all_means.append(means)
            all_variances.append(variances)
            all_targets.append(targets)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹
        test_preds = np.concatenate(all_means)
        test_variances = np.concatenate(all_variances)
        test_targets = np.concatenate(all_targets)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    test_rmse = np.sqrt(mean_squared_error(test_targets, test_preds))
    test_r2 = r2_score(test_targets, test_preds)
    test_mae = np.mean(np.abs(test_targets - test_preds))  # æ·»åŠ MAEè®¡ç®—
    
    # è®¡ç®—95%ç½®ä¿¡åŒºé—´
    test_std = np.sqrt(test_variances)
    lower_bound = test_preds - 1.96 * test_std
    upper_bound = test_preds + 1.96 * test_std
    
    # è®¡ç®—ç½®ä¿¡åŒºé—´è¦†ç›–ç‡
    coverage = np.mean((test_targets >= lower_bound) & (test_targets <= upper_bound))
    
    # è®¡ç®—æ ‡å‡†åŒ–çš„ç½®ä¿¡åŒºé—´å®½åº¦
    nciw = np.mean(upper_bound - lower_bound) / np.std(test_targets)
    
    metrics = {
        'test_rmse': float(test_rmse),
        'test_r2': float(test_r2),
        'test_mae': float(test_mae),  # æ·»åŠ MAEåˆ°è¿”å›çš„å­—å…¸ä¸­
        'coverage_prob': float(coverage),
        'nciw': float(nciw)
    }
    
    predictions = {
        'mean': test_preds,
        'variance': test_variances,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'targets': test_targets
    }
    
    return metrics, predictions

def train_evaluate_stgpr_model(X_train, y_train, X_test=None, y_test=None, feature_names=None,
                              num_inducing_points=500, optimize_hyperparams=True, max_evals=10, 
                              model_path=None, device=None, X_train_full=None):
    """
    è®­ç»ƒå’Œè¯„ä¼°STGPRæ¨¡å‹çš„å®Œæ•´æµç¨‹
    
    å‚æ•°:
    X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µï¼ˆå¯ä»¥æ˜¯DataFrameæˆ–numpyæ•°ç»„ï¼‰
    y_train: è®­ç»ƒç›®æ ‡å˜é‡
    X_test: æµ‹è¯•ç‰¹å¾çŸ©é˜µï¼ˆå¯é€‰ï¼‰
    y_test: æµ‹è¯•ç›®æ ‡å˜é‡ï¼ˆå¯é€‰ï¼‰
    feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    num_inducing_points: è¯±å¯¼ç‚¹æ•°é‡
    optimize_hyperparams: æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    max_evals: hyperoptæœ€å¤§è¯„ä¼°æ¬¡æ•°
    model_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    device: è®¡ç®—è®¾å¤‡
    X_train_full: åŒ…å«å®Œæ•´ä¿¡æ¯çš„åŸå§‹DataFrameï¼ˆç”¨äºè¯±å¯¼ç‚¹é€‰æ‹©ï¼ŒåŒ…å«h3_indexå’Œyearï¼‰
    
    è¿”å›:
    dict: åŒ…å«è®­ç»ƒç»“æœã€æ¨¡å‹ã€è¯„ä¼°æŒ‡æ ‡ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    if feature_names is None and isinstance(X_train, pd.DataFrame):
        feature_names = list(X_train.columns)
    
    # å¼€å§‹è®¡æ—¶
    start_time = time.time()
    
    # ç¡®å®šè®¾å¤‡
    if device is None:
        device = torch.device('cpu')
    
    # å‡†å¤‡æ•°æ®
    if isinstance(X_train, pd.DataFrame):
        X_train_np = X_train.values.astype(np.float32)
    else:
        X_train_np = np.asarray(X_train).astype(np.float32)
    
    if isinstance(y_train, pd.Series):
        y_train_np = y_train.values.astype(np.float32)
    else:
        y_train_np = np.asarray(y_train).astype(np.float32)
    
    # è®°å½•è®­ç»ƒä¿¡æ¯
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {X_train_np.shape[0]} æ ·æœ¬, {X_train_np.shape[1]} ç‰¹å¾")
    
    # æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if optimize_hyperparams and HAS_HYPEROPT:
        print(f"\nğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ– (æœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals})")
        opt_result = optimize_stgpr_hyperparameters(
            X_train, y_train, feature_names,
            num_inducing_points=num_inducing_points,
            max_evals=max_evals,
            device=device,
            scaler=None,  # ä¸ä½¿ç”¨scaler
            X_train_full=X_train_full
        )
        
        if opt_result and opt_result['status'] == STATUS_OK:
            model = opt_result['model']
            likelihood = opt_result['likelihood']
            best_params = opt_result.get('best_params', {})
            iterations = opt_result.get('iterations', CONFIG['model']['num_iterations'])
            
            print(f"\nâœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ")
            print(f"  æœ€ä½³RMSE: {opt_result['rmse']:.6f}")
            print(f"  æœ€ä½³RÂ²: {opt_result['r2']:.4f}")
            
            # æ‰“å°æœ€ä½³å‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            print(f"  å…³é”®å‚æ•°:")
            print(f"    ç©ºé—´é•¿åº¦å°ºåº¦: {best_params.get('spatial_lengthscale', 'N/A'):.3f}")
            print(f"    æ—¶é—´é•¿åº¦å°ºåº¦: {best_params.get('temporal_lengthscale', 'N/A'):.3f}")
            print(f"    ç‰¹å¾é•¿åº¦å°ºåº¦: {best_params.get('feature_lengthscale', 'N/A'):.3f}")
        else:
            print(f"\nâš ï¸ è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            model, X_train_tensor, y_train_tensor, device = create_stgpr_model(
                X_train, y_train, 
                num_inducing_points=num_inducing_points,
                device=device,
                scaler=None,  # ä¸ä½¿ç”¨scaler
                X_train_full=X_train_full
            )
            likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
            iterations = CONFIG['model']['num_iterations']
            best_params = {}
    else:
        # ä¸è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œç›´æ¥åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ—ï¸ ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹")
        model, X_train_tensor, y_train_tensor, device = create_stgpr_model(
            X_train, y_train, 
            num_inducing_points=num_inducing_points,
            device=device,
            scaler=None,  # ä¸ä½¿ç”¨scaler
            X_train_full=X_train_full
        )
        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
        iterations = CONFIG['model']['num_iterations']
        best_params = {}
    
    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ (è¿­ä»£æ¬¡æ•°: {iterations})")
    
    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
    last_update_time = [time.time()]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    def progress_callback(iteration, total_iterations, loss):
        """è®­ç»ƒè¿›åº¦å›è°ƒå‡½æ•°"""
        current_time = time.time()
        # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡æˆ–åœ¨æœ€åä¸€æ¬¡è¿­ä»£æ—¶æ›´æ–°
        if current_time - last_update_time[0] > 0.5 or iteration == total_iterations - 1:
            progress = (iteration + 1) / total_iterations * 100
            elapsed = current_time - start_time
            eta = elapsed / (iteration + 1) * total_iterations - elapsed
            
            # ä½¿ç”¨\rå®ç°è¡Œå†…æ›´æ–°
            print(f"\r  è¿›åº¦: {progress:.1f}% | è¿­ä»£: {iteration+1}/{total_iterations} | "
                  f"æŸå¤±: {loss:.6f} | å·²ç”¨æ—¶: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {eta:.1f}s", 
                  end='', flush=True)
            
            last_update_time[0] = current_time
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®å¼ é‡ï¼ˆæ— è®ºæ˜¯å¦è¿›è¡Œäº†è¶…å‚æ•°ä¼˜åŒ–éƒ½éœ€è¦ï¼‰
    if isinstance(X_train, pd.DataFrame):
        X_train_np = X_train.values.astype(np.float32)
    else:
        X_train_np = np.asarray(X_train).astype(np.float32)
    
    if isinstance(y_train, pd.Series):
        y_train_np = y_train.values.astype(np.float32)
    else:
        y_train_np = np.asarray(y_train).astype(np.float32)
    
    # åˆ›å»ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).to(device)
    
    # è®­ç»ƒæ¨¡å‹
    model, likelihood, train_metrics = train_stgpr_model(
        model, likelihood, X_train_tensor, y_train_tensor,
        num_iterations=iterations,
        callback=progress_callback,
        device=device
    )
    
    print()  # æ¢è¡Œä»¥ç»“æŸè¿›åº¦æ˜¾ç¤º
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"  è®­ç»ƒRMSE: {train_metrics['train_rmse']:.6f}")
    print(f"  è®­ç»ƒRÂ²: {train_metrics['train_r2']:.4f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆå¦‚æœæä¾›ï¼‰
    test_metrics = {}
    predictions = None  # åˆå§‹åŒ–predictionså˜é‡
    if X_test is not None and y_test is not None:
        print(f"\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ({len(X_test)} æ ·æœ¬)")
        
        if isinstance(X_test, pd.DataFrame):
            X_test_np = X_test.values.astype(np.float32)
        else:
            X_test_np = np.asarray(X_test).astype(np.float32)
        
        # ğŸ”´ ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
        X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test.values if hasattr(y_test, 'values') else y_test, 
                                   dtype=torch.float32).to(device)
        
        test_metrics, predictions = evaluate_stgpr_model(model, likelihood, X_test_tensor, y_test_tensor)
        
        print(f"  æµ‹è¯•RMSE: {test_metrics['test_rmse']:.6f}")
        print(f"  æµ‹è¯•RÂ²: {test_metrics['test_r2']:.4f}")
        print(f"  æµ‹è¯•MAE: {test_metrics['test_mae']:.6f}")
    
    # æ€»è€—æ—¶
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    # ç»„è£…è¿”å›ç»“æœ
    result = {
        'model': model,
        'likelihood': likelihood,
        'feature_names': feature_names,
        'scaler': None,  # ä¸å†ä½¿ç”¨scaler
        'metrics': {
            'train_rmse': train_metrics['train_rmse'],
            'train_r2': train_metrics['train_r2'],
            **test_metrics,
            'training_time': total_time
        },
        'hyperparameters': best_params,
        'X': X_train,  # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®ç”¨äºSHAPåˆ†æ
        'y': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'y_train': y_train,  # æ·»åŠ è¿™ä¸€è¡Œ
        'device': device
    }
    
    # ğŸ”´ ç§»é™¤åŸºäºæ¨¡å‹å‚æ•°çš„ç‰¹å¾é‡è¦æ€§è®¡ç®— - ç¡®ä¿åªæœ‰GeoShapleyæˆåŠŸæ‰æœ‰feature_importance
    # result['feature_importance'] = feature_importance  # å·²åˆ é™¤ï¼Œç”±GeoShapleyè®¡ç®—
    
    # æ·»åŠ predictionsåˆ°resultï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if predictions is not None:
        result['predictions'] = predictions
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¹Ÿæ·»åŠ y_predå’Œy_testä½œä¸ºé¡¶çº§å­—æ®µ
        result['y_pred'] = predictions['mean']
        result['y_test'] = predictions['targets']
    
    # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šè·¯å¾„ï¼‰
    if model_path:
        save_stgpr_model(result, model_path)
        print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    return result

def save_stgpr_model(model_dict, model_path):
    """
    ä¿å­˜STGPRæ¨¡å‹åŠå…¶ç›¸å…³æ•°æ®
    
    å‚æ•°:
    model_dict: åŒ…å«æ¨¡å‹åŠç›¸å…³æ•°æ®çš„å­—å…¸
    model_path: ä¿å­˜è·¯å¾„
    
    è¿”å›:
    bool: æ˜¯å¦æˆåŠŸä¿å­˜
    """
    try:
        directory = os.path.dirname(model_path)
        if directory:
            success, _ = ensure_dir_exists(directory)
            if not success:
                return False
        
        # æå–éœ€è¦ä¿å­˜çš„æ•°æ®
        model = model_dict.get('model')
        likelihood = model_dict.get('likelihood')
        feature_names = model_dict.get('feature_names')
        scaler = model_dict.get('scaler')
        metrics = model_dict.get('metrics', {})
        
        if model is None or likelihood is None:
            return False
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPU
        model = model.cpu()
        likelihood = likelihood.cpu()
        
        # ä¿å­˜çŠ¶æ€å­—å…¸
        state_dict = {
            'model_state_dict': model.state_dict(),
            'likelihood_state_dict': likelihood.state_dict(),
            'feature_names': feature_names,
            'scaler': scaler,
            'metrics': metrics
        }
        
        torch.save(state_dict, model_path)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        print(traceback.format_exc())
        return False

# ä»stgpr_utilså¯¼å…¥explain_stgpr_predictionså‡½æ•°
from .stgpr_utils import explain_stgpr_predictions
