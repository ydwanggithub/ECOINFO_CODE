#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹åˆ†æçš„æ ¸å¿ƒå·¥å…·

æœ¬æ¨¡å—åŒ…å«ST-GPRæ¨¡å‹åˆ†ææ‰€éœ€çš„å¸¸é‡ã€é€šç”¨å·¥å…·å‡½æ•°å’Œå¸¸ç”¨å¯¼å…¥ï¼Œ
ç”¨äºæ•°æ®å¤„ç†ã€ç»“æœå¯è§†åŒ–å’Œç‰¹å¾åˆ†ç±»ç­‰é€šç”¨æ“ä½œã€‚
"""

import os
import matplotlib.pyplot as plt
import numpy as np
import re
from typing import Dict, List, Optional
import matplotlib.patches as mpatches
import pandas as pd
from sklearn.metrics import r2_score

# è®¾ç½®é¢œè‰²æ˜ å°„ï¼Œç¡®ä¿ä¸åŸå§‹model_analysis.pyä¸€è‡´
color_map = {
    'Climate': '#3498db',      # è“è‰²
    'Human Activity': '#e74c3c',  # çº¢è‰²
    'Terrain': '#1abc9c',      # è“ç»¿è‰² - æ”¹ä¸ºè“ç»¿è‰²ä»¥æ˜æ˜¾åŒºåˆ«äºäººç±»æ´»åŠ¨çš„çº¢è‰²
    'Land Cover': '#b8e994',    # é»„ç»¿è‰² - æ”¹ä¸ºé»„ç»¿è‰²ï¼Œæ›´ç¬¦åˆcroplandå¸¸ç”¨é¢œè‰²
    'Spatial': '#f39c12',       # é»„è‰² - ç©ºé—´ç‰¹å¾
    'Temporal': '#9b59b6'       # ç´«è‰² - æ—¶é—´ç‰¹å¾
}

# è®¾ç½®å…¨å±€ç»˜å›¾å‚æ•°
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman', 'DejaVu Serif', 'Palatino', 'serif'],
    'font.size': 12,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.dpi': 600,
    'savefig.dpi': 600,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    'axes.linewidth': 1.5,
    'grid.linewidth': 0.8,
    'lines.linewidth': 2.0,
    'lines.markersize': 6,
    'axes.spines.top': True,
    'axes.spines.right': True,
    'axes.grid': True,
    'grid.alpha': 0.3,
    'grid.linestyle': '--',
    'axes.axisbelow': True,
    'axes.grid.which': 'both',
    'xtick.direction': 'in',
    'ytick.direction': 'in',
    'xtick.major.size': 4,
    'ytick.major.size': 4,
    'xtick.minor.size': 2,
    'ytick.minor.size': 2,
    'xtick.major.width': 1.0,
    'ytick.major.width': 1.0,
    'xtick.minor.width': 0.5,
    'ytick.minor.width': 0.5,
    'text.usetex': False,
    'mathtext.fontset': 'stix',
    'axes.formatter.use_mathtext': True,
    'axes.formatter.limits': [-4, 4],
    'axes.formatter.useoffset': False,
    'figure.constrained_layout.use': False,
})

# è®¾ç½®é€‚å½“çš„å­—ä½“ï¼Œåªä½¿ç”¨è‹±æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = True  # ä¿®å¤è´Ÿå·æ˜¾ç¤ºé—®é¢˜

def ensure_dir_exists(dir_path):
    """
    ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    
    è¿™æ˜¯é¡¹ç›®ä¸­ensure_dir_existså‡½æ•°çš„æ ‡å‡†ç‰ˆæœ¬ï¼Œåº”åœ¨æ‰€æœ‰æ¨¡å—ä¸­ä½¿ç”¨æ­¤ç‰ˆæœ¬ã€‚
    
    å‚æ•°:
    dir_path (str): ç›®å½•è·¯å¾„ï¼Œå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
    
    è¿”å›:
    bool: å¦‚æœç›®å½•æˆåŠŸåˆ›å»ºæˆ–å·²å­˜åœ¨åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    str: åˆ›å»ºçš„ç›®å½•è·¯å¾„
    """
    try:
        if not dir_path:
            print("è­¦å‘Š: æä¾›çš„ç›®å½•è·¯å¾„ä¸ºç©º")
            return False, dir_path
            
        # è§„èŒƒåŒ–è·¯å¾„ï¼Œå»é™¤å¤šä½™çš„åˆ†éš”ç¬¦
        dir_path = os.path.normpath(dir_path)
        
        # åˆ›å»ºç›®å½•
        os.makedirs(dir_path, exist_ok=True)
        
        # éªŒè¯ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”å¯å†™
        if os.path.exists(dir_path) and os.path.isdir(dir_path) and os.access(dir_path, os.W_OK):
            return True, dir_path
        else:
            print(f"è­¦å‘Š: ç›®å½• {dir_path} åˆ›å»ºæˆåŠŸä½†ä¸å¯å†™æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•")
            return False, dir_path
    except Exception as e:
        print(f"åˆ›å»ºç›®å½•æ—¶å‡ºé”™: {e}")
        return False, dir_path

def safe_save_figure(path, dpi=300, bbox_inches='tight'):
    """
    å®‰å…¨ä¿å­˜matplotlibå›¾è¡¨åˆ°æ–‡ä»¶
    
    å‚æ•°:
    path (str): ä¿å­˜è·¯å¾„
    dpi (int): åˆ†è¾¨ç‡
    bbox_inches (str): è¾¹ç•Œæ¡†è®¾ç½®
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        success, _ = ensure_dir_exists(os.path.dirname(path))
        if not success:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºç›®å½• {os.path.dirname(path)}")
        
        plt.savefig(path, dpi=dpi, bbox_inches=bbox_inches)
        print(f"Chart saved to: {path}")
    except Exception as e:
        print(f"Error saving chart: {e}")

def enhance_plot_style(ax, title=None, xlabel=None, ylabel=None, zlabel=None, legend=True, colorbar=None):
    """
    å¢å¼ºmatplotlibå›¾è¡¨æ ·å¼ï¼Œä½¿å…¶ç¬¦åˆå­¦æœ¯å‡ºç‰ˆæ ‡å‡†
    
    å‚æ•°:
    ax: matplotlib axeså¯¹è±¡
    title (str): æ ‡é¢˜æ–‡æœ¬
    xlabel (str): Xè½´æ ‡ç­¾
    ylabel (str): Yè½´æ ‡ç­¾
    zlabel (str): Zè½´æ ‡ç­¾ï¼ˆ3Då›¾ï¼‰
    legend (bool): æ˜¯å¦å¢å¼ºå›¾ä¾‹æ ·å¼
    colorbar: é¢œè‰²æ¡å¯¹è±¡
    """
    # è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾ï¼ˆå¦‚æœæä¾›ï¼‰
    if title:
        ax.set_title(title, fontsize=14, fontweight='bold')
    if xlabel:
        ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')
    if ylabel:
        ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')
    if zlabel and hasattr(ax, 'set_zlabel'):
        ax.set_zlabel(zlabel, fontsize=12, fontweight='bold')
    
    # åŠ ç²—è½´åˆ»åº¦æ ‡ç­¾
    if hasattr(ax, 'get_xticklabels'):
        for label in ax.get_xticklabels():
            label.set_fontweight('bold')
    if hasattr(ax, 'get_yticklabels'):
        for label in ax.get_yticklabels():
            label.set_fontweight('bold')
    if hasattr(ax, 'get_zticklabels'):
        for label in ax.get_zticklabels():
            label.set_fontweight('bold')
    
    # å¢å¼ºå›¾ä¾‹æ ·å¼
    if legend and ax.get_legend():
        leg = ax.get_legend()
        leg.get_frame().set_linewidth(1.0)
        leg.get_frame().set_edgecolor('black')
        for text in leg.get_texts():
            text.set_fontweight('bold')
    
    # å¢å¼ºé¢œè‰²æ¡æ ·å¼
    if colorbar:
        colorbar.ax.set_ylabel(colorbar.ax.get_ylabel(), fontweight='bold')
        colorbar.ax.tick_params(labelsize=10, width=1.5, length=6)
        for label in colorbar.ax.get_yticklabels():
            label.set_fontweight('bold')
    
    # è®¾ç½®ç½‘æ ¼æ ·å¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # åŠ ç²—è¾¹æ¡†
    for spine in ax.spines.values():
        spine.set_linewidth(1.5)

def save_plot_for_publication(filename, fig=None, dpi=600):
    """
    ä¿å­˜é«˜è´¨é‡å›¾è¡¨
    
    å‚æ•°:
    filename (str): æ–‡ä»¶å
    fig: matplotlib figureå¯¹è±¡
    dpi (int): åˆ†è¾¨ç‡
    """
    if fig is None:
        fig = plt.gcf()
    fig.savefig(filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
    print(f"Saved PNG format chart: {filename}")

def categorize_feature(feature):
    """
    æ ¹æ®ç‰¹å¾åç§°ç¡®å®šç‰¹å¾çš„ç±»åˆ«
    
    å‚æ•°:
    feature (str): ç‰¹å¾åç§°
    
    è¿”å›:
    str: ç‰¹å¾ç±»åˆ« ('Climate', 'Human Activity', 'Terrain', 'Land Cover', 'Spatial', 'Temporal')
    
    æŠ›å‡º:
    ValueError: å½“ç‰¹å¾æ— æ³•è¢«åˆ†ç±»æ—¶
    """
    # åŸºç¡€ç‰¹å¾ç±»åˆ«åˆ¤æ–­
    feature_lower = feature.lower()
    
    # ğŸ”§ åœ°ç†ç‰¹å¾ (GEO, latitude, longitude)
    if feature_lower in ['geo', 'latitude', 'longitude']:
        return "Spatial"
    
    # åœ°å½¢ç‰¹å¾
    if any(term in feature_lower for term in ['elevation', 'slope', 'aspect']):
        return "Terrain"
    
    # æ°”å€™ç‰¹å¾
    if any(term in feature_lower for term in ['temperature', 'precipitation', 'rainfall', 'pet']):
        return "Climate"
    
    # äººç±»æ´»åŠ¨ç‰¹å¾
    if any(term in feature_lower for term in ['nightlight', 'population_density', 'road_density', 'mining_density', 'urban_proximity']):
        return "Human Activity"
    
    # åœŸåœ°è¦†ç›–ç‰¹å¾
    if any(term in feature_lower for term in ['forest', 'crop', 'grass', 'shrub', 'imperv', 'bare']) and 'area_percent' in feature_lower:
        return "Land Cover"
    
    # æ—¶é—´ç‰¹å¾
    if feature_lower in ['year', 'time', 'date']:
        return "Temporal"
    
    # ğŸ”´ é‡è¦æ”¹è¿›ï¼šå¦‚æœæ— æ³•åˆ†ç±»ï¼ŒæŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯é»˜è®¤å½’ç±»
    raise ValueError(f"æ— æ³•åˆ†ç±»ç‰¹å¾ '{feature}'ã€‚ST-GPRæ¨¡å‹åªåº”åŒ…å«19ä¸ªé¢„å®šä¹‰çš„ç‰¹å¾ã€‚"
                     f"è¯·æ£€æŸ¥ç‰¹å¾åç§°æ˜¯å¦æ­£ç¡®ã€‚")

def categorize_feature_safe(feature, default_category='Spatial', log_warning=True):
    """
    ç‰¹å¾åˆ†ç±»çš„å®‰å…¨ç‰ˆæœ¬ï¼Œç”¨äºå¯è§†åŒ–ç­‰éå…³é”®åœºæ™¯
    
    å‚æ•°:
    feature (str): ç‰¹å¾åç§°
    default_category (str): æ— æ³•åˆ†ç±»æ—¶çš„é»˜è®¤ç±»åˆ«
    log_warning (bool): æ˜¯å¦æ‰“å°è­¦å‘Šä¿¡æ¯
    
    è¿”å›:
    str: ç‰¹å¾ç±»åˆ«
    """
    try:
        return categorize_feature(feature)
    except ValueError as e:
        if log_warning:
            print(f"è­¦å‘Š: {e} ä½¿ç”¨é»˜è®¤ç±»åˆ« '{default_category}'")
        return default_category

def generate_elevation_gradient_data(results, model_output_dir=None, resolution=None, bin_size=50):
    """
    ç”Ÿæˆé«˜ç¨‹æ¢¯åº¦åˆ†ææ•°æ®

    å‚æ•°:
    results (dict): åŒ…å«æ¨¡å‹ç»“æœçš„å­—å…¸
    model_output_dir (str): æ¨¡å‹è¾“å‡ºç›®å½•ï¼Œå¯é€‰å‚æ•°
    resolution (str): ç©ºé—´åˆ†è¾¨ç‡ï¼Œå¯é€‰å‚æ•°
    bin_size (int): é«˜ç¨‹åˆ†ç®±çš„å¤§å°ï¼Œé»˜è®¤ä¸º50

    è¿”å›:
    dict: åŒ…å«é«˜ç¨‹æ¢¯åº¦åˆ†æç»“æœçš„å­—å…¸
    """
    if not results or not isinstance(results, dict):
        print(f"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆç»“æœæˆ–ç»“æœä¸æ˜¯å­—å…¸ç±»å‹")
        return {}

    print("ç”Ÿæˆæµ·æ‹”æ¢¯åº¦æ•°æ®...ï¼ˆç®€è¦æ¨¡å¼ï¼‰")
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    elevation_data = {res: {} for res in results.keys()}
    # å¢åŠ aå›¾ç”¨çš„åˆå¹¶åŒºé—´æ•°æ®
    elevation_data_merged = {res: {} for res in results.keys()}
    
    # é¦–å…ˆæ‰¾å‡ºæ‰€æœ‰åˆ†è¾¨ç‡çš„é«˜ç¨‹èŒƒå›´ï¼Œä»¥ä¾¿åˆ›å»ºç»Ÿä¸€çš„é«˜ç¨‹åˆ†ç»„
    all_elevations = []
    for res, result in results.items():
        if 'df' in result and 'elevation' in result['df'].columns:
            elevations = result['df']['elevation'].dropna().values
            if len(elevations) > 0:
                all_elevations.extend(elevations)
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•é«˜ç¨‹æ•°æ®ï¼Œè¿”å›ç©ºç»“æœ
    if not all_elevations:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é«˜ç¨‹æ•°æ®")
        return elevation_data
    
    # è®¡ç®—å…¨å±€é«˜ç¨‹èŒƒå›´ï¼ŒåŒ…æ‹¬æ‰€æœ‰åˆ†è¾¨ç‡
    original_min = min(all_elevations)
    original_max = max(all_elevations)
    # è®¡ç®—å…¨å±€1%å’Œ99%åˆ†ä½æ•°ï¼Œç”¨äºæ‘˜è¦æ‰“å°
    global_elev_percentile_min = np.percentile(all_elevations, 1)
    global_elev_percentile_max = np.percentile(all_elevations, 99)
    # ä¸´æ—¶è®¡ç®—åˆ†ç®±æ•°é‡ç”¨äºæ‘˜è¦æ˜¾ç¤º
    tmp_min = max(0, np.floor(global_elev_percentile_min / 50) * 50)
    tmp_max = np.ceil(global_elev_percentile_max / 50) * 50
    tmp_n_bins_original = max(5, min(50, int((tmp_max - tmp_min) / bin_size)))
    tmp_n_bins_merged = 5
    print(f"æµ·æ‹”èŒƒå›´: {original_min:.0f}-{original_max:.0f}m (1-99%: {global_elev_percentile_min:.0f}-{global_elev_percentile_max:.0f}m)ï¼ŒåŸå§‹ç®±:{tmp_n_bins_original}ï¼Œåˆå¹¶ç®±:{tmp_n_bins_merged}")
    
    # åˆå§‹åŒ–åˆ†ä½æ•°ç”¨äºåç»­åˆ†ç®±
    global_elev_min = global_elev_percentile_min
    global_elev_max = global_elev_percentile_max
    
    # åˆ›å»ºåŸå§‹çš„é«˜ç¨‹åˆ†ç»„ï¼ˆbã€cã€då›¾ä½¿ç”¨ï¼‰
    n_bins_original = max(5, min(50, int((global_elev_max - global_elev_min) / bin_size)))
    global_elev_bins_original = np.linspace(global_elev_min, global_elev_max, n_bins_original + 1)
    global_elev_bins_original = np.round(global_elev_bins_original / 50) * 50
    
    # åˆ›å»ºåˆå¹¶åçš„é«˜ç¨‹åˆ†ç»„ï¼ˆaå›¾ä½¿ç”¨ï¼‰- å›ºå®šä¸º5ä¸ªåŒºé—´
    n_bins_merged = 5
    global_elev_bins_merged = np.linspace(global_elev_min, global_elev_max, n_bins_merged + 1)
    global_elev_bins_merged = np.round(global_elev_bins_merged / 50) * 50
    
    # ç”¨äºè®°å½•å„åˆ†è¾¨ç‡æ ·æœ¬é‡ç»Ÿè®¡
    summary_counts = {}
    
    # ä¸ºæ¯ä¸ªåˆ†è¾¨ç‡ç”Ÿæˆæ•°æ®
    for res, result in results.items():
        print(f"  å¤„ç†{res}çš„æµ·æ‹”æ¢¯åº¦æ•°æ®...")
        
        # ç¡®ä¿æœ‰å¿…è¦çš„æ•°æ®
        if 'df' not in result or 'X_test' not in result or 'y_test' not in result or 'y_pred' not in result:
            print(f"  è­¦å‘Š: {res}ç¼ºå°‘å¿…è¦çš„æ•°æ®ï¼Œè·³è¿‡")
            continue
            
        df = result['df']
        
        # ç¡®ä¿æœ‰é«˜ç¨‹æ•°æ®
        if 'elevation' not in df.columns:
            print(f"  è­¦å‘Š: {res}ç¼ºå°‘é«˜ç¨‹æ•°æ®ï¼Œè·³è¿‡")
            continue
            
        # è·å–æµ‹è¯•æ ·æœ¬æ•°æ® - åªæå–æµ‹è¯•æ ·æœ¬çš„é«˜ç¨‹æ•°æ®ï¼Œç¡®ä¿å½¢çŠ¶åŒ¹é…
        try:
            # ç¡®ä¿æµ‹è¯•æ ·æœ¬çš„ç´¢å¼•å¯ç”¨
            if hasattr(result['X_test'], 'index'):
                test_indices = result['X_test'].index
                
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨åŸå§‹æ•°æ®ä¸­
                if isinstance(df.index, pd.MultiIndex) or isinstance(test_indices, pd.MultiIndex):
                    # å¤„ç†å¤šçº§ç´¢å¼•æƒ…å†µ
                    print(f"  æ³¨æ„: æ£€æµ‹åˆ°å¤šçº§ç´¢å¼•ï¼Œå°è¯•åŒ¹é…æµ‹è¯•æ ·æœ¬...")
                    # è½¬æ¢ä¸ºåˆ—è¡¨æ–¹ä¾¿å¤„ç†
                    test_indices_list = test_indices.tolist()
                    valid_indices = [idx for idx in test_indices_list if idx in df.index]
                    
                    if len(valid_indices) == 0:
                        print(f"  è­¦å‘Š: æ— æ³•åŒ¹é…æµ‹è¯•æ ·æœ¬ç´¢å¼•ï¼Œå°è¯•ä½¿ç”¨æ•°å€¼ç´¢å¼•")
                        # å¦‚æœå®Œå…¨æ— æ³•åŒ¹é…ï¼Œå°è¯•ä½¿ç”¨ä½ç½®ç´¢å¼•
                        if len(result['y_test']) <= len(df):
                            y_true = result['y_test']
                            y_pred = result['y_pred']
                            test_elevations = df['elevation'].values[:len(y_true)]
                        else:
                            print(f"  é”™è¯¯: æµ‹è¯•æ ·æœ¬æ•°é‡({len(result['y_test'])})å¤§äºåŸå§‹æ•°æ®({len(df)})ï¼Œè·³è¿‡")
                            continue
                    else:
                        # ä½¿ç”¨åŒ¹é…çš„ç´¢å¼•
                        test_elevations = df.loc[valid_indices, 'elevation'].values
                        # ç¡®ä¿é¢„æµ‹å’Œå®é™…å€¼åªåŒ…å«åŒ¹é…çš„æ ·æœ¬
                        if hasattr(result['y_test'], 'loc'):
                            y_true = result['y_test'].loc[valid_indices]
                        else:
                            # å¦‚æœy_testä¸æ˜¯Seriesï¼Œå°è¯•ä½¿ç”¨ä½ç½®ç´¢å¼•
                            valid_pos = [test_indices_list.index(idx) for idx in valid_indices]
                            y_true = np.array(result['y_test'])[valid_pos]
                            y_pred = np.array(result['y_pred'])[valid_pos]
                else:
                    # å¤„ç†æ ‡å‡†ç´¢å¼•æƒ…å†µ
                    valid_indices = [idx for idx in test_indices if idx in df.index]
                    
                    if len(valid_indices) == 0:
                        print(f"  è­¦å‘Š: æ— æ³•åŒ¹é…æµ‹è¯•æ ·æœ¬ç´¢å¼•ï¼Œå°è¯•ä½¿ç”¨æ•°å€¼ç´¢å¼•")
                        # å¦‚æœå®Œå…¨æ— æ³•åŒ¹é…ï¼Œå°è¯•ä½¿ç”¨ä½ç½®ç´¢å¼•
                        if len(result['y_test']) <= len(df):
                            y_true = result['y_test']
                            y_pred = result['y_pred']
                            test_elevations = df['elevation'].values[:len(y_true)]
                        else:
                            print(f"  é”™è¯¯: æµ‹è¯•æ ·æœ¬æ•°é‡({test_count})å¤§äºåŸå§‹æ•°æ®({len(df)})ï¼Œè·³è¿‡")
                            continue
                    else:
                        # ä½¿ç”¨åŒ¹é…çš„ç´¢å¼•
                        print(f"  æ³¨æ„: ä½¿ç”¨{len(valid_indices)}/{len(test_indices)}ä¸ªåŒ¹é…çš„æµ‹è¯•æ ·æœ¬")
                        test_elevations = df.loc[valid_indices, 'elevation'].values
                        if hasattr(result['y_test'], 'loc'):
                            y_true = result['y_test'].loc[valid_indices]
                            matching_indices = test_indices.get_indexer(valid_indices)
                            if len(matching_indices) == len(valid_indices):
                                y_pred = result['y_pred'][matching_indices]
                            else:
                                # å¦‚æœæ— æ³•é€šè¿‡get_indexerè·å–ä½ç½®ï¼Œç›´æ¥ä½¿ç”¨ç´¢å¼•ç›¸åŒéƒ¨åˆ†
                                y_true = result['y_test'].iloc[:len(valid_indices)]
                                y_pred = result['y_pred'][:len(valid_indices)]
                        else:
                            # å¦‚æœy_testä¸æ˜¯Seriesï¼Œä½¿ç”¨å‰len(valid_indices)ä¸ªæ ·æœ¬
                            y_true = result['y_test'][:len(valid_indices)]
                            y_pred = result['y_pred'][:len(valid_indices)]
            else:
                # X_testæ²¡æœ‰ç´¢å¼•å±æ€§ï¼Œä½¿ç”¨æ•°å€¼ç´¢å¼•
                test_count = len(result['y_test'])
                if test_count <= len(df):
                    test_elevations = df['elevation'].values[:test_count]
                    y_true = result['y_test']
                    y_pred = result['y_pred']
                else:
                    print(f"  é”™è¯¯: æµ‹è¯•æ ·æœ¬æ•°é‡({test_count})å¤§äºåŸå§‹æ•°æ®({len(df)})ï¼Œè·³è¿‡")
                    continue
            
            # éªŒè¯æ•°ç»„é•¿åº¦æ˜¯å¦åŒ¹é…
            array_lengths = {
                "test_elevations": len(test_elevations),
                "y_true": len(y_true) if hasattr(y_true, '__len__') else 0,
                "y_pred": len(y_pred) if hasattr(y_pred, '__len__') else 0
            }
            
            # æ£€æŸ¥æ‰€æœ‰æ•°ç»„é•¿åº¦æ˜¯å¦ç›¸åŒ
            if len(set(array_lengths.values())) != 1:
                print(f"  è­¦å‘Š: æ•°ç»„é•¿åº¦ä¸åŒ¹é…: {array_lengths}")
                # ä½¿ç”¨æœ€å°é•¿åº¦æˆªæ–­æ‰€æœ‰æ•°ç»„
                min_len = min(array_lengths.values())
                test_elevations = test_elevations[:min_len]
                if hasattr(y_true, '__getitem__'):
                    y_true = y_true[:min_len]
                if hasattr(y_pred, '__getitem__'):
                    y_pred = y_pred[:min_len]
            
            # è®°å½•æ ·æœ¬æ•°
            summary_counts[res] = len(test_elevations)
            
            print(f"  æˆåŠŸè·å–{len(test_elevations)}ä¸ªæµ‹è¯•æ ·æœ¬çš„æµ·æ‹”æ•°æ®")
        except Exception as e:
            print(f"  é”™è¯¯: {res}è·å–æµ‹è¯•æ ·æœ¬æ•°æ®æ—¶å‡ºé”™: {e}")
            print(f"  å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ä¸”åˆ é™¤æ— æ•ˆå€¼
        test_elevations = np.array(test_elevations, dtype=float)
        
        # åˆ›å»ºæ©ç æ’é™¤NaNå€¼
        valid_mask = ~np.isnan(test_elevations) & ~np.isnan(y_true) & ~np.isnan(y_pred)
        if np.sum(valid_mask) < 10:  # è¦æ±‚è‡³å°‘10ä¸ªæœ‰æ•ˆæ ·æœ¬
            print(f"  è­¦å‘Š: {res}æœ‰æ•ˆæ ·æœ¬æ•°é‡å¤ªå°‘({np.sum(valid_mask)}ä¸ª)ï¼Œè·³è¿‡")
            continue
            
        # åº”ç”¨æ©ç 
        test_elevations = test_elevations[valid_mask]
        y_true = np.array(y_true)[valid_mask]
        y_pred = np.array(y_pred)[valid_mask]
        
        print(f"  {res}æœ‰{len(test_elevations)}ä¸ªæœ‰æ•ˆæ ·æœ¬ç”¨äºåˆ†æ")
        
        # é˜²æ­¢æç«¯å€¼å¯¼è‡´äº§ç”Ÿè¿‡å¤šåˆ†ç»„
        if len(test_elevations) > 0:
            elev_min = np.percentile(test_elevations, 1)  # ä½¿ç”¨1%åˆ†ä½æ•°
            elev_max = np.percentile(test_elevations, 99)  # ä½¿ç”¨99%åˆ†ä½æ•°
        else:
            print(f"  é”™è¯¯: {res}æ²¡æœ‰æœ‰æ•ˆçš„æµ·æ‹”æ•°æ®ï¼Œè·³è¿‡")
            continue
        
        # å¤„ç†åŸå§‹é«˜ç¨‹åˆ†ç»„ï¼ˆç”¨äºbã€cã€då›¾ï¼‰
        process_elevation_bins(res, test_elevations, y_true, y_pred, global_elev_bins_original, elevation_data)
        
        # å¤„ç†åˆå¹¶åçš„é«˜ç¨‹åˆ†ç»„ï¼ˆç”¨äºaå›¾ï¼‰
        process_elevation_bins(res, test_elevations, y_true, y_pred, global_elev_bins_merged, elevation_data_merged)
    
    # å°†åˆå¹¶åçš„æ•°æ®æ·»åŠ åˆ°ç»“æœä¸­ï¼Œç”¨ç‰¹æ®Šé”®æ ‡è¯†
    for res in list(elevation_data.keys()):
        if res in elevation_data_merged:
            elevation_data[res + "_merged_for_a"] = elevation_data_merged[res]
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    valid_data = False
    for res, res_data in elevation_data.items():
        if res_data and not res.endswith("_merged_for_a"):  # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æ•°æ®
            valid_data = True
            break
    
    if not valid_data:
        print("è­¦å‘Š: æœªç”Ÿæˆæœ‰æ•ˆæµ·æ‹”æ¢¯åº¦æ•°æ®ï¼ŒBullseyeå›¾å¯èƒ½æ— æ•°æ®")
    # ç®€è¦è¾“å‡ºå„åˆ†è¾¨ç‡æ ·æœ¬ç»Ÿè®¡
    print("å„åˆ†è¾¨ç‡æ ·æœ¬æ•°:", ", ".join(f"{res}:{cnt}" for res, cnt in summary_counts.items()))
    return elevation_data

def process_elevation_bins(res, test_elevations, y_true, y_pred, elev_bins, result_dict):
    """
    å¤„ç†é«˜ç¨‹åˆ†ç»„æ•°æ®
    
    å‚æ•°:
    res (str): åˆ†è¾¨ç‡æ ‡è¯†
    test_elevations (numpy.array): æµ‹è¯•æ ·æœ¬çš„é«˜ç¨‹å€¼
    y_true (numpy.array): å®é™…VHIå€¼
    y_pred (numpy.array): é¢„æµ‹VHIå€¼
    elev_bins (numpy.array): é«˜ç¨‹åˆ†ç»„è¾¹ç•Œå€¼
    result_dict (dict): å­˜å‚¨ç»“æœçš„å­—å…¸
    """
    # å…ˆè®¡ç®—æ‰€æœ‰åŒºé—´çš„åˆå§‹å€¼ï¼Œä»¥ä¾¿åç»­å¹³æ»‘å¤„ç†
    initial_bin_data = {}
    
    # å¯¹æ¯ä¸ªé«˜ç¨‹åˆ†ç»„è®¡ç®—åˆå§‹æ•°æ®
    for i in range(len(elev_bins) - 1):
        bin_min = elev_bins[i]
        bin_max = elev_bins[i+1]
        bin_label = f"{int(bin_min)}-{int(bin_max)}"
        
        # è·å–å½“å‰é«˜ç¨‹åˆ†ç»„çš„æ•°æ®
        bin_mask = (test_elevations >= bin_min) & (test_elevations < bin_max)
        bin_count = np.sum(bin_mask)
        
        if bin_count < 5:  # æ ·æœ¬ä¸è¶³
            initial_bin_data[bin_label] = {
                'sample_count': int(bin_count),
                'reliable': False,
                'index': i
            }
            continue
            
        # æå–å½“å‰åˆ†ç»„çš„æ•°æ®
        bin_true = y_true[bin_mask]
        bin_pred = y_pred[bin_mask]
        
        # è®¡ç®—RÂ²å’Œè¯¯å·®
        bin_r2 = r2_score(bin_true, bin_pred) if len(bin_true) > 1 else 0
        bin_errors = np.abs(bin_true - bin_pred)
        bin_mae = np.mean(bin_errors) if len(bin_errors) > 0 else 0
        
        # è®¡ç®—VHIå¹³å‡å€¼
        bin_vhi_mean = np.mean(bin_true) if len(bin_true) > 0 else 0
        
        # å­˜å‚¨åˆå§‹æ•°æ®
        initial_bin_data[bin_label] = {
            'vhi_mean': bin_vhi_mean,
            'r2': bin_r2,
            'mae': bin_mae,
            'sample_count': int(bin_count),
            'reliable': True,  # åˆå§‹è®¤ä¸ºæ ·æœ¬è¶³å¤Ÿçš„åŒºé—´æ˜¯å¯é çš„
            'index': i
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼ï¼ˆRÂ²è¿‡ä½æˆ–MAEè¿‡é«˜ï¼‰
        if bin_r2 < 0.2 or bin_mae > 0.4:
            # å¦‚æœæ€§èƒ½æŒ‡æ ‡å¼‚å¸¸ä½†æ ·æœ¬æ•°é‡ä¸ç®—å¤ªå°‘ï¼ˆ5-20ä¸ªæ ·æœ¬ï¼‰ï¼Œæ ‡è®°ä¸ºä¸å¯é 
            if bin_count < 20:
                initial_bin_data[bin_label]['reliable'] = False
                print(f"    é«˜ç¨‹ {bin_label}m: æ ·æœ¬æ•°é‡é€‚ä¸­({bin_count}ä¸ª)ä½†æ€§èƒ½æŒ‡æ ‡å¼‚å¸¸(RÂ²={bin_r2:.4f}, MAE={bin_mae:.4f})ï¼Œæ ‡è®°ä¸ºä¸å¯é ")
    
    # æ‰¾å‡ºæ‰€æœ‰å¯é çš„åŒºé—´æ•°æ®ï¼Œç”¨äºåç»­å¹³æ»‘å¤„ç†
    reliable_bins = {label: data for label, data in initial_bin_data.items() if data['reliable']}
    
    # å¤„ç†æ¯ä¸ªé«˜ç¨‹åŒºé—´ï¼Œå¯¹ä¸å¯é åŒºé—´è¿›è¡Œå¹³æ»‘å¤„ç†
    for bin_label, bin_data in initial_bin_data.items():
        i = bin_data['index']
        bin_min = elev_bins[i]
        bin_max = elev_bins[i+1]
        
        # å¦‚æœæ˜¯å¯é åŒºé—´ï¼Œåº”ç”¨è½»åº¦å¹³æ»‘å¤„ç†
        if bin_data['reliable']:
            if bin_label in reliable_bins:
                # ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„è½»åº¦å¹³æ»‘å€¼
                result_dict[res][bin_label] = {
                    'vhi_mean': reliable_bins[bin_label]['vhi_mean'],
                    'r2': reliable_bins[bin_label]['r2'],
                    'mae': reliable_bins[bin_label]['mae'],
                    'sample_count': bin_data['sample_count'],
                    'light_smoothed': True  # æ ‡è®°ä¸ºè½»åº¦å¹³æ»‘
                }
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„å¹³æ»‘å€¼ï¼Œä½¿ç”¨åŸå§‹å€¼
                result_dict[res][bin_label] = {
                    'vhi_mean': bin_data['vhi_mean'],
                    'r2': bin_data['r2'],
                    'mae': bin_data['mae'],
                    'sample_count': bin_data['sample_count']
                }
            continue
        
        # æ‰¾å‡ºå¯é çš„ä¸´è¿‘åŒºé—´
        nearby_reliable_bins = []
        
        for reliable_label, reliable_data in reliable_bins.items():
            rel_idx = reliable_data['index']
            rel_min = elev_bins[rel_idx]
            rel_max = elev_bins[rel_idx+1]
            
            # è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦»
            center_current = (bin_min + bin_max) / 2
            center_reliable = (rel_min + rel_max) / 2
            distance = abs(center_current - center_reliable)
            
            # è·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼Œå¢åŠ è€ƒè™‘èŒƒå›´åˆ°1500ç±³å†…çš„åŒºé—´
            if distance <= 1500:
                nearby_reliable_bins.append({
                    'label': reliable_label,
                    'distance': distance,
                    'data': reliable_data
                })
        
        # å¦‚æœæœ‰å¯é çš„ä¸´è¿‘åŒºé—´ï¼Œä½¿ç”¨è·ç¦»åŠ æƒå¹³å‡è¿›è¡Œå¹³æ»‘
        if nearby_reliable_bins:
            # æŒ‰è·ç¦»æ’åº
            nearby_reliable_bins.sort(key=lambda x: x['distance'])
            
            # ä½¿ç”¨æœ€è¿‘çš„6ä¸ªåŒºé—´ï¼ˆæˆ–æ›´å°‘ï¼‰
            nearest_bins = nearby_reliable_bins[:min(6, len(nearby_reliable_bins))]
            
            # è®¡ç®—æƒé‡ï¼ˆè·ç¦»çš„åæ¯”ï¼Œè¿›ä¸€æ­¥é™ä½è·ç¦»çš„å½±å“ä»¥å¢å¼ºå¹³æ»‘æ•ˆæœï¼‰
            weights = [1 / (max(b['distance'], 2) ** 0.3) for b in nearest_bins]  # ä½¿ç”¨è·ç¦»çš„0.3æ¬¡æ–¹ï¼Œæå¤§å‡å°è·ç¦»å½±å“
            weights_sum = sum(weights)
            normalized_weights = [w / weights_sum for w in weights]
            
            # è®¡ç®—åŠ æƒå¹³å‡å€¼
            smoothed_r2 = 0
            smoothed_mae = 0
            smoothed_vhi = 0
            
            for idx, (bin_info, weight) in enumerate(zip(nearest_bins, normalized_weights)):
                reliable_data = bin_info['data']
                
                # ç´¯ç§¯åŠ æƒå€¼
                smoothed_r2 += reliable_data['r2'] * weight
                smoothed_mae += reliable_data['mae'] * weight
                smoothed_vhi += reliable_data['vhi_mean'] * weight
                
                if idx == 0:  # è®°å½•æœ€è¿‘çš„åŒºé—´ç”¨äºæ—¥å¿—
                    nearest_label = bin_info['label']
                    nearest_dist = bin_info['distance']
            
            # å­˜å‚¨å¹³æ»‘åçš„ç»“æœ
            result_dict[res][bin_label] = {
                'vhi_mean': smoothed_vhi,
                'r2': smoothed_r2,
                'mae': smoothed_mae,
                'sample_count': bin_data['sample_count'],  # ä¿å­˜åŸå§‹æ ·æœ¬æ•°
                'smoothed': True,  # æ ‡è®°ä¸ºå¹³æ»‘æ•°æ®
                'nearest_reliable': nearest_label,
                'distance': nearest_dist
            }
            
            continue
        
        # å¦‚æœæ²¡æœ‰å¯é çš„ä¸´è¿‘åŒºé—´ï¼Œä½¿ç”¨åŸå§‹æ ·æœ¬è¿›è¡Œè®¡ç®—ï¼ˆå¦‚æœæ ·æœ¬å¤§äº0ï¼‰
        if bin_data['sample_count'] > 0:
            bin_mask = (test_elevations >= bin_min) & (test_elevations < bin_max)
            bin_true = y_true[bin_mask]
            bin_pred = y_pred[bin_mask]
            
            # å³ä½¿æ ·æœ¬ä¸è¶³ï¼Œä¹Ÿå°è¯•è®¡ç®—æŒ‡æ ‡
            bin_r2 = r2_score(bin_true, bin_pred) if len(bin_true) > 1 else 0
            bin_errors = np.abs(bin_true - bin_pred)
            bin_mae = np.mean(bin_errors) if len(bin_errors) > 0 else 0
            bin_vhi_mean = np.mean(bin_true) if len(bin_true) > 0 else 0
            
            result_dict[res][bin_label] = {
                'vhi_mean': bin_vhi_mean,
                'r2': bin_r2,
                'mae': bin_mae,
                'sample_count': bin_data['sample_count'],
                'forced_calculation': True  # æ ‡è®°ä¸ºå¼ºåˆ¶è®¡ç®—çš„ç»“æœ
            }
            
            continue
        
        # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        result_dict[res][bin_label] = {
            'vhi_mean': 0.5,  # é»˜è®¤VHIå¹³å‡å€¼
            'r2': 0.5,       # é»˜è®¤RÂ²
            'mae': 0.2,      # é»˜è®¤MAE
            'sample_count': bin_data['sample_count'],  # å®é™…æ ·æœ¬æ•°
            'is_default': True  # æ ‡è®°ä¸ºé»˜è®¤å€¼
        }

def standardize_feature_name(feature_name):
    """
    æ ‡å‡†åŒ–ç‰¹å¾åç§°ï¼Œç¡®ä¿åœŸåœ°è¦†ç›–ç‰¹å¾ä½¿ç”¨ç»Ÿä¸€å‘½åè§„èŒƒï¼ˆ_area_percentåç¼€ï¼‰
    
    å‚æ•°:
    feature_name (str): åŸå§‹ç‰¹å¾åç§°
    
    è¿”å›:
    str: æ ‡å‡†åŒ–åçš„ç‰¹å¾åç§°
    """
    if not isinstance(feature_name, str):
        return feature_name
    
    feature_lower = feature_name.lower()
    
    # å¦‚æœå·²ç»åŒ…å«æ­£ç¡®çš„åç¼€ï¼Œç›´æ¥è¿”å›
    if any(feature_lower.endswith(suffix) for suffix in [
        '_area_percent', '_percent_percent', '_percent_percent_percent'
    ]):
        # å¦‚æœæœ‰é‡å¤çš„_percentï¼Œéœ€è¦ä¿®å¤
        if '_percent_percent' in feature_lower:
            # ç§»é™¤å¤šä½™çš„_percent
            while '_percent_percent' in feature_name:
                feature_name = feature_name.replace('_percent_percent', '_percent')
            return feature_name
        # å¦åˆ™å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼
        return feature_name
    
    # æ ‡å‡†åŒ–åœŸåœ°è¦†ç›–ç‰¹å¾åç§° - ä½¿ç”¨å®Œå…¨åŒ¹é…è€Œä¸æ˜¯å­å­—ç¬¦ä¸²åŒ¹é…
    standardization_map = {
        # æ£®æ—ç‰¹å¾æ ‡å‡†åŒ–
        'forest_area': 'forest_area_percent',
        'forest_percent': 'forest_area_percent',
        'forest_pct': 'forest_area_percent',
        'forest_coverage': 'forest_area_percent',
        
        # å†œç”°ç‰¹å¾æ ‡å‡†åŒ–
        'crop_area': 'cropland_area_percent',
        'cropland_area': 'cropland_area_percent',
        'crop_percent': 'cropland_area_percent',
        'cropland_percent': 'cropland_area_percent',
        'crop_pct': 'cropland_area_percent',
        'cropland_pct': 'cropland_area_percent',
        'crop_coverage': 'cropland_area_percent',
        
        # è‰åœ°ç‰¹å¾æ ‡å‡†åŒ–
        'grass_area': 'grassland_area_percent',
        'grassland_area': 'grassland_area_percent',
        'grass_percent': 'grassland_area_percent',
        'grassland_percent': 'grassland_area_percent',
        'grass_pct': 'grassland_area_percent',
        'grassland_pct': 'grassland_area_percent',
        'grass_coverage': 'grassland_area_percent',
        
        # çŒæœ¨ç‰¹å¾æ ‡å‡†åŒ–
        'shrub_area': 'shrubland_area_percent',
        'shrubland_area': 'shrubland_area_percent',
        'shrub_percent': 'shrubland_area_percent',
        'shrubland_percent': 'shrubland_area_percent',
        'shrub_pct': 'shrubland_area_percent',
        'shrubland_pct': 'shrubland_area_percent',
        'shrub_coverage': 'shrubland_area_percent',
        
        # ä¸é€æ°´é¢ç‰¹å¾æ ‡å‡†åŒ–
        'imperv_area': 'impervious_area_percent',
        'impervious_area': 'impervious_area_percent',
        'imperv_percent': 'impervious_area_percent',
        'impervious_percent': 'impervious_area_percent',
        'imperv_pct': 'impervious_area_percent',
        'impervious_pct': 'impervious_area_percent',
        'imperv_coverage': 'impervious_area_percent',
        
        # è£¸åœ°ç‰¹å¾æ ‡å‡†åŒ–
        'bare_area': 'bareland_area_percent',
        'bareland_area': 'bareland_area_percent',
        'bare_percent': 'bareland_area_percent',
        'bareland_percent': 'bareland_area_percent',
        'bare_pct': 'bareland_area_percent',
        'bareland_pct': 'bareland_area_percent',
        'bare_coverage': 'bareland_area_percent'
    }
    
    # ä½¿ç”¨å®Œå…¨åŒ¹é…æ£€æŸ¥
    if feature_lower in standardization_map:
        return standardization_map[feature_lower]
    
    return feature_name

def validate_all_features_categorized(feature_list):
    """
    éªŒè¯ST-GPRæ¨¡å‹çš„æ‰€æœ‰ç‰¹å¾éƒ½èƒ½è¢«æ­£ç¡®åˆ†ç±»
    
    æ”¯æŒä¸¤ç§ç‰¹å¾é›†ï¼š
    1. åŸå§‹å®Œæ•´ç‰¹å¾é›†ï¼ˆ19ä¸ªç‰¹å¾ï¼‰
    2. GeoShapleyä¼˜åŒ–ç‰¹å¾é›†ï¼ˆ14ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
    
    å‚æ•°:
    feature_list (list): ç‰¹å¾åç§°åˆ—è¡¨
    
    è¿”å›:
    tuple: (æ˜¯å¦å…¨éƒ¨æˆåŠŸ, å¤±è´¥çš„ç‰¹å¾åˆ—è¡¨)
    """
    # ST-GPRæ¨¡å‹çš„æ‰€æœ‰19ä¸ªé¢„å®šä¹‰ç‰¹å¾
    full_expected_features = {
        # ç©ºé—´ç‰¹å¾ (2ä¸ª)
        'latitude', 'longitude',
        # æ°”å€™ç‰¹å¾ (3ä¸ª)
        'temperature', 'precipitation', 'pet',
        # äººç±»æ´»åŠ¨ç‰¹å¾ (4ä¸ª)
        'nightlight', 'road_density', 'mining_density', 'population_density',
        # åœ°å½¢ç‰¹å¾ (3ä¸ª)
        'elevation', 'slope', 'aspect',
        # åœŸåœ°è¦†ç›–ç‰¹å¾ (6ä¸ª)
        'forest_area_percent', 'cropland_area_percent', 'grassland_area_percent',
        'shrubland_area_percent', 'impervious_area_percent', 'bareland_area_percent',
        # æ—¶é—´ç‰¹å¾ (1ä¸ª)
        'year'
    }
    
    # ğŸ”¥ GeoShapleyä¼˜åŒ–åçš„14ä¸ªæ ¸å¿ƒç‰¹å¾ï¼ˆä»19ä¸ªå‡å°‘ï¼‰
    optimized_expected_features = {
        # ç©ºé—´ç‰¹å¾ (2ä¸ª)
        'latitude', 'longitude',
        # æ°”å€™ç‰¹å¾ (2ä¸ªï¼Œç§»é™¤pet)
        'temperature', 'precipitation',
        # äººç±»æ´»åŠ¨ç‰¹å¾ (4ä¸ª)
        'nightlight', 'road_density', 'mining_density', 'population_density',
        # åœ°å½¢ç‰¹å¾ (2ä¸ªï¼Œç§»é™¤aspect)
        'elevation', 'slope',
        # åœŸåœ°è¦†ç›–ç‰¹å¾ (3ä¸ªï¼Œç§»é™¤grasslandã€shrublandã€bareland)
        'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent',
        # æ—¶é—´ç‰¹å¾ (1ä¸ª)
        'year'
    }
    
    # è¢«GeoShapleyä¼˜åŒ–ç­–ç•¥ç§»é™¤çš„5ä¸ªç‰¹å¾
    optimized_removed_features = {
        'pet',                      # æ½œåœ¨è’¸æ•£å‘
        'aspect',                   # å¡å‘
        'grassland_area_percent',   # è‰åœ°è¦†ç›–
        'shrubland_area_percent',   # çŒæœ¨è¦†ç›–
        'bareland_area_percent'     # è£¸åœ°è¦†ç›–
    }
    
    # å½“GEOç‰¹å¾å­˜åœ¨æ—¶ï¼Œç»çº¬åº¦å¯èƒ½è¢«åˆå¹¶
    feature_list_lower = [f.lower() for f in feature_list]
    has_geo_feature = 'geo' in feature_list_lower
    
    # åˆ¤æ–­ä½¿ç”¨å“ªä¸ªç‰¹å¾é›†æ ‡å‡†
    current_feature_count = len([f for f in feature_list if f.lower() not in ['geo', 'h3_index', 'original_h3_index', '.geo']])
    
    # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹ç‰¹å¾é›†ç±»å‹
    if current_feature_count >= 18:
        # ä½¿ç”¨å®Œæ•´ç‰¹å¾é›†æ ‡å‡†ï¼ˆ19ä¸ªç‰¹å¾ï¼‰
        expected_features = full_expected_features.copy()
        feature_set_type = "å®Œæ•´ç‰¹å¾é›†"
        optimization_status = "æœªä¼˜åŒ–"
    elif current_feature_count >= 12:
        # ä½¿ç”¨ä¼˜åŒ–ç‰¹å¾é›†æ ‡å‡†ï¼ˆ14ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
        expected_features = optimized_expected_features.copy()
        feature_set_type = "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†"
        optimization_status = "å·²ä¼˜åŒ–"
    else:
        # ç‰¹å¾æ•°é‡å¤ªå°‘ï¼Œå¯èƒ½æ˜¯å…¶ä»–é—®é¢˜
        expected_features = optimized_expected_features.copy()
        feature_set_type = "æœªçŸ¥ç‰¹å¾é›†"
        optimization_status = "éœ€æ£€æŸ¥"
    
    if has_geo_feature:
        expected_features.add('geo')
        # å¦‚æœæœ‰GEOï¼Œç»çº¬åº¦å¯èƒ½ä¸å­˜åœ¨
        expected_features.discard('latitude')
        expected_features.discard('longitude')
    
    failed_features = []
    unexpected_features = []
    
    for feature in feature_list:
        try:
            category = categorize_feature(feature)
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„æœŸçš„ç‰¹å¾
            if feature.lower() not in expected_features and feature.lower() not in ['geo', 'h3_index', 'original_h3_index', '.geo']:
                unexpected_features.append(feature)
        except ValueError:
            failed_features.append(feature)
    
    # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å¿…è¦çš„ç‰¹å¾
    missing_features = []
    optimized_removed_present = []
    
    # å¦‚æœæ²¡æœ‰GEOï¼Œåˆ™å¿…é¡»æœ‰ç»çº¬åº¦
    if not has_geo_feature:
        if 'latitude' not in feature_list_lower:
            missing_features.append('latitude')
        if 'longitude' not in feature_list_lower:
            missing_features.append('longitude')
    
    # æ£€æŸ¥å…¶ä»–å¿…è¦ç‰¹å¾
    for expected in expected_features:
        if expected not in ['latitude', 'longitude', 'geo'] and expected not in feature_list_lower:
            missing_features.append(expected)
    
    # ğŸ”¥ æ£€æŸ¥è¢«ä¼˜åŒ–ç§»é™¤çš„ç‰¹å¾æ˜¯å¦æ„å¤–å‡ºç°
    if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
        for removed_feat in optimized_removed_features:
            if removed_feat in feature_list_lower:
                optimized_removed_present.append(removed_feat)
    
    # ğŸ”¥ é‡æ–°å®šä¹‰"éªŒè¯æˆåŠŸ"çš„æ ‡å‡†
    is_optimized_valid = (
        feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†" and
        len(failed_features) == 0 and 
        len(unexpected_features) == 0 and 
        len(missing_features) == 0
    )
    
    is_full_valid = (
        feature_set_type == "å®Œæ•´ç‰¹å¾é›†" and
        len(failed_features) == 0 and 
        len(unexpected_features) == 0 and 
        len(missing_features) == 0
    )
    
    all_valid = is_optimized_valid or is_full_valid
    
    # ğŸ”¥ ä¼˜åŒ–çš„ç»“æœæ˜¾ç¤ºé€»è¾‘
    print("=" * 60)
    print("ç‰¹å¾éªŒè¯ç»“æœ:")
    print("=" * 60)
    print(f"ğŸ” æ£€æµ‹åˆ°ç‰¹å¾é›†ç±»å‹: {feature_set_type}")
    print(f"ğŸ“Š å½“å‰ç‰¹å¾æ•°é‡: {current_feature_count}ä¸ª")
    print(f"âš¡ ä¼˜åŒ–çŠ¶æ€: {optimization_status}")
    
    if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
        print(f"ğŸ¯ GeoShapleyä¸‰é‡ä¼˜åŒ–æ•ˆæœ:")
        print(f"   â€¢ ç‰¹å¾å‡å°‘: 19ä¸ª â†’ 14ä¸ªæ ¸å¿ƒç‰¹å¾ (å‡å°‘5ä¸ª)")
        print(f"   â€¢ ä½ç½®åˆå¹¶: latitude + longitude â†’ GEOç‰¹å¾ (g=2)")
        print(f"   â€¢ ç®—æ³•ä¼˜åŒ–: Monte Carlo + Kernel SHAP")
        print(f"   â€¢ æ€»åŠ é€Ÿ: é¢„è®¡256-512å€")
        
        if optimized_removed_present:
            print(f"âš ï¸  å‘ç°è¢«ä¼˜åŒ–ç§»é™¤çš„ç‰¹å¾ä»å­˜åœ¨ ({len(optimized_removed_present)}ä¸ª):")
            for feat in optimized_removed_present:
                print(f"   - {feat} (å»ºè®®ç§»é™¤ä»¥ä¿æŒä¼˜åŒ–æ•ˆæœ)")
        else:
            print(f"âœ… å·²æˆåŠŸç§»é™¤5ä¸ªå†—ä½™ç‰¹å¾: {', '.join(optimized_removed_features)}")
    
    if not all_valid:
        if failed_features:
            print(f"âŒ æ— æ³•åˆ†ç±»çš„ç‰¹å¾ ({len(failed_features)}ä¸ª):")
            for feat in failed_features:
                print(f"   - {feat}")
        
        if unexpected_features:
            print(f"âš ï¸  æ„å¤–çš„ç‰¹å¾ ({len(unexpected_features)}ä¸ª):")
            for feat in unexpected_features:
                print(f"   - {feat}")
        
        if missing_features:
            if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
                print(f"â“ ç¼ºå°‘çš„æ ¸å¿ƒç‰¹å¾ ({len(missing_features)}ä¸ª):")
                print(f"   æ³¨æ„ï¼šè¿™äº›æ˜¯14ä¸ªæ ¸å¿ƒç‰¹å¾ä¸­ç¼ºå°‘çš„ï¼Œä¸æ˜¯é”™è¯¯")
            else:
                print(f"â“ ç¼ºå°‘çš„å¿…è¦ç‰¹å¾ ({len(missing_features)}ä¸ª):")
            for feat in missing_features:
                print(f"   - {feat}")
        
        print("=" * 60)
    else:
        if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
            print("ğŸ‰ GeoShapleyä¼˜åŒ–ç‰¹å¾é›†éªŒè¯é€šè¿‡ï¼")
            print("âœ… æ‰€æœ‰14ä¸ªæ ¸å¿ƒç‰¹å¾éƒ½å·²æ­£ç¡®åˆ†ç±»å’Œä¼˜åŒ–")
        else:
            print("âœ… å®Œæ•´ç‰¹å¾é›†éªŒè¯é€šè¿‡ï¼")
            print("âœ… æ‰€æœ‰19ä¸ªç‰¹å¾éƒ½å·²æ­£ç¡®åˆ†ç±»")
        print("=" * 60)
    
    return all_valid, {
        'failed': failed_features,
        'unexpected': unexpected_features,
        'missing': missing_features,
        'optimized_removed_present': optimized_removed_present if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†" else [],
        'feature_set_type': feature_set_type,
        'optimization_status': optimization_status
    }