#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹ (ST-GPR) - å¯è§†åŒ–è½¬æ¢å™¨æ¨¡å—

è¯¥æ¨¡å—ä¸ºST-GPRæ¨¡å‹æä¾›æ•°æ®è½¬æ¢åŠŸèƒ½ï¼Œå°†ST-GPRæ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå¯è§†åŒ–æ¨¡å—æ‰€éœ€çš„æ ¼å¼ï¼š
1. ç¡®ä¿æ•°æ®ç»“æ„ä¸XGBoostå¯è§†åŒ–æ¨¡å—å…¼å®¹
2. å¤„ç†ST-GPRç‰¹æœ‰çš„è¾“å‡ºæ ¼å¼
3. ç”Ÿæˆ10ä¸ªæ ‡å‡†å¯è§†åŒ–å›¾è¡¨
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
import json
import warnings
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.neighbors import KNeighborsRegressor
import traceback

# ç¡®ä¿visualizationç›®å½•åœ¨Pythonè·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
visualization_dir = os.path.join(parent_dir, 'visualization')
if os.path.exists(visualization_dir) and visualization_dir not in sys.path:
    sys.path.insert(0, visualization_dir)
    print(f"å·²æ·»åŠ visualizationç›®å½•åˆ°Pythonè·¯å¾„: {visualization_dir}")

# ä»visualization.utilså¯¼å…¥ç®€åŒ–ç‰¹å¾åç§°çš„å‡½æ•°
try:
    from visualization.utils import simplify_feature_name_for_plot, categorize_feature
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥visualization.utilsæ¨¡å—ï¼ŒæŸäº›ç‰¹å¾æ ¼å¼åŒ–åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    # å®šä¹‰ç®€å•çš„å¤‡ç”¨å‡½æ•°
    def simplify_feature_name_for_plot(name, max_length=None):
        return name[:max_length] if max_length and len(name) > max_length else name
    def categorize_feature(name):
        return "Unknown"

# ä»coreæ¨¡å—å¯¼å…¥ensure_dir_existså‡½æ•°
from .core import ensure_dir_exists

def convert_stgpr_to_visualization_format(stgpr_results):
    """
    å°†ST-GPRæ¨¡å‹ç»“æœè½¬æ¢ä¸ºå¯è§†åŒ–æ¨¡å—æ‰€éœ€çš„æ ¼å¼
    
    å‚æ•°:
    stgpr_results: ST-GPRæ¨¡å‹çš„åŸå§‹è¾“å‡º
    
    è¿”å›:
    dict: è½¬æ¢åçš„ç»“æœï¼Œå…¼å®¹å¯è§†åŒ–æ¨¡å—
    """
    converted_results = {}
    
    for res in stgpr_results:
        if stgpr_results[res] is None:
            continue
            
        result = stgpr_results[res]
        converted_results[res] = {}
        
        # 1. åŸºæœ¬ä¿¡æ¯è½¬æ¢
        # æ¨¡å‹ç±»å‹æ ‡è®°ä¸ºSTGPR
        converted_results[res]['model_type'] = 'STGPR'
        converted_results[res]['resolution'] = res
        
        # 2. å¤„ç†é¢„æµ‹ç»“æœå’Œåº¦é‡æŒ‡æ ‡
        if 'predictions' in result:
            predictions = result['predictions']
            
            # ç¡®ä¿æœ‰y_testå’Œy_pred
            # ST-GPRæ¨¡å‹è¿”å›çš„æ˜¯'targets'å’Œ'mean'ï¼Œéœ€è¦è½¬æ¢
            if 'targets' in predictions and 'mean' in predictions:
                converted_results[res]['y_test'] = np.array(predictions['targets'])
                converted_results[res]['y_pred'] = np.array(predictions['mean'])
            elif 'y_true' in predictions and 'y_pred' in predictions:
                converted_results[res]['y_test'] = np.array(predictions['y_true'])
                converted_results[res]['y_pred'] = np.array(predictions['y_pred'])
            
            # è®¡ç®—æˆ–å¤åˆ¶åº¦é‡æŒ‡æ ‡
            if 'y_test' in converted_results[res] and 'y_pred' in converted_results[res]:
                y_test = converted_results[res]['y_test']
                y_pred = converted_results[res]['y_pred']
                
                # è®¡ç®—test_metrics
                converted_results[res]['test_metrics'] = {
                    'r2': r2_score(y_test, y_pred),
                    'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                    'mae': mean_absolute_error(y_test, y_pred)
                }
                
                # ä¹Ÿä¿å­˜åœ¨metricså­—æ®µä¸­
                converted_results[res]['metrics'] = converted_results[res]['test_metrics'].copy()
        
        # ç›´æ¥æ£€æŸ¥y_predå­—æ®µ
        if 'y_pred' in result and 'y_pred' not in converted_results[res]:
            converted_results[res]['y_pred'] = np.array(result['y_pred'])
        
        # ç›´æ¥æ£€æŸ¥y_testå­—æ®µ
        if 'y_test' in result and 'y_test' not in converted_results[res]:
            converted_results[res]['y_test'] = np.array(result['y_test'])
        
        # å¦‚æœæœ‰y_testå’Œy_predä½†æ²¡æœ‰metricsï¼Œè®¡ç®—metrics
        if 'y_test' in converted_results[res] and 'y_pred' in converted_results[res] and 'test_metrics' not in converted_results[res]:
            y_test = converted_results[res]['y_test']
            y_pred = converted_results[res]['y_pred']
            
            converted_results[res]['test_metrics'] = {
                'r2': r2_score(y_test, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'mae': mean_absolute_error(y_test, y_pred)
            }
            converted_results[res]['metrics'] = converted_results[res]['test_metrics'].copy()
        
        # å¦‚æœæœ‰metricså­—æ®µï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        elif 'metrics' in result:
            # ST-GPRæ¨¡å‹å¯èƒ½ä½¿ç”¨test_r2/test_rmseç­‰é”®å
            metrics = result['metrics']
            standardized_metrics = {}
            
            # æ ‡å‡†åŒ–é”®å
            if 'test_r2' in metrics:
                standardized_metrics['r2'] = metrics['test_r2']
            elif 'R2' in metrics:
                standardized_metrics['r2'] = metrics['R2']
            elif 'r2' in metrics:
                standardized_metrics['r2'] = metrics['r2']
                
            if 'test_rmse' in metrics:
                standardized_metrics['rmse'] = metrics['test_rmse']
            elif 'RMSE' in metrics:
                standardized_metrics['rmse'] = metrics['RMSE']
            elif 'rmse' in metrics:
                standardized_metrics['rmse'] = metrics['rmse']
                
            if 'test_mae' in metrics:
                standardized_metrics['mae'] = metrics['test_mae']
            elif 'MAE' in metrics:
                standardized_metrics['mae'] = metrics['MAE']
            elif 'mae' in metrics:
                standardized_metrics['mae'] = metrics['mae']
            
            converted_results[res]['test_metrics'] = standardized_metrics
            converted_results[res]['metrics'] = standardized_metrics
        
        # ğŸ”´ æ–°å¢ï¼šå¦‚æœä»ç„¶æ²¡æœ‰é¢„æµ‹ç»“æœï¼Œå°è¯•ä»æ¨¡å‹ç”Ÿæˆé¢„æµ‹
        if ('y_pred' not in converted_results[res] or 'y_test' not in converted_results[res]) and 'model' in result:
            print(f"  ğŸ“Š ä¸º{res}ç”Ÿæˆé¢„æµ‹ç»“æœ...")
            
            # å°è¯•è·å–æ¨¡å‹å’Œä¼¼ç„¶å‡½æ•°
            model = result.get('model')
            likelihood = result.get('likelihood')
            
            # ä¼˜å…ˆä½¿ç”¨æµ‹è¯•é›†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†
            if 'X_test' in result and 'y_test' in result:
                X_pred = result['X_test']
                y_true = result['y_test']
                data_type = "æµ‹è¯•é›†"
            elif 'X_train' in result and 'y_train' in result:
                # ä½¿ç”¨è®­ç»ƒé›†çš„æœ€å20%ä½œä¸ºéªŒè¯
                X_pred = result['X_train']
                y_true = result['y_train']
                n_samples = len(X_pred)
                if n_samples > 100:
                    # åªä½¿ç”¨æœ€å20%çš„æ•°æ®
                    start_idx = int(n_samples * 0.8)
                    X_pred = X_pred.iloc[start_idx:] if hasattr(X_pred, 'iloc') else X_pred[start_idx:]
                    y_true = y_true.iloc[start_idx:] if hasattr(y_true, 'iloc') else y_true[start_idx:]
                data_type = "è®­ç»ƒé›†(éªŒè¯éƒ¨åˆ†)"
            elif 'X' in result and 'y' in result:
                # ä½¿ç”¨å®Œæ•´æ•°æ®çš„ä¸€éƒ¨åˆ†
                X_pred = result['X']
                y_true = result['y']
                n_samples = len(X_pred)
                if n_samples > 500:
                    # éšæœºé‡‡æ ·500ä¸ªç‚¹
                    import random
                    indices = random.sample(range(n_samples), 500)
                    X_pred = X_pred.iloc[indices] if hasattr(X_pred, 'iloc') else X_pred[indices]
                    y_true = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]
                data_type = "é‡‡æ ·æ•°æ®"
            else:
                print(f"    âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨çš„æ•°æ®ç”Ÿæˆé¢„æµ‹")
                continue
            
            # å°è¯•ç”Ÿæˆé¢„æµ‹
            try:
                # ä½¿ç”¨é¢„æµ‹å‡½æ•°
                if 'model_dict' in result:
                    # ä½¿ç”¨stgpr_ioä¸­çš„predict_with_st_gprå‡½æ•°
                    from .stgpr_io import predict_with_st_gpr
                    y_pred = predict_with_st_gpr(result, X_pred, return_variance=False)
                elif model is not None and hasattr(model, '__class__'):
                    # ç›´æ¥ä½¿ç”¨æ¨¡å‹é¢„æµ‹
                    import torch
                    import gpytorch
                    
                    model.eval()
                    if likelihood:
                        likelihood.eval()
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    if isinstance(X_pred, pd.DataFrame):
                        X_tensor = torch.tensor(X_pred.values, dtype=torch.float32)
                    else:
                        X_tensor = torch.tensor(X_pred, dtype=torch.float32)
                    
                    # é¢„æµ‹
                    with torch.no_grad(), gpytorch.settings.fast_pred_var():
                        output = model(X_tensor)
                        if likelihood:
                            pred_dist = likelihood(output)
                            y_pred = pred_dist.mean.cpu().numpy()
                        else:
                            y_pred = output.mean.cpu().numpy()
                else:
                    print(f"    âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
                    continue
                
                # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„
                if hasattr(y_true, 'values'):
                    y_true = y_true.values
                else:
                    y_true = np.array(y_true)
                
                # ä¿å­˜é¢„æµ‹ç»“æœ
                converted_results[res]['y_pred'] = y_pred
                converted_results[res]['y_test'] = y_true
                
                # è®¡ç®—åº¦é‡æŒ‡æ ‡
                converted_results[res]['test_metrics'] = {
                    'r2': r2_score(y_true, y_pred),
                    'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
                    'mae': mean_absolute_error(y_true, y_pred)
                }
                converted_results[res]['metrics'] = converted_results[res]['test_metrics'].copy()
                
                print(f"    âœ… æˆåŠŸç”Ÿæˆé¢„æµ‹ï¼ˆä½¿ç”¨{data_type}ï¼‰: RÂ²={converted_results[res]['test_metrics']['r2']:.3f}")
                
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        # 3. å¤„ç†ç‰¹å¾æ•°æ®
        # ä¼˜å…ˆçº§ï¼šX_test > X_sample > X
        for key in ['X_test', 'X_sample', 'X']:
            if key in result and result[key] is not None:
                if isinstance(result[key], pd.DataFrame):
                    converted_results[res]['X_test'] = result[key].copy()
                    converted_results[res]['X_sample'] = result[key].copy()
                    # æ·»åŠ Xå­—æ®µç”¨äºç©ºé—´åˆ†æï¼ˆpreprocess_data_for_clusteringéœ€è¦ï¼‰
                    converted_results[res]['X'] = result[key].copy()
                    break
                elif isinstance(result[key], np.ndarray):
                    # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œéœ€è¦ç‰¹å¾åç§°
                    feature_names = result.get('feature_names', 
                                              [f'feature_{i}' for i in range(result[key].shape[1])])
                    converted_results[res]['X_test'] = pd.DataFrame(result[key], columns=feature_names)
                    converted_results[res]['X_sample'] = converted_results[res]['X_test'].copy()
                    # æ·»åŠ Xå­—æ®µç”¨äºç©ºé—´åˆ†æ
                    converted_results[res]['X'] = converted_results[res]['X_test'].copy()
                    break
        
        # ç¡®ä¿æœ‰y_testæ•°æ®
        if 'y_test' not in converted_results[res] and 'y' in result:
            if 'X_test' in converted_results[res]:
                # å¦‚æœæœ‰X_testï¼Œå–ç›¸åº”é•¿åº¦çš„yå€¼
                n_test = len(converted_results[res]['X_test'])
                if isinstance(result['y'], (np.ndarray, pd.Series)) and len(result['y']) >= n_test:
                    converted_results[res]['y_test'] = np.array(result['y'][-n_test:])
        
        # 4. å¤„ç†åŸå§‹æ•°æ®ï¼ˆç”¨äºç©ºé—´åˆ†æï¼‰ - ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿å®Œæ•´çš„ç©ºé—´æ•°æ®ä¼ é€’
        if 'df' in result:
            converted_results[res]['df'] = result['df']
        elif 'data' in result:
            converted_results[res]['df'] = result['data']
        elif 'raw_data' in result:
            converted_results[res]['df'] = result['raw_data']
        
        # æ·»åŠ raw_dataå­—æ®µï¼ˆç©ºé—´åˆ†æéœ€è¦ï¼‰
        if 'raw_data' in result:
            converted_results[res]['raw_data'] = result['raw_data']
        elif 'df' in result:
            converted_results[res]['raw_data'] = result['df']
        elif 'data' in result:
            converted_results[res]['raw_data'] = result['data']
        
        # ğŸ”¥ ç¡®ä¿X_sampleæœ‰å®Œæ•´çš„ç©ºé—´ä¿¡æ¯
        if 'X_sample' in converted_results[res]:
            X_sample = converted_results[res]['X_sample']
            
            # å¦‚æœX_sampleç¼ºå°‘h3_indexä½†åŸå§‹æ•°æ®æœ‰ï¼Œå°è¯•æ·»åŠ 
            if 'h3_index' not in X_sample.columns:
                for data_source in ['df', 'raw_data', 'data']:
                    if data_source in result and result[data_source] is not None:
                        source_df = result[data_source]
                        if hasattr(source_df, 'columns') and 'h3_index' in source_df.columns:
                            if len(X_sample) <= len(source_df):
                                # å°è¯•é€šè¿‡ç´¢å¼•åŒ¹é…
                                if X_sample.index.max() < len(source_df):
                                    X_sample['h3_index'] = source_df.loc[X_sample.index, 'h3_index'].values
                                    print(f"  âœ… ä¸º{res}çš„X_sampleæ·»åŠ äº†h3_indexåˆ—")
                                    break
                                # å°è¯•é€šè¿‡ç»çº¬åº¦åŒ¹é…
                                elif all(col in X_sample.columns for col in ['latitude', 'longitude']) and \
                                     all(col in source_df.columns for col in ['latitude', 'longitude']):
                                    from sklearn.neighbors import NearestNeighbors
                                    knn = NearestNeighbors(n_neighbors=1)
                                    knn.fit(source_df[['latitude', 'longitude']].values)
                                    _, indices = knn.kneighbors(X_sample[['latitude', 'longitude']].values)
                                    X_sample['h3_index'] = source_df.iloc[indices.flatten()]['h3_index'].values
                                    print(f"  âœ… ä¸º{res}çš„X_sampleé€šè¿‡ç©ºé—´åŒ¹é…æ·»åŠ äº†h3_indexåˆ—")
                                    break
            
            converted_results[res]['X_sample'] = X_sample
        
        # 5. å¤„ç†æ¨¡å‹å¯¹è±¡
        if 'model' in result:
            converted_results[res]['model'] = result['model']
        
        # 5.5 å¤„ç†yå­—æ®µï¼ˆpreprocess_data_for_clusteringéœ€è¦ï¼‰
        if 'y' in result:
            converted_results[res]['y'] = result['y']
        elif 'y_test' in converted_results[res]:
            # å¦‚æœæœ‰y_testï¼Œä¹Ÿå°†å…¶å¤åˆ¶åˆ°yå­—æ®µ
            converted_results[res]['y'] = converted_results[res]['y_test']
        
        # 5.6 å¤„ç†h3_shap_mappingå­—æ®µï¼ˆç©ºé—´åˆ†æéœ€è¦ï¼‰
        if 'h3_shap_mapping' in result:
            converted_results[res]['h3_shap_mapping'] = result['h3_shap_mapping']
        
        # 5.7 å¤„ç†coords_dfå­—æ®µï¼ˆç©ºé—´åˆ†æéœ€è¦ï¼‰
        if 'coords_df' in result:
            converted_results[res]['coords_df'] = result['coords_df']
        
        # 6. å¤„ç†ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in result:
            converted_results[res]['feature_importance'] = result['feature_importance']
        elif 'feature_importances' in result:
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ [(feature_name, importance), ...]
            if isinstance(result['feature_importances'], dict):
                converted_results[res]['feature_importance'] = [
                    (k, v) for k, v in sorted(result['feature_importances'].items(), 
                                            key=lambda x: x[1], reverse=True)
                ]
            elif isinstance(result['feature_importances'], list):
                converted_results[res]['feature_importance'] = result['feature_importances']
        
        # 7. å¤„ç†SHAPå€¼ - ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿GeoShapleyæ•°æ®å®Œæ•´ä¼ é€’
        if 'shap_values' in result:
            shap_values = result['shap_values']
            
            # ç¡®ä¿æœ‰feature_nameså­—æ®µ
            if 'feature_names' in result:
                converted_results[res]['feature_names'] = result['feature_names']
            elif 'X_sample' in converted_results[res]:
                feature_names = list(converted_results[res]['X_sample'].columns)
                converted_results[res]['feature_names'] = feature_names
            
            # ç¡®ä¿SHAPå€¼æ˜¯æ­£ç¡®çš„æ ¼å¼
            if isinstance(shap_values, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆæŒ‰ç‰¹å¾ç»„ç»‡ï¼‰ï¼Œä¿æŒåŸæ ¼å¼å¹¶è½¬æ¢ä¸ºçŸ©é˜µæ ¼å¼
                converted_results[res]['shap_values_by_feature'] = shap_values.copy()
                
                # åˆ›å»ºçŸ©é˜µæ ¼å¼
                feature_names = result.get('feature_names', list(shap_values.keys()))
                if len(shap_values) > 0:
                    n_samples = len(next(iter(shap_values.values())))
                    shap_matrix = np.zeros((n_samples, len(feature_names)))
                    
                    for i, feat in enumerate(feature_names):
                        if feat in shap_values:
                            shap_matrix[:, i] = shap_values[feat]
                    
                    converted_results[res]['shap_values'] = shap_matrix
                    converted_results[res]['feature_names'] = feature_names
            else:
                # å¦‚æœå·²ç»æ˜¯çŸ©é˜µæ ¼å¼
                converted_results[res]['shap_values'] = np.array(shap_values)
                
                # åˆ›å»ºæŒ‰ç‰¹å¾ç»„ç»‡çš„æ ¼å¼ï¼ˆç”¨äºç©ºé—´åˆ†æï¼‰
                feature_names = result.get('feature_names', [])
                if feature_names and len(feature_names) > 0:
                    shap_values_by_feature = {}
                    for i, feat in enumerate(feature_names):
                        if i < shap_values.shape[1]:
                            shap_values_by_feature[feat] = shap_values[:, i]
                    converted_results[res]['shap_values_by_feature'] = shap_values_by_feature
                    converted_results[res]['feature_names'] = feature_names
        
        # 7.5 å¦‚æœåªæœ‰shap_values_by_featureï¼Œä¹Ÿè¦ç¡®ä¿è®¾ç½®feature_names
        elif 'shap_values_by_feature' in result:
            converted_results[res]['shap_values_by_feature'] = result['shap_values_by_feature'].copy()
            # ä»shap_values_by_featureæå–feature_names
            converted_results[res]['feature_names'] = list(result['shap_values_by_feature'].keys())
            
            # ğŸ”¥ åˆ›å»ºçŸ©é˜µæ ¼å¼çš„SHAPå€¼
            shap_by_feature = result['shap_values_by_feature']
            if len(shap_by_feature) > 0:
                feature_names = list(shap_by_feature.keys())
                n_samples = len(next(iter(shap_by_feature.values())))
                shap_matrix = np.zeros((n_samples, len(feature_names)))
                
                for i, feat in enumerate(feature_names):
                    shap_matrix[:, i] = shap_by_feature[feat]
                
                converted_results[res]['shap_values'] = shap_matrix
        
        # 7.6 å¤„ç†geoshapley_valuesï¼ˆGeoShapleyä¸‰éƒ¨åˆ†ç»“æœï¼‰
        if 'geoshapley_values' in result:
            converted_results[res]['geoshapley_values'] = result['geoshapley_values']
        
        # 8. å¤„ç†ç‰¹å¾ç±»åˆ«ä¿¡æ¯
        if 'X_sample' in converted_results[res]:
            feature_names = list(converted_results[res]['X_sample'].columns)
            
            # åˆ›å»ºç‰¹å¾ç±»åˆ«æ˜ å°„
            feature_categories = {}
            feature_categories_grouped = {
                'æ°”å€™å› ç´ ': [],
                'äººç±»æ´»åŠ¨': [],
                'åœ°å½¢å› ç´ ': [],
                'åœŸåœ°è¦†ç›–': [],
                'æ—¶ç©ºä¿¡æ¯': []
            }
            
            for feat in feature_names:
                category = categorize_feature(feat)
                feature_categories[feat] = category
                
                # åˆ†ç»„
                if feat in ['temperature', 'precipitation', 'pet']:
                    feature_categories_grouped['æ°”å€™å› ç´ '].append(feat)
                elif feat in ['nightlight', 'road_density', 'mining_density', 'population_density']:
                    feature_categories_grouped['äººç±»æ´»åŠ¨'].append(feat)
                elif feat in ['elevation', 'slope', 'aspect']:
                    feature_categories_grouped['åœ°å½¢å› ç´ '].append(feat)
                elif 'percent' in feat.lower() or feat in ['forest_area_percent', 'cropland_area_percent', 
                                                            'grassland_area_percent', 'shrubland_area_percent',
                                                            'impervious_area_percent', 'bareland_area_percent']:
                    feature_categories_grouped['åœŸåœ°è¦†ç›–'].append(feat)
                elif feat in ['latitude', 'longitude', 'year', 'h3_index']:
                    feature_categories_grouped['æ—¶ç©ºä¿¡æ¯'].append(feat)
            
            converted_results[res]['feature_categories'] = feature_categories
            converted_results[res]['feature_categories_grouped'] = feature_categories_grouped
        
        # 9. å¤„ç†SHAPäº¤äº’å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'shap_interaction_values' in result:
            converted_results[res]['shap_interaction_values'] = result['shap_interaction_values']
    
    return converted_results

def ensure_elevation_data(model_results):
    """
    ç¡®ä¿æ¯ä¸ªåˆ†è¾¨ç‡çš„ç»“æœéƒ½åŒ…å«æµ·æ‹”æ•°æ®ï¼Œç”¨äºå¯è§†åŒ–ç€è‰²
    
    å‚æ•°:
    model_results: æ¨¡å‹ç»“æœå­—å…¸
    
    è¿”å›:
    updated_model_results: æ›´æ–°åçš„æ¨¡å‹ç»“æœå­—å…¸ï¼Œç¡®ä¿åŒ…å«æµ·æ‹”æ•°æ®
    """
    # åˆå§‹åŒ–éšæœºç§å­ä»¥ä¿æŒä¸€è‡´æ€§
    np.random.seed(42)
    
    # é¦–å…ˆæ‰¾å‡ºæ‰€æœ‰åˆ†è¾¨ç‡ä¸­å·²å­˜åœ¨çš„æµ·æ‹”å€¼èŒƒå›´
    all_elevations = []
    for res in model_results:
        # æ£€æŸ¥å„ç§å¯èƒ½çš„æ•°æ®æº
        for data_key in ['X_test', 'X_sample', 'X', 'df']:
            if data_key in model_results[res] and isinstance(model_results[res][data_key], pd.DataFrame):
                if 'elevation' in model_results[res][data_key].columns:
                    all_elevations.extend(model_results[res][data_key]['elevation'].values)
                    break
    
    # å¦‚æœæœ‰æµ·æ‹”æ•°æ®ï¼Œè®¡ç®—ç»Ÿä¸€çš„èŒƒå›´ï¼›å¦åˆ™ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
    if all_elevations:
        min_elev = np.percentile(all_elevations, 5)  # ä½¿ç”¨5%åˆ†ä½æ•°é¿å…å¼‚å¸¸å€¼
        max_elev = np.percentile(all_elevations, 95)  # ä½¿ç”¨95%åˆ†ä½æ•°é¿å…å¼‚å¸¸å€¼
    else:
        # ä½¿ç”¨åˆç†çš„é»˜è®¤æµ·æ‹”èŒƒå›´ (0-2000m)
        min_elev = 0
        max_elev = 2000
    
    print(f"æµ·æ‹”æ•°æ®èŒƒå›´: {min_elev:.1f}m - {max_elev:.1f}m")
    
    # å¯¹æ¯ä¸ªåˆ†è¾¨ç‡ç¡®ä¿æœ‰æµ·æ‹”æ•°æ®
    for res in model_results:
        # ç¡®ä¿X_testå’ŒX_sampleéƒ½æœ‰æµ·æ‹”æ•°æ®
        for data_key in ['X_test', 'X_sample']:
            if data_key not in model_results[res] or not isinstance(model_results[res][data_key], pd.DataFrame):
                continue
            
            df = model_results[res][data_key]
            
            # å¦‚æœå·²æœ‰æµ·æ‹”æ•°æ®ï¼Œè·³è¿‡
            if 'elevation' in df.columns:
                continue
            
            # å°è¯•ä»å…¶ä»–æ•°æ®æºè·å–æµ·æ‹”æ•°æ®
            elevation_found = False
            
            # 1. ä»dfå­—æ®µè·å–
            if not elevation_found and 'df' in model_results[res]:
                source_df = model_results[res]['df']
                if isinstance(source_df, pd.DataFrame) and 'elevation' in source_df.columns:
                    if 'latitude' in df.columns and 'longitude' in df.columns:
                        try:
                            # ä½¿ç”¨KNNåŒ¹é…æµ·æ‹”å€¼
                            knn = KNeighborsRegressor(n_neighbors=1)
                            knn.fit(
                                source_df[['latitude', 'longitude']],
                                source_df['elevation']
                            )
                            df['elevation'] = knn.predict(df[['latitude', 'longitude']])
                            elevation_found = True
                            print(f"ä¸º{res}çš„{data_key}ä»åŸå§‹æ•°æ®åŒ¹é…äº†æµ·æ‹”å€¼")
                        except Exception as e:
                            print(f"è­¦å‘Š: æ— æ³•ä»åŸå§‹æ•°æ®åŒ¹é…æµ·æ‹”å€¼: {e}")
            
            # 2. ç”Ÿæˆæ¨¡æ‹Ÿæµ·æ‹”æ•°æ®
            if not elevation_found:
                if 'latitude' in df.columns and 'longitude' in df.columns:
                    # åŸºäºåœ°ç†ä½ç½®ç”Ÿæˆç©ºé—´ç›¸å…³çš„æµ·æ‹”
                    lat = df['latitude'].values
                    lon = df['longitude'].values
                    
                    # æ ‡å‡†åŒ–åæ ‡
                    lat_norm = (lat - np.min(lat)) / (np.max(lat) - np.min(lat) + 1e-10)
                    lon_norm = (lon - np.min(lon)) / (np.max(lon) - np.min(lon) + 1e-10)
                    
                    # ç”ŸæˆåŸºäºä½ç½®çš„æµ·æ‹”å€¼
                    elevation = min_elev + (max_elev - min_elev) * (
                        0.6 * np.sin(5 * lat_norm) * np.cos(5 * lon_norm) + 
                        0.4 * np.random.normal(0.5, 0.2, size=len(lat_norm))
                    )
                    
                    # ç¡®ä¿èŒƒå›´åœ¨åˆç†å€¼å†…
                    elevation = np.clip(elevation, min_elev, max_elev)
                    df['elevation'] = elevation
                    print(f"ä¸º{res}çš„{data_key}åˆ›å»ºäº†ç©ºé—´ç›¸å…³çš„æ¨¡æ‹Ÿæµ·æ‹”å€¼")
                else:
                    # å®Œå…¨éšæœºçš„æµ·æ‹”å€¼
                    n_samples = len(df)
                    df['elevation'] = np.random.uniform(min_elev, max_elev, n_samples)
                    print(f"ä¸º{res}çš„{data_key}åˆ›å»ºäº†éšæœºæµ·æ‹”å€¼")
    
    return model_results

def ensure_required_data_for_plots(model_results):
    """
    ç¡®ä¿æ¨¡å‹ç»“æœåŒ…å«æ‰€æœ‰10ä¸ªå›¾è¡¨æ‰€éœ€çš„æ•°æ®
    
    å‚æ•°:
    model_results: è½¬æ¢åçš„æ¨¡å‹ç»“æœ
    
    è¿”å›:
    model_results: è¡¥å……å®Œæ•´çš„æ¨¡å‹ç»“æœ
    """
    for res in model_results:
        result = model_results[res]
        
        # 1. ç¡®ä¿æœ‰é¢„æµ‹ç»“æœï¼ˆå›¾è¡¨1éœ€è¦ï¼‰
        if 'y_test' not in result or 'y_pred' not in result:
            print(f"è­¦å‘Š: {res}ç¼ºå°‘é¢„æµ‹ç»“æœï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ¨æ–­é€»è¾‘
        
        # 2. ç¡®ä¿æœ‰SHAPå€¼ï¼ˆå›¾è¡¨2ã€5ã€6éœ€è¦ï¼‰
        if 'shap_values' not in result:
            print(f"è­¦å‘Š: {res}ç¼ºå°‘SHAPå€¼ï¼ŒæŸäº›å›¾è¡¨å¯èƒ½æ— æ³•ç”Ÿæˆ")
            # å¯ä»¥å°è¯•è®¡ç®—SHAPå€¼ï¼Œä½†éœ€è¦æ¨¡å‹å¯¹è±¡
            if 'model' in result and 'X_sample' in result:
                try:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ SHAPå€¼è®¡ç®—é€»è¾‘
                    pass
                except Exception as e:
                    print(f"æ— æ³•è®¡ç®—SHAPå€¼: {e}")
        
        # 3. ç¡®ä¿æœ‰ç‰¹å¾é‡è¦æ€§ï¼ˆå›¾è¡¨3éœ€è¦ï¼‰
        if 'feature_importance' not in result:
            print(f"è­¦å‘Š: {res}ç¼ºå°‘ç‰¹å¾é‡è¦æ€§")
            # æ³¨æ„ï¼šç‰¹å¾é‡è¦æ€§ç°åœ¨ç»Ÿä¸€åœ¨create_all_visualizationsä¸­
            # åŸºäºSHAPå€¼è®¡ç®—ï¼Œä»¥ç¡®ä¿ä¸SHAPåˆ†å¸ƒå›¾çš„ä¸€è‡´æ€§
            # è¿™é‡Œä¸å†é¢„å…ˆè®¡ç®—ï¼Œé¿å…ä½¿ç”¨éSHAPçš„ç‰¹å¾é‡è¦æ€§
        
        # 4. ç¡®ä¿æœ‰ç©ºé—´ä¿¡æ¯ï¼ˆå›¾è¡¨5ã€6éœ€è¦ï¼‰
        if 'df' not in result and 'X_test' in result:
            # å°è¯•åˆ›å»ºåŒ…å«å¿…è¦å­—æ®µçš„df
            df = result['X_test'].copy()
            
            # æ·»åŠ ç›®æ ‡å˜é‡
            if 'y_test' in result:
                df['VHI'] = result['y_test']
            
            # ç¡®ä¿æœ‰h3_indexï¼ˆå¦‚æœæ²¡æœ‰ï¼Œåˆ›å»ºæ¨¡æ‹Ÿçš„ï¼‰
            if 'h3_index' not in df.columns:
                df['h3_index'] = [f'h3_{i}' for i in range(len(df))]
            
            result['df'] = df
        
        # 5. ç¡®ä¿æµ·æ‹”æ¢¯åº¦æ•°æ®ï¼ˆå›¾è¡¨7éœ€è¦ï¼‰
        # è¿™ä¸ªé€šå¸¸éœ€è¦åœ¨è¿è¡Œæ—¶è®¡ç®—ï¼Œè¿™é‡Œåªæ˜¯æ ‡è®°
        if 'elevation_gradient_data' not in result:
            result['needs_elevation_gradient_calculation'] = True
    
    return model_results

def prepare_stgpr_results_for_visualization(results, output_dir=None):
    """
    å‡†å¤‡ST-GPRæ¨¡å‹ç»“æœç”¨äºå¯è§†åŒ–
    
    è¿™æ˜¯ä¸»è¦çš„è½¬æ¢å‡½æ•°ï¼Œå°†ST-GPRè¾“å‡ºè½¬æ¢ä¸ºå¯è§†åŒ–æ¨¡å—æ‰€éœ€çš„æ ¼å¼
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. é¢„å…ˆè®¡ç®—æ’å€¼åçš„å®Œæ•´ç½‘æ ¼SHAPæ•°æ®
    2. å°†å¢å¼ºæ•°æ®ä¿å­˜åˆ°model_resultsä¸­ä¾›æ‰€æœ‰å›¾è¡¨ä½¿ç”¨
    3. ç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œæœ€å¤§åŒ–å›¾è¡¨è´¨é‡
    
    å‚æ•°:
    results: ST-GPRè®­ç»ƒç»“æœå­—å…¸ï¼ŒæŒ‰åˆ†è¾¨ç‡ç»„ç»‡
    output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›:
    dict: å¤„ç†åçš„æ¨¡å‹ç»“æœï¼Œå…¼å®¹å¯è§†åŒ–æ¨¡å—ï¼ŒåŒ…å«å¢å¼ºçš„å®Œæ•´ç½‘æ ¼æ•°æ®
    """
    print("ä½¿ç”¨stgpr_visualizationæ¨¡å—å‡†å¤‡æ•°æ®ç”¨äºå¯è§†åŒ–...")
    
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    success, created_dir = ensure_dir_exists(output_dir)
    if not success:
        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• {output_dir}")
    
    # ç¡®ä¿ç»“æœæ˜¯ä¸€ä¸ªå­—å…¸
    if not isinstance(results, dict):
        print("é”™è¯¯: è¾“å…¥çš„æ¨¡å‹ç»“æœä¸æ˜¯ä¸€ä¸ªå­—å…¸")
        return {}
    
    # 1. è½¬æ¢ST-GPRæ ¼å¼åˆ°å¯è§†åŒ–æ ¼å¼
    model_results = convert_stgpr_to_visualization_format(results)
    
    # 2. ç¡®ä¿æœ‰æµ·æ‹”æ•°æ®
    model_results = ensure_elevation_data(model_results)
    
    # 3. ç¡®ä¿æœ‰æ‰€æœ‰å¿…éœ€çš„æ•°æ®å­—æ®µ
    model_results = ensure_required_data_for_plots(model_results)
    
    # 4. ğŸ”¥ NEW: é¢„å…ˆè®¡ç®—æ’å€¼åçš„å®Œæ•´ç½‘æ ¼SHAPæ•°æ®
    print("\nğŸš€ é¢„å…ˆè®¡ç®—æ’å€¼åçš„å®Œæ•´ç½‘æ ¼SHAPæ•°æ®ä»¥æå‡æ‰€æœ‰å›¾è¡¨è´¨é‡...")
    
    enhanced_count = 0
    for res in ['res7', 'res6', 'res5']:
        if res not in model_results:
            continue
            
        print(f"\n  ğŸ“Š å¤„ç†{res}çš„å®Œæ•´ç½‘æ ¼æ’å€¼...")
        
        # è·å–åŸå§‹æ•°æ®
        res_data = model_results[res]
        shap_values_by_feature = res_data.get('shap_values_by_feature', {})
        X_sample = res_data.get('X_sample') if 'X_sample' in res_data else res_data.get('X')
        
        if not shap_values_by_feature or X_sample is None:
            print(f"    âš ï¸ {res}ç¼ºå°‘SHAPæ•°æ®ï¼Œè·³è¿‡æ’å€¼")
            continue
        
        # è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®
        try:
            full_h3_grid = get_full_h3_grid_data_for_visualization(res_data, res)
            if full_h3_grid is None:
                print(f"    âš ï¸ {res}æ— æ³•è·å–å®Œæ•´H3ç½‘æ ¼ï¼Œè·³è¿‡æ’å€¼")
                continue
        except Exception as e:
            print(f"    âš ï¸ {res}è·å–å®Œæ•´ç½‘æ ¼å¤±è´¥: {e}")
            continue
        
        # ğŸ”‡ ç§»é™¤å†—ä½™çš„æ’å€¼å¯¼å…¥å°è¯•
        # å®é™…çš„æ’å€¼åŠŸèƒ½ç”±å…¶ä»–æ¨¡å—å¤„ç†ï¼Œè¿™é‡Œçš„å¯¼å…¥æ€»æ˜¯å¤±è´¥ä½†ä¸å½±å“å›¾è¡¨ç”Ÿæˆ
        interpolated_shap_data = None  # è·³è¿‡é¢„æ’å€¼ï¼Œä½¿ç”¨ç°æœ‰çš„åŠ¨æ€æ’å€¼
        
        if interpolated_shap_data is None:
            # ğŸ”‡ è·³è¿‡é¢„æ’å€¼é˜¶æ®µï¼Œä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œåç»­å¤„ç†
            # å›¾è¡¨ç”Ÿæˆæ—¶ä¼šåŠ¨æ€è¿›è¡Œæ’å€¼ï¼Œæ— éœ€é¢„è®¡ç®—
            continue
        
        # ğŸ’¾ ä¿å­˜å¢å¼ºçš„æ•°æ®åˆ°model_resultsä¸­
        enhanced_res_data = res_data.copy()
        
        # ä¿å­˜æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®
        enhanced_res_data['enhanced_X_sample'] = interpolated_shap_data['X_sample']
        enhanced_res_data['enhanced_shap_values_by_feature'] = {}
        
        # æ„å»ºå¢å¼ºçš„SHAPå€¼å­—å…¸
        feature_names = interpolated_shap_data['feature_names']
        shap_values_list = interpolated_shap_data['shap_values']
        
        for i, feat_name in enumerate(feature_names):
            if i < len(shap_values_list):
                enhanced_res_data['enhanced_shap_values_by_feature'][feat_name] = shap_values_list[i]
        
        # åŒæ—¶æ›´æ–°çŸ©é˜µæ ¼å¼çš„SHAPå€¼
        if len(shap_values_list) > 0:
            enhanced_shap_matrix = np.column_stack(shap_values_list)
            enhanced_res_data['enhanced_shap_values'] = enhanced_shap_matrix
        
        # æ›´æ–°ç‰¹å¾åç§°
        enhanced_res_data['enhanced_feature_names'] = feature_names
        
        # æ ‡è®°ä¸ºå¢å¼ºæ•°æ®
        enhanced_res_data['has_enhanced_data'] = True
        enhanced_res_data['enhancement_factor'] = len(interpolated_shap_data['X_sample']) / len(X_sample)
        
        # æ›´æ–°model_results
        model_results[res] = enhanced_res_data
        enhanced_count += 1
        
        print(f"    âœ… {res}å®Œæ•´ç½‘æ ¼æ’å€¼æˆåŠŸ:")
        print(f"      â€¢ å®Œæ•´ç½‘æ ¼æ•°æ®é‡: {len(interpolated_shap_data['X_sample'])}ä¸ªç½‘æ ¼")
        print(f"      â€¢ æ•°æ®å¢å¼ºå€æ•°: {enhanced_res_data['enhancement_factor']:.1f}x")
        print(f"      â€¢ ç‰¹å¾æ•°é‡: {len(enhanced_res_data['enhanced_shap_values_by_feature'])}ä¸ª")
    
    if enhanced_count > 0:
        print(f"\n  âœ… é¢„è®¡ç®—å®Œæˆï¼š{enhanced_count}ä¸ªåˆ†è¾¨ç‡çš„æ•°æ®å·²å¢å¼ºï¼Œæ‰€æœ‰å›¾è¡¨å°†å—ç›Šäºé«˜è´¨é‡æ’å€¼æ•°æ®")
        print("  ğŸ“ˆ é¢„æœŸå›¾è¡¨è´¨é‡æå‡ï¼šæ›´å¯†é›†çš„æ•£ç‚¹ã€æ›´å¹³æ»‘çš„åˆ†å¸ƒã€æ›´ç¨³å®šçš„ç»Ÿè®¡ç»“æœ")
    else:
        # ğŸ”‡ ç§»é™¤å†—ä½™è­¦å‘Šï¼šå›¾è¡¨èƒ½æ­£å¸¸ç”Ÿæˆï¼Œä½¿ç”¨åŠ¨æ€æ’å€¼
        # print(f"\n  âš ï¸ æœªèƒ½é¢„è®¡ç®—å¢å¼ºæ•°æ®ï¼Œå›¾è¡¨å°†ä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
        pass
    
    # 5. ç®€åŒ–ç‰¹å¾åç§°ç”¨äºæ˜¾ç¤º
    for res in model_results:
        if 'feature_importance' in model_results[res]:
            # åˆ›å»ºç®€åŒ–ç‰ˆæœ¬
            simplified_importance = []
            for feature, importance in model_results[res]['feature_importance']:
                simplified_name = simplify_feature_name_for_plot(feature)
                simplified_importance.append((simplified_name, importance))
            model_results[res]['simplified_feature_importance'] = simplified_importance
        
        # åˆ›å»ºç‰¹å¾åç§°æ˜ å°„
        if 'X_sample' in model_results[res]:
            feature_names = list(model_results[res]['X_sample'].columns)
            model_results[res]['simplified_feature_names'] = {
                name: simplify_feature_name_for_plot(name) for name in feature_names
            }
    
    print("æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¯ä»¥è¿›è¡Œå¯è§†åŒ–")
    return model_results

def get_full_h3_grid_data_for_visualization(res_data, resolution):
    """
    è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®ç”¨äºå¯è§†åŒ–æ’å€¼
    
    å‚æ•°:
    - res_data: åˆ†è¾¨ç‡æ•°æ®
    - resolution: åˆ†è¾¨ç‡æ ‡è¯†
    
    è¿”å›:
    - full_h3_data: å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®DataFrame
    """
    # å°è¯•ä»å¤šä¸ªæ¥æºè·å–å®Œæ•´æ•°æ®
    full_data = None
    
    # æ–¹æ³•1ï¼šä»dfå­—æ®µè·å–ï¼ˆé€šå¸¸åŒ…å«å®Œæ•´æ•°æ®ï¼‰
    if 'df' in res_data and res_data['df'] is not None:
        full_data = res_data['df']
        print(f"    ä»dfè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•2ï¼šä»raw_dataè·å–
    elif 'raw_data' in res_data and res_data['raw_data'] is not None:
        full_data = res_data['raw_data']
        print(f"    ä»raw_dataè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•3ï¼šå°è¯•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶
    else:
        try:
            import os
            data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
            data_file = os.path.join(data_dir, f"ALL_DATA_with_VHI_PCA_{resolution}.csv")
            if os.path.exists(data_file):
                import pandas as pd
                full_data = pd.read_csv(data_file)
                print(f"    ä»æ–‡ä»¶åŠ è½½å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
        except Exception as e:
            print(f"    æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {e}")
    
    if full_data is None:
        return None
    
    # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
    required_cols = ['h3_index', 'latitude', 'longitude']
    if not all(col in full_data.columns for col in required_cols):
        print(f"    æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—")
        return None
    
    # è·å–å”¯ä¸€çš„H3ç½‘æ ¼
    h3_grid = full_data.drop_duplicates(subset=['h3_index'])[['h3_index', 'latitude', 'longitude']].copy()
    print(f"    å”¯ä¸€H3ç½‘æ ¼æ•°: {len(h3_grid)}")
    
    return h3_grid

def create_all_visualizations(model_results, output_dir=None):
    """
    ä¸ºST-GPRæ¨¡å‹ç»“æœåˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
    
    æŒ‰ç…§è®¾è®¡æ–‡æ¡£è¦æ±‚ï¼Œç”Ÿæˆ10ä¸ªå¯è§†åŒ–å›¾è¡¨:
    1. æ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
    2. SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
    3. ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾
    4. PDPäº¤äº’åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
    5. SHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾
    6. SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾
    7. å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾
    8. ç‰¹å¾äº¤äº’ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾
    9. æ—¶åºç‰¹å¾çƒ­å›¾
    10. GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾
    
    å‚æ•°:
    model_results: æ ¼å¼åŒ–åçš„æ¨¡å‹ç»“æœï¼Œç”±prepare_stgpr_results_for_visualizationå‡½æ•°ç”Ÿæˆ
    output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›:
    bool: æ˜¯å¦æˆåŠŸåˆ›å»ºæ‰€æœ‰å›¾è¡¨
    """
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = os.path.join(os.getcwd(), 'output')
    success, created_dir = ensure_dir_exists(output_dir)
    if not success:
        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• {output_dir}")
        return False
    
    # å…³é—­matplotlibçš„äº¤äº’æ¨¡å¼
    plt.ioff()
    
    # ç”¨äºè¿½è¸ªå·²åˆ›å»ºçš„å›¾è¡¨
    created_charts = []
    failed_charts = []
    
    print("\nå¼€å§‹åˆ›å»ºST-GPRæ¨¡å‹çš„10ä¸ªæ ‡å‡†å¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å¯è§†åŒ–å‡½æ•°
        from visualization import (
            plot_feature_importance_comparison,
            plot_combined_shap_summary_distribution_v2 as plot_combined_shap_summary_distribution,  # ä½¿ç”¨æ–°ç‰ˆæœ¬
            plot_regionkmeans_shap_clusters_by_resolution,
            plot_regionkmeans_feature_target_analysis,
            plot_elevation_gradient_bullseye,
            plot_temporal_feature_heatmap,
            plot_geoshapley_spatial_top3,
            plot_combined_model_performance_prediction
        )
        
        # ä¿®æ”¹ï¼šå¯¼å…¥å•ç‰¹å¾ä¾èµ–å‡½æ•°æ›¿ä»£PDPäº¤äº’å‡½æ•°
        from visualization.pdp_plots import plot_single_feature_dependency_grid
        from visualization.elevation_gradient_single_feature import plot_elevation_gradient_single_feature_grid
        
        # å…¶ä»–è¾…åŠ©å‡½æ•°
        from visualization.utils import (
            simplify_feature_name_for_plot,
            ensure_spatiotemporal_features,
            enhance_feature_display_name
        )
        from model_analysis.core import categorize_feature
        VISUALIZATION_AVAILABLE = True
        
        # 1. æ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
        print("\n[1/10] åˆ›å»ºæ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾...")
        try:
            fig = plot_combined_model_performance_prediction(model_results, output_dir)
            if fig:
                plt.close(fig)
                created_charts.append("æ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                print("âœ“ æˆåŠŸåˆ›å»º")
            else:
                failed_charts.append("æ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                print("âœ— åˆ›å»ºå¤±è´¥")
        except Exception as e:
            failed_charts.append("æ¨¡å‹æ€§èƒ½è·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 2. SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
        print("\n[2/10] åˆ›å»ºSHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾...")
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰SHAPå€¼æ•°æ®
            has_shap = any('shap_values' in model_results[res] or 'shap_values_by_feature' in model_results[res] 
                          for res in model_results)
            
            if has_shap:
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°ï¼Œé€‚é…GeoShapleyä¸‰éƒ¨åˆ†æ•ˆåº”ç»“æ„
                fig = plot_combined_shap_summary_distribution(
                    model_results, 
                    output_dir=output_dir, 
                    top_n=25,  # 12ä¸»æ•ˆåº”+1GEO+12äº¤äº’æ•ˆåº”
                    include_interaction=True  # ç¡®ä¿åŒ…å«äº¤äº’æ•ˆåº”
                )
                if fig:
                    created_charts.append("SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šç¼ºå°‘SHAPå€¼æ•°æ®")
                failed_charts.append("SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾ï¼ˆç¼ºå°‘æ•°æ®ï¼‰")
        except Exception as e:
            failed_charts.append("SHAPå€¼åˆ†å¸ƒè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 3. ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾
        print("\n[3/10] åˆ›å»ºç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾...")
        try:
            feature_importances = {}
            
            for res in model_results:
                if res not in ['res5', 'res6', 'res7']:
                    continue
                
                # ä¼˜å…ˆä½¿ç”¨å·²ç»è®¡ç®—å¥½çš„feature_importance
                if 'feature_importance' in model_results[res] and model_results[res]['feature_importance']:
                    feature_importances[res] = model_results[res]['feature_importance']
                    print(f"  {res}: ä½¿ç”¨å·²è®¡ç®—çš„ç‰¹å¾é‡è¦æ€§ï¼Œå…±{len(model_results[res]['feature_importance'])}ä¸ªç‰¹å¾")
                
                # å¦‚æœæ²¡æœ‰feature_importanceï¼Œå°è¯•ä»shap_valuesè®¡ç®—
                elif 'shap_values' in model_results[res] and 'feature_names' in model_results[res]:
                    try:
                        shap_vals = model_results[res]['shap_values']
                        
                        # å¦‚æœshap_valsæ˜¯numpyæ•°ç»„ï¼Œè®¡ç®—ç‰¹å¾é‡è¦æ€§
                        if isinstance(shap_vals, np.ndarray) and shap_vals.ndim == 2:
                            feature_importance_list = []
                            feature_names = model_results[res]['feature_names']
                            
                            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
                            for i, feat in enumerate(feature_names):
                                if i < shap_vals.shape[1]:
                                    importance = np.abs(shap_vals[:, i]).mean()
                                    feature_importance_list.append((feat, importance))
                            
                            # æŒ‰é‡è¦æ€§æ’åº
                            feature_importance_list.sort(key=lambda x: x[1], reverse=True)
                            feature_importances[res] = feature_importance_list
                            
                            print(f"  {res}: åŸºäºSHAPå€¼è®¡ç®—äº†{len(feature_importance_list)}ä¸ªç‰¹å¾çš„é‡è¦æ€§")
                        else:
                            print(f"  {res}: SHAPå€¼æ ¼å¼ä¸æ­£ç¡®")
                    except Exception as e:
                        print(f"  {res}: ä»SHAPå€¼è®¡ç®—ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
                
                # å¦‚æœæœ‰shap_values_by_featureï¼Œä½¿ç”¨å®ƒï¼ˆè¿™æ˜¯æ›´å‡†ç¡®çš„æ ¼å¼ï¼‰
                elif 'shap_values_by_feature' in model_results[res]:
                    shap_by_feature = model_results[res]['shap_values_by_feature']
                    
                    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
                    feature_importance_list = []
                    for feat, shap_vals in shap_by_feature.items():
                        importance = np.abs(shap_vals).mean()
                        feature_importance_list.append((feat, importance))
                    
                    # æŒ‰é‡è¦æ€§æ’åº
                    feature_importance_list.sort(key=lambda x: x[1], reverse=True)
                    feature_importances[res] = feature_importance_list
                    
                    print(f"  {res}: åŸºäºshap_values_by_featureè®¡ç®—äº†{len(feature_importance_list)}ä¸ªç‰¹å¾çš„é‡è¦æ€§")
                else:
                    print(f"  è­¦å‘Š: {res}ç¼ºå°‘ç‰¹å¾é‡è¦æ€§æ•°æ®")
            
            if feature_importances:
                fig = plot_feature_importance_comparison(feature_importances, output_dir, results=model_results)
                if fig:
                    plt.close(fig)
                    created_charts.append("ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šç¼ºå°‘ç‰¹å¾é‡è¦æ€§æ•°æ®")
                failed_charts.append("ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾ï¼ˆç¼ºå°‘æ•°æ®ï¼‰")
        except Exception as e:
            failed_charts.append("ç‰¹å¾é‡è¦æ€§åˆ†ç±»æ¯”è¾ƒå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 4. PDPäº¤äº’åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾
        print("\n[4/10] åˆ›å»ºPDPå•ç‰¹å¾ä¾èµ–åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾...")
        print("  å±•ç¤ºæ¯ä¸ªåˆ†è¾¨ç‡Top 3ç‰¹å¾çš„å•ç‰¹å¾ä¾èµ–å…³ç³»")
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹å¯¹è±¡
            has_model = any('model' in model_results[res] for res in model_results)
            if has_model:
                # å•ç‰¹å¾ä¾èµ–åˆ†æä½¿ç”¨GeoShapley primaryæ•ˆåº”æˆ–PDPæ–¹æ³•
                fig = plot_single_feature_dependency_grid(model_results, output_dir=output_dir)
                if fig:
                    plt.close(fig)
                    created_charts.append("PDPå•ç‰¹å¾ä¾èµ–åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("PDPå•ç‰¹å¾ä¾èµ–åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šç¼ºå°‘æ¨¡å‹å¯¹è±¡")
                failed_charts.append("PDPå•ç‰¹å¾ä¾èµ–åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾ï¼ˆç¼ºå°‘æ¨¡å‹ï¼‰")
        except Exception as e:
            failed_charts.append("PDPå•ç‰¹å¾ä¾èµ–åˆ†æè·¨åˆ†è¾¨ç‡æ¯”è¾ƒå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 5. SHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾
        print("\n[5/10] åˆ›å»ºSHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾...")
        try:
            # ä½¿ç”¨regionkmeansæµç¨‹ç”Ÿæˆç©ºé—´èšç±»å›¾
            fig, cluster_results = plot_regionkmeans_shap_clusters_by_resolution(
                model_results, 
                output_dir=output_dir,
                top_n=14,  # ä½¿ç”¨ä¼˜åŒ–åçš„14ä¸ªç‰¹å¾ï¼ˆGeoShapleyè¾“å‡ºï¼‰
                n_clusters=3
            )
            if fig:
                plt.close(fig)
                print("  âœ“ SHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾åˆ›å»ºæˆåŠŸ")
                
                # ä¿å­˜èšç±»ç»“æœä¾›ç¬¬6ä¸ªå›¾è¡¨ä½¿ç”¨
                model_results['_cluster_results'] = cluster_results
            else:
                print("  âœ— SHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾åˆ›å»ºå¤±è´¥")
        except Exception as e:
            print(f"  âœ— åˆ›å»ºSHAPç©ºé—´æ•æ„Ÿæ€§åˆ†æå›¾å¤±è´¥: {e}")
            import traceback as tb
            tb.print_exc()
        
        # 6. SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾
        print("\n[6/10] åˆ›å»ºSHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾...")
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰èšç±»ç»“æœ
            if '_cluster_results' in model_results and model_results['_cluster_results']:
                # ä½¿ç”¨ç¬¬5æ­¥ç”Ÿæˆçš„èšç±»ç»“æœ
                cluster_results = model_results['_cluster_results']
                fig = plot_regionkmeans_feature_target_analysis(
                    cluster_results,
                    output_dir=output_dir
                )
                if fig:
                    plt.close(fig)
                    print("  âœ“ SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾åˆ›å»ºæˆåŠŸ")
                else:
                    print("  âœ— SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾åˆ›å»ºå¤±è´¥")
            else:
                # å¦‚æœæ²¡æœ‰èšç±»ç»“æœï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ
                print("  æœªæ‰¾åˆ°èšç±»ç»“æœï¼Œå°è¯•é‡æ–°ç”Ÿæˆ...")
                
                # å‡†å¤‡èšç±»æ•°æ®
                from visualization.regionkmeans_data import preprocess_data_for_clustering
                from visualization.regionkmeans_cluster import perform_spatial_clustering
                
                cluster_data = preprocess_data_for_clustering(model_results, top_n=14)  # ä½¿ç”¨ä¼˜åŒ–åçš„14ä¸ªç‰¹å¾
                
                if cluster_data:
                    # å¯¹æ¯ä¸ªåˆ†è¾¨ç‡æ‰§è¡Œèšç±»
                    cluster_results = {}
                    for res in cluster_data:
                        if res not in ['res5', 'res6', 'res7']:
                            continue
                            
                        res_data = cluster_data[res]
                        shap_features = res_data['shap_features']
                        coords_df = res_data['coords_df']
                        target_values = res_data.get('target_values', None)
                        
                        # æ‰§è¡Œç©ºé—´èšç±»
                        clusters, X_clustered = perform_spatial_clustering(
                            shap_features, 
                            coords_df, 
                            n_clusters=3,
                            grid_disk_k=1
                        )
                        
                        # æ„å»ºèšç±»ç»“æœ
                        cluster_results[res] = {
                            'clusters': clusters,
                            'shap_features': shap_features,
                            'coords_df': coords_df,
                            'top_features': res_data['top_features'],
                            'target_values': target_values,
                            'X_clustered': X_clustered,
                            'standardized_features': X_clustered  # æ·»åŠ è¿™ä¸ªå­—æ®µ
                        }
                        
                        print(f"  {res}: ç”Ÿæˆäº†{len(np.unique(clusters))}ä¸ªèšç±»")
                    
                    # è°ƒç”¨ç»˜å›¾å‡½æ•°
                    fig = plot_regionkmeans_feature_target_analysis(
                        cluster_results,
                        output_dir=output_dir
                    )
                    if fig:
                        plt.close(fig)
                        print("  âœ“ SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾åˆ›å»ºæˆåŠŸ")
                    else:
                        print("  âœ— SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾åˆ›å»ºå¤±è´¥")
                else:
                    print("  âœ— æ— æ³•å‡†å¤‡èšç±»æ•°æ®ï¼Œè·³è¿‡SHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾")
        except Exception as e:
            print(f"  âœ— åˆ›å»ºSHAPèšç±»ç‰¹å¾è´¡çŒ®ä¸ç›®æ ‡åˆ†æå›¾å¤±è´¥: {e}")
            import traceback as tb
            tb.print_exc()
        
        # 7. å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾
        print("\n[7/10] åˆ›å»ºå¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾...")
        try:
            # é¦–å…ˆéœ€è¦è®¡ç®—æµ·æ‹”æ¢¯åº¦æ•°æ®
            elevation_gradient_data = {}
            
            # å®šä¹‰æœŸæœ›çš„é«˜ç¨‹åŒºé—´ï¼ˆ16ä¸ªåŒºé—´ï¼‰
            elevation_bins_config = [
                (0, 200),       # åŒºé—´1
                (200, 400),     # åŒºé—´2
                (400, 600),     # åŒºé—´3
                (600, 800),     # åŒºé—´4
                (800, 1000),    # åŒºé—´5
                (1000, 1200),   # åŒºé—´6
                (1200, 1400),   # åŒºé—´7
                (1400, 1600),   # åŒºé—´8
                (1600, 1800),   # åŒºé—´9
                (1800, 2000),   # åŒºé—´10
                (2000, 2200),   # åŒºé—´11
                (2200, 2400),   # åŒºé—´12
                (2400, 2600),   # åŒºé—´13
                (2600, 2800),   # åŒºé—´14
                (2800, 3000),   # åŒºé—´15
                (3000, 3200)    # åŒºé—´16
            ]
            
            for res in model_results:
                if res not in ['res5', 'res6', 'res7']:
                    continue
                    
                elevation_gradient_data[res] = {}
                
                # è·å–æ•°æ®
                if 'X_test' in model_results[res] and 'y_test' in model_results[res] and 'y_pred' in model_results[res]:
                    X = model_results[res]['X_test']
                    y_true = model_results[res]['y_test']
                    y_pred = model_results[res]['y_pred']
                    
                    # ç¡®ä¿æœ‰elevationåˆ—
                    if 'elevation' not in X.columns:
                        print(f"  è­¦å‘Š: {res}ç¼ºå°‘elevationæ•°æ®ï¼Œè·³è¿‡æµ·æ‹”æ¢¯åº¦åˆ†æ")
                        continue
                    
                    # è·å–æµ·æ‹”å€¼
                    elevation_values = X['elevation'].values
                    
                    # æŒ‰ç…§é¢„å®šä¹‰çš„åŒºé—´åˆ›å»ºæ•°æ®
                    for bin_start, bin_end in elevation_bins_config:
                        bin_label = f"{int(bin_start)}-{int(bin_end)}"
                        
                        # æ‰¾å‡ºåœ¨è¿™ä¸ªåŒºé—´çš„æ ·æœ¬
                        mask = (elevation_values >= bin_start) & (elevation_values <= bin_end)
                        
                        if mask.sum() > 0:
                            # è®¡ç®—è¿™ä¸ªåŒºé—´çš„ç»Ÿè®¡æ•°æ®
                            vhi_mean = y_true[mask].mean()
                            mae = np.abs(y_true[mask] - y_pred[mask]).mean()
                            
                            # è®¡ç®—RÂ²
                            ss_res = np.sum((y_true[mask] - y_pred[mask])**2)
                            ss_tot = np.sum((y_true[mask] - y_true[mask].mean())**2)
                            if ss_tot > 0:
                                r2 = 1 - ss_res / ss_tot
                            else:
                                r2 = 0.0
                            
                            elevation_gradient_data[res][bin_label] = {
                                'vhi_mean': vhi_mean,
                                'mae': mae,
                                'r2': r2,
                                'sample_count': mask.sum(),
                                'elevation_range': (bin_start, bin_end)
                            }
            
            # å¦‚æœæœ‰æµ·æ‹”æ¢¯åº¦æ•°æ®ï¼Œç”Ÿæˆå›¾è¡¨
            if elevation_gradient_data and any(elevation_gradient_data[res] for res in elevation_gradient_data):
                fig = plot_elevation_gradient_bullseye(elevation_gradient_data, output_dir=output_dir)
                if fig:
                    plt.close(fig)
                    created_charts.append("å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šç¼ºå°‘æµ·æ‹”æ¢¯åº¦æ•°æ®")
                failed_charts.append("å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾ï¼ˆç¼ºå°‘æ•°æ®ï¼‰")
        except Exception as e:
            failed_charts.append("å¤šåˆ†è¾¨ç‡æµ·æ‹”æ¢¯åº¦æ•ˆåº”åˆ†æå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 8. ç‰¹å¾äº¤äº’ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾
        print("\n[8/10] åˆ›å»ºå•ç‰¹å¾ä¾èµ–ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾...")
        print("  å±•ç¤º3Ã—3ç½‘æ ¼ï¼šåˆ†è¾¨ç‡ Ã— æµ·æ‹”åŒºé—´çš„å•ç‰¹å¾ä¾èµ–å…³ç³»")
        try:
            # ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°ï¼šplot_elevation_gradient_single_feature_grid
            # è¿™ä¸ªå‡½æ•°ç”Ÿæˆelevation_gradient_pdp_grid.pngæ–‡ä»¶ï¼Œæ˜¾ç¤ºå•ç‰¹å¾ä¾èµ–å›¾
            fig = plot_elevation_gradient_single_feature_grid(model_results, output_dir=output_dir)
            if fig:
                plt.close(fig)
                created_charts.append("å•ç‰¹å¾ä¾èµ–ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾")
                print("âœ“ æˆåŠŸåˆ›å»º")
            else:
                failed_charts.append("å•ç‰¹å¾ä¾èµ–ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾")
                print("âœ— åˆ›å»ºå¤±è´¥")
        except Exception as e:
            failed_charts.append("å•ç‰¹å¾ä¾èµ–ä¸æµ·æ‹”æ¢¯åº¦åˆ†æå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 9. æ—¶åºç‰¹å¾çƒ­å›¾
        print("\n[9/10] åˆ›å»ºæ—¶åºç‰¹å¾çƒ­å›¾...")
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰SHAPå€¼å’Œæ—¶é—´ä¿¡æ¯
            has_temporal_data = False
            for res in model_results:
                # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
                if 'shap_values_by_feature' in model_results[res]:
                    # æ–¹æ³•1: æ£€æŸ¥X_sampleä¸­æ˜¯å¦æœ‰yearåˆ—
                    if 'X_sample' in model_results[res]:
                        df = model_results[res]['X_sample']
                        if 'year' in df.columns:
                            has_temporal_data = True
                            break
                    
                    # æ–¹æ³•2: æ£€æŸ¥dfï¼ˆåŸå§‹æ•°æ®ï¼‰ä¸­æ˜¯å¦æœ‰yearåˆ—
                    if 'df' in model_results[res]:
                        df = model_results[res]['df']
                        if 'year' in df.columns:
                            has_temporal_data = True
                            # å¦‚æœX_sampleæ²¡æœ‰yearï¼Œå°è¯•å°†yearä¿¡æ¯æ·»åŠ åˆ°X_sample
                            if 'X_sample' in model_results[res] and 'year' not in model_results[res]['X_sample'].columns:
                                X_sample = model_results[res]['X_sample']
                                # ç¡®ä¿ç´¢å¼•åŒ¹é…
                                if len(X_sample) <= len(df):
                                    # ä½¿ç”¨ç´¢å¼•åŒ¹é…yearæ•°æ®
                                    if X_sample.index.max() < len(df):
                                        model_results[res]['X_sample']['year'] = df.loc[X_sample.index, 'year'].values
                                        print(f"  âœ“ ä¸º{res}çš„X_sampleæ·»åŠ äº†yearåˆ—")
                                    else:
                                        # å°è¯•ä½¿ç”¨å‰Nä¸ªæ ·æœ¬
                                        model_results[res]['X_sample']['year'] = df['year'].iloc[:len(X_sample)].values
                                        print(f"  âœ“ ä¸º{res}çš„X_sampleæ·»åŠ äº†yearåˆ—ï¼ˆä½¿ç”¨å‰{len(X_sample)}ä¸ªæ ·æœ¬ï¼‰")
                            break
                    
                    # æ–¹æ³•3: æ£€æŸ¥feature_namesä¸­æ˜¯å¦åŒ…å«year
                    if 'feature_names' in model_results[res] and 'year' in model_results[res]['feature_names']:
                        has_temporal_data = True
                        print(f"  â„¹ï¸ {res}çš„ç‰¹å¾ä¸­åŒ…å«yearï¼Œå¯èƒ½å¯ä»¥ç”Ÿæˆæ—¶åºçƒ­å›¾")
                        break
            
            if has_temporal_data:
                fig = plot_temporal_feature_heatmap(model_results, output_dir=output_dir, top_n_features=18)
                if fig:
                    plt.close(fig)
                    created_charts.append("æ—¶åºç‰¹å¾çƒ­å›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("æ—¶åºç‰¹å¾çƒ­å›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šç¼ºå°‘æ—¶åºæ•°æ®")
                print("  éœ€è¦ï¼šshap_values_by_feature å’Œ yearåˆ—")
                failed_charts.append("æ—¶åºç‰¹å¾çƒ­å›¾ï¼ˆç¼ºå°‘æ—¶åºæ•°æ®ï¼‰")
        except Exception as e:
            failed_charts.append("æ—¶åºç‰¹å¾çƒ­å›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # 10. GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾
        print("\n[10/10] åˆ›å»ºGeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾...")
        try:
            # ğŸ”¥ ä¿®å¤ï¼šæ›´å…¨é¢çš„æ•°æ®æ£€æŸ¥å’Œä¿®å¤é€»è¾‘
            has_spatial_shap = False
            data_issues = []
            
            for res in ['res5', 'res6', 'res7']:
                if res not in model_results:
                    data_issues.append(f"{res}: ç¼ºå°‘åŸºç¡€æ•°æ®")
                    continue
                
                res_data = model_results[res]
                
                # æ£€æŸ¥SHAPæ•°æ®
                if 'shap_values_by_feature' not in res_data:
                    data_issues.append(f"{res}: ç¼ºå°‘shap_values_by_feature")
                    continue
                
                if 'feature_importance' not in res_data:
                    data_issues.append(f"{res}: ç¼ºå°‘feature_importance")
                    continue
                
                # æ£€æŸ¥ç©ºé—´æ•°æ®å¹¶å°è¯•ä¿®å¤
                spatial_data_ok = False
                
                # æ£€æŸ¥X_sample
                if 'X_sample' in res_data and res_data['X_sample'] is not None:
                    X_sample = res_data['X_sample']
                    
                    # æ£€æŸ¥ç»çº¬åº¦
                    if all(col in X_sample.columns for col in ['longitude', 'latitude']):
                        spatial_data_ok = True
                        print(f"  âœ“ {res}: X_sampleåŒ…å«ç»çº¬åº¦")
                    else:
                        # å°è¯•ä»å…¶ä»–æ•°æ®æºè¡¥å……ç»çº¬åº¦
                        for source_key in ['df', 'raw_data']:
                            if source_key in res_data and res_data[source_key] is not None:
                                source_df = res_data[source_key]
                                if hasattr(source_df, 'columns') and all(col in source_df.columns for col in ['longitude', 'latitude']):
                                    try:
                                        # é€šè¿‡ç´¢å¼•åŒ¹é…æ·»åŠ ç»çº¬åº¦
                                        if len(X_sample) <= len(source_df):
                                            if 'longitude' not in X_sample.columns:
                                                if X_sample.index.max() < len(source_df):
                                                    X_sample['longitude'] = source_df.loc[X_sample.index, 'longitude'].values
                                                else:
                                                    X_sample['longitude'] = source_df['longitude'].iloc[:len(X_sample)].values
                                            
                                            if 'latitude' not in X_sample.columns:
                                                if X_sample.index.max() < len(source_df):
                                                    X_sample['latitude'] = source_df.loc[X_sample.index, 'latitude'].values
                                                else:
                                                    X_sample['latitude'] = source_df['latitude'].iloc[:len(X_sample)].values
                                            
                                            model_results[res]['X_sample'] = X_sample
                                            spatial_data_ok = True
                                            print(f"  âœ“ {res}: ä»{source_key}è¡¥å……ç»çº¬åº¦åˆ°X_sample")
                                            break
                                    except Exception as e:
                                        print(f"  âš ï¸ {res}: ä»{source_key}è¡¥å……ç»çº¬åº¦å¤±è´¥: {e}")
                
                # å¦‚æœä»ç„¶æ²¡æœ‰ç©ºé—´æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ä»å…¶ä»–å­—æ®µè·å–
                if not spatial_data_ok:
                    # æ£€æŸ¥æ˜¯å¦æœ‰dfæˆ–raw_dataå¯ä»¥ç›´æ¥ä½¿ç”¨
                    for source_key in ['df', 'raw_data']:
                        if source_key in res_data and res_data[source_key] is not None:
                            source_df = res_data[source_key]
                            if hasattr(source_df, 'columns') and all(col in source_df.columns for col in ['longitude', 'latitude', 'h3_index']):
                                print(f"  âœ“ {res}: {source_key}åŒ…å«å®Œæ•´ç©ºé—´ä¿¡æ¯")
                                spatial_data_ok = True
                                break
                
                if spatial_data_ok:
                    has_spatial_shap = True
                    print(f"  âœ… {res}: ç©ºé—´SHAPæ•°æ®æ£€æŸ¥é€šè¿‡")
                else:
                    data_issues.append(f"{res}: ç¼ºå°‘ç©ºé—´ä¿¡æ¯ï¼ˆç»çº¬åº¦/h3_indexï¼‰")
            
            if has_spatial_shap:
                print(f"  ğŸ¯ æ•°æ®éªŒè¯é€šè¿‡ï¼Œå¼€å§‹ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾...")
                fig = plot_geoshapley_spatial_top3(model_results, output_dir=output_dir)
                if fig:
                    plt.close(fig)
                    created_charts.append("GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾")
                    print("âœ“ æˆåŠŸåˆ›å»º")
                else:
                    failed_charts.append("GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾")
                    print("âœ— åˆ›å»ºå¤±è´¥")
            else:
                print("âš  è·³è¿‡ï¼šæ•°æ®æ£€æŸ¥æœªé€šè¿‡")
                if data_issues:
                    print("  æ•°æ®é—®é¢˜:")
                    for issue in data_issues:
                        print(f"    â€¢ {issue}")
                print("  éœ€è¦ï¼šshap_values_by_featureã€feature_importance å’Œ ç»çº¬åº¦/h3_indexä¿¡æ¯")
                failed_charts.append("GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾ï¼ˆæ•°æ®ä¸å®Œæ•´ï¼‰")
                
        except Exception as e:
            failed_charts.append("GeoShapley Top 3ç‰¹å¾ç©ºé—´åˆ†å¸ƒå›¾")
            print(f"âœ— åˆ›å»ºå¤±è´¥: {e}")
            if hasattr(e, '__traceback__'):
                traceback.print_exc()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"å¯è§†åŒ–è¾“å‡ºå®Œæˆ: æˆåŠŸåˆ›å»º {len(created_charts)}/10 ä¸ªå›¾è¡¨")
        print(f"{'='*60}")
        
        if created_charts:
            print("\nâœ“ æˆåŠŸåˆ›å»ºçš„å›¾è¡¨:")
            for i, chart in enumerate(created_charts):
                print(f"  {i+1}. {chart}")
        
        if failed_charts:
            print("\nâœ— æœªæˆåŠŸåˆ›å»ºçš„å›¾è¡¨:")
            for i, chart in enumerate(failed_charts):
                print(f"  {i+1}. {chart}")
        
        print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
        
        return len(created_charts) > 0
    
    except Exception as e:
        print(f"\nä¸¥é‡é”™è¯¯: åˆ›å»ºå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºç°å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False
    finally:
        # ç¡®ä¿æ‰€æœ‰matplotlibå›¾è¡¨éƒ½å·²å…³é—­
        plt.close('all')

def create_additional_visualizations(results, extended_results_by_resolution=None, output_dir=None, plots_to_create=None):
    """
    åˆ›å»ºé¢å¤–çš„å¯è§†åŒ–å›¾è¡¨ï¼ˆéæ ‡å‡†10ä¸ªå›¾è¡¨ï¼‰
    
    å‚æ•°:
    results: æ¨¡å‹ç»“æœå­—å…¸
    extended_results_by_resolution: æ‰©å±•ç»“æœå­—å…¸
    output_dir: è¾“å‡ºç›®å½•
    plots_to_create: è¦åˆ›å»ºçš„å›¾è¡¨åˆ—è¡¨
    
    è¿”å›:
    bool: æ˜¯å¦æˆåŠŸåˆ›å»ºæ‰€æœ‰å›¾è¡¨
    """
    if not output_dir:
        print("é”™è¯¯: æœªæŒ‡å®šè¾“å‡ºç›®å½•")
        return False
    
    success, created_dir = ensure_dir_exists(output_dir)
    if not success:
        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• {output_dir}")
        return False
    
    if plots_to_create is None:
        plots_to_create = ['all']
    
    print("\nå¼€å§‹åˆ›å»ºé¢å¤–å¯è§†åŒ–å›¾è¡¨...")
    
    # é¢å¤–çš„å¯è§†åŒ–å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
    # ä¾‹å¦‚ï¼šSHAPåˆ†å¸ƒç®±çº¿å›¾ã€ç‰¹å¾ç±»åˆ«å¢å¼ºå›¾ç­‰
    
    print("é¢å¤–å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ")
    return True

# è¾…åŠ©å‡½æ•°
def get_simplified_feature_names(results, resolution):
    """
    ä»resultsä¸­è·å–æŒ‡å®šåˆ†è¾¨ç‡çš„ç®€åŒ–ç‰¹å¾åç§°æ˜ å°„
    """
    if resolution not in results:
        return {}
    
    res_data = results[resolution]
    if 'simplified_feature_names' in res_data:
        return res_data['simplified_feature_names']
    
    # å¦‚æœä¸å­˜åœ¨ä½†æœ‰X_sampleï¼Œå°è¯•åˆ›å»º
    if 'X_sample' in res_data and isinstance(res_data['X_sample'], pd.DataFrame):
        feature_names = list(res_data['X_sample'].columns)
        mapping = {name: simplify_feature_name_for_plot(name) for name in feature_names}
        res_data['simplified_feature_names'] = mapping
        return mapping
    
    return {}

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ST-GPRå¯è§†åŒ–è½¬æ¢å™¨æ¨¡å—")
    print("=====================================")
    print("åŠŸèƒ½ï¼šå°†ST-GPRæ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå¯è§†åŒ–æ ¼å¼")
    print("ç”¨æ³•ï¼š")
    print("1. ä»main.pyè°ƒç”¨prepare_stgpr_results_for_visualization()")
    print("2. ç„¶åè°ƒç”¨create_all_visualizations()ç”Ÿæˆ10ä¸ªæ ‡å‡†å›¾è¡¨")
