#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GeoShapleyå†…å­˜ä¼˜åŒ–é…ç½®

è§£å†³ST-GPRæ¨¡å‹åœ¨GeoShapleyè®¡ç®—ä¸­çš„å†…å­˜é—®é¢˜
ä¼˜åŒ–ç­–ç•¥ï¼š
1. ç»Ÿä¸€ä½¿ç”¨6ä¸ªèƒŒæ™¯æ•°æ®ç‚¹
2. å¯¹res7ä½¿ç”¨10%æ—¶ç©ºåˆ†å±‚é‡‡æ ·
3. å¯ç”¨è‡ªé€‚åº”æ‰¹å¤„ç†
"""

# é˜²æ­¢é‡å¤è¾“å‡ºçš„å…¨å±€æ ‡å¿—
_PRINTED_MESSAGES = set()

def print_once(message):
    """åªæ‰“å°ä¸€æ¬¡çš„å‡½æ•°"""
    if message not in _PRINTED_MESSAGES:
        print(message)
        _PRINTED_MESSAGES.add(message)

# ğŸ›¡ï¸ å†…å­˜å®‰å…¨ + æ€§èƒ½ä¼˜åŒ–çš„GeoShapleyå‚æ•° (RTX 4070 SUPER + 28çº¿ç¨‹CPU)
# ç­–ç•¥ï¼šå•è¿›ç¨‹å†…ä¿å®ˆï¼ˆé¿å…KMeanså†…å­˜æ³„æ¼ï¼‰ï¼Œå¤šè¿›ç¨‹é—´æ¿€è¿›ï¼ˆå……åˆ†åˆ©ç”¨CPUï¼‰
MEMORY_OPTIMIZED_GEOSHAPLEY_PARAMS = {
    'n_jobs': 22,  # ğŸš€ å¢åŠ åˆ°22æ ¸å¿ƒå¹¶è¡Œï¼Œè¡¥å¿å•è¿›ç¨‹å†…çº¿ç¨‹é™åˆ¶
    'batch_size': 30,  # ğŸ›¡ï¸ é€‚åº¦æ‰¹æ¬¡å¤§å°ï¼Œå¹³è¡¡å†…å­˜å®‰å…¨ä¸æ•ˆç‡
    'n_background': 8,  # ğŸ¯ å¢åŠ èƒŒæ™¯æ•°æ®ç‚¹ï¼Œæé«˜SHAPå€¼ç¨³å®šæ€§
    'enable_shap_interactions': True,  # å¯ç”¨SHAPäº¤äº’å€¼è®¡ç®—ï¼ˆPDPäº¤äº’å›¾éœ€è¦ï¼‰
    'enable_memory_cleanup': True,  # å¯ç”¨å†…å­˜æ¸…ç†
    'memory_limit_mb': 6144,  # ğŸ›¡ï¸ é€‚åº¦å†…å­˜é™åˆ¶6GBï¼Œé¿å…è¿‡åº¦å ç”¨
    'use_shap_kmeans': True,  # ä½¿ç”¨SHAPçš„K-meansï¼ˆå·²é™åˆ¶å•çº¿ç¨‹ï¼‰
    'timeout_per_sample': 150,  # ğŸ”§ å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé€‚åº”æ›´å¤šè¿›ç¨‹
    'chunk_size': 50,  # ğŸ›¡ï¸ é€‚ä¸­å—å¤§å°ï¼Œå‡å°‘å•è¿›ç¨‹å†…å­˜å‹åŠ›
    'memory_safe_mode': True  # ğŸ›¡ï¸ å¯ç”¨å†…å­˜å®‰å…¨æ¨¡å¼
}

def create_memory_optimized_prediction_function(model_dict, feature_names, batch_size=30):
    """
    åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„é¢„æµ‹å‡½æ•°ï¼Œæ”¯æŒè‡ªé€‚åº”æ‰¹å¤„ç†
    
    å‚æ•°:
    model_dict: æ¨¡å‹å­—å…¸
    feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    batch_size: åˆå§‹æ‰¹å¤„ç†å¤§å°
    
    è¿”å›:
    é¢„æµ‹å‡½æ•°
    """
    import numpy as np
    import pandas as pd
    import torch
    import gc
    import psutil
    from .stgpr_io import predict_with_st_gpr
    
    # ğŸš€ è·å–é’ˆå¯¹RTX 4070 SUPERä¼˜åŒ–çš„æ‰¹å¤„ç†é…ç½®
    from .stgpr_config import GEOSHAPLEY_CONFIG
    batch_config = GEOSHAPLEY_CONFIG.get('batch_processing', {})
    enable_adaptive = batch_config.get('enable_adaptive_batch_size', True)
    memory_threshold_mb = batch_config.get('memory_threshold_mb', 4096)  # ä»1GBå¢åŠ åˆ°4GB
    min_batch_size = batch_config.get('min_batch_size', 50)  # ä»10å¢åŠ åˆ°50
    max_batch_size = batch_config.get('max_batch_size', 200)  # ä»50å¢åŠ åˆ°200
    gc_frequency = batch_config.get('gc_frequency', 10)  # ä»5å¢åŠ åˆ°10ï¼Œå‡å°‘GCå¼€é”€
    
    def prediction_function(x):
        """å†…å­˜ä¼˜åŒ–çš„é¢„æµ‹å‡½æ•°ï¼Œæ”¯æŒè‡ªé€‚åº”æ‰¹å¤„ç†"""
        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        if isinstance(x, pd.DataFrame):
            x_array = x.values
        else:
            x_array = np.asarray(x)
        
        n_samples = x_array.shape[0]
        
        # åˆå§‹æ‰¹æ¬¡å¤§å°
        current_batch_size = min(batch_size, max_batch_size)
        
        if n_samples <= current_batch_size:
            # æ ·æœ¬æ•°å°‘ï¼Œç›´æ¥é¢„æµ‹
            x_df = pd.DataFrame(x_array, columns=feature_names)
            return predict_with_st_gpr(model_dict, x_df, return_variance=False)
        else:
            # åˆ†æ‰¹é¢„æµ‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
            predictions = []
            batch_count = 0
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ä¸»è¿›ç¨‹ä¸­ï¼ˆé¿å…å¹¶è¡Œå¤„ç†æ—¶çš„è¿›åº¦æ˜¾ç¤ºæ··ä¹±ï¼‰
            import multiprocessing
            is_main_process = multiprocessing.current_process().name == 'MainProcess'
            
            # ä½¿ç”¨ç®€æ´çš„è¿›åº¦æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºä¸€æ¬¡æ€»ä½“è¿›åº¦ï¼‰
            total_batches = (n_samples + current_batch_size - 1) // current_batch_size
            last_progress = -1  # è®°å½•ä¸Šæ¬¡æ˜¾ç¤ºçš„è¿›åº¦
            
            i = 0
            while i < n_samples:
                # è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
                if enable_adaptive:
                    memory_info = psutil.virtual_memory()
                    memory_available_mb = memory_info.available / (1024 * 1024)
                    
                    # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°
                    if memory_available_mb < memory_threshold_mb:
                        # å†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹æ¬¡å¤§å°
                        current_batch_size = max(min_batch_size, current_batch_size // 2)
                        # åªåœ¨ç¬¬ä¸€æ¬¡å†…å­˜ä¸è¶³æ—¶æ˜¾ç¤ºè­¦å‘Š
                        if batch_count == 0 and is_main_process:
                            print(f"  âš ï¸ å†…å­˜å—é™ï¼Œä½¿ç”¨è¾ƒå°æ‰¹æ¬¡ï¼ˆ{current_batch_size}æ ·æœ¬/æ‰¹ï¼‰")
                    elif memory_available_mb > memory_threshold_mb * 2:
                        # å†…å­˜å……è¶³ï¼Œå¯ä»¥å¢å¤§æ‰¹æ¬¡å¤§å°
                        current_batch_size = min(max_batch_size, current_batch_size * 1.5)
                        current_batch_size = int(current_batch_size)
                
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸç´¢å¼•
                end_idx = min(i + current_batch_size, n_samples)
                batch_data = x_array[i:end_idx]
                batch_df = pd.DataFrame(batch_data, columns=feature_names)
                
                # æ¯æ‰¹é¢„æµ‹
                try:
                    batch_pred = predict_with_st_gpr(model_dict, batch_df, return_variance=False)
                    predictions.append(batch_pred)
                except Exception as e:
                    # å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œå°è¯•å‡å°æ‰¹æ¬¡å¤§å°
                    if current_batch_size > min_batch_size:
                        current_batch_size = max(min_batch_size, current_batch_size // 2)
                        continue
                    else:
                        raise e
                
                batch_count += 1
                i = end_idx
                
                # å®šæœŸåƒåœ¾å›æ”¶
                if batch_count % gc_frequency == 0:
                    gc.collect()
                    # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦æ˜¾ç¤ºï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­æ˜¾ç¤ºï¼‰
                if is_main_process:
                    current_progress = int((i / n_samples) * 10) * 10  # ä»¥10%ä¸ºå•ä½
                    if current_progress > last_progress:
                        # ä½¿ç”¨ç®€æ´çš„å•è¡Œè¿›åº¦æ˜¾ç¤º
                        print(f"  æ‰¹å¤„ç†è¿›åº¦: {current_progress}% ({i}/{n_samples}æ ·æœ¬)", end='\r', flush=True)
                        last_progress = current_progress
            
            # å®Œæˆæ—¶æ¸…é™¤è¿›åº¦æ˜¾ç¤ºè¡Œï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰
            if is_main_process:
                print(f"  æ‰¹å¤„ç†å®Œæˆ: 100% ({n_samples}/{n_samples}æ ·æœ¬)    ")  # ç”¨ç©ºæ ¼è¦†ç›–ä¹‹å‰çš„å†…å®¹
            
            # æœ€ç»ˆåƒåœ¾å›æ”¶
            gc.collect()
            
            return np.concatenate(predictions)
    
    return prediction_function

def optimize_inducing_points_for_memory(n_samples, res_level):
    """
    ğŸš€ é’ˆå¯¹RTX 4070 SUPER + 28çº¿ç¨‹CPUä¼˜åŒ–è¯±å¯¼ç‚¹æ•°é‡
    
    å‚æ•°:
    n_samples: æ ·æœ¬æ•°é‡
    res_level: åˆ†è¾¨ç‡çº§åˆ«
    
    è¿”å›:
    ä¼˜åŒ–åçš„è¯±å¯¼ç‚¹æ•°é‡
    """
    # ğŸš€ é«˜æ€§èƒ½ç¡¬ä»¶æ”¯æŒæ›´å¤šè¯±å¯¼ç‚¹ï¼Œæå‡æ¨¡å‹ç²¾åº¦
    gpu_optimized_inducing = {
        'res5': min(200, int(n_samples * 0.025)),   # ä»80å¢åŠ åˆ°200ï¼ŒGPUåŠ é€Ÿ
        'res6': min(400, int(n_samples * 0.010)),   # ä»150å¢åŠ åˆ°400ï¼Œå……åˆ†åˆ©ç”¨12GBæ˜¾å­˜
        'res7': min(600, int(n_samples * 0.003))    # ä»250å¢åŠ åˆ°600ï¼Œå¤§å¹…æå‡ç²¾åº¦
    }
    
    optimized_count = gpu_optimized_inducing.get(res_level, 200)
    print(f"  ğŸ¯ {res_level}è¯±å¯¼ç‚¹ä¼˜åŒ–: {optimized_count}ä¸ª (GPUåŠ é€Ÿæ”¯æŒ)")
    return optimized_count

def select_background_data_stratified(X_train, n_background, feature_columns=None):
    """
    ä½¿ç”¨åˆ†å±‚ç­–ç•¥é€‰æ‹©èƒŒæ™¯æ•°æ®ï¼Œç¡®ä¿æ›´å¥½çš„ä»£è¡¨æ€§
    
    å‚æ•°:
    X_train: è®­ç»ƒæ•°æ®
    n_background: èƒŒæ™¯æ•°æ®ç‚¹æ•°é‡
    feature_columns: ç”¨äºåˆ†å±‚çš„ç‰¹å¾åˆ—
    
    è¿”å›:
    èƒŒæ™¯æ•°æ®ç‚¹
    """
    import numpy as np
    import pandas as pd
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    
    # å¦‚æœæ˜¯DataFrameï¼Œè½¬æ¢ä¸ºæ•°ç»„
    if isinstance(X_train, pd.DataFrame):
        X_array = X_train.values
    else:
        X_array = X_train
    
    # å¦‚æœæ ·æœ¬æ•°å°‘äºèƒŒæ™¯ç‚¹æ•°ï¼Œè¿”å›æ‰€æœ‰æ ·æœ¬
    if len(X_array) <= n_background:
        return X_array
    
    # ä½¿ç”¨å…³é”®ç‰¹å¾è¿›è¡Œåˆ†å±‚ï¼ˆå¦‚elevation, temperatureç­‰ï¼‰
    if feature_columns is not None and isinstance(X_train, pd.DataFrame):
        # é€‰æ‹©é‡è¦ç‰¹å¾è¿›è¡Œèšç±»
        key_features = ['elevation', 'temperature', 'precipitation']
        available_features = [f for f in key_features if f in X_train.columns]
        if available_features:
            X_stratify = X_train[available_features].values
            # æ ‡å‡†åŒ–ç”¨äºèšç±»çš„ç‰¹å¾
            scaler = StandardScaler()
            X_stratify = scaler.fit_transform(X_stratify)
        else:
            X_stratify = X_array
    else:
        X_stratify = X_array
    
    # ä½¿ç”¨KMeansé€‰æ‹©ä»£è¡¨æ€§ç‚¹
    kmeans = KMeans(n_clusters=n_background, random_state=42, n_init=10)
    kmeans.fit(X_stratify)
    
    # è·å–æ¯ä¸ªèšç±»ä¸­å¿ƒæœ€è¿‘çš„å®é™…æ ·æœ¬
    background_indices = []
    for center in kmeans.cluster_centers_:
        distances = np.sum((X_stratify - center) ** 2, axis=1)
        closest_idx = np.argmin(distances)
        if closest_idx not in background_indices:
            background_indices.append(closest_idx)
    
    # å¦‚æœä¸å¤Ÿï¼Œéšæœºè¡¥å……
    while len(background_indices) < n_background:
        idx = np.random.randint(0, len(X_array))
        if idx not in background_indices:
            background_indices.append(idx)
    
    return X_array[background_indices]

# å…¨å±€æ ‡å¿—ï¼Œé¿å…é‡å¤è¾“å‡ºé…ç½®ä¿¡æ¯
_CONFIG_ALREADY_APPLIED = False

def apply_memory_optimization():
    """
    åº”ç”¨å†…å­˜ä¼˜åŒ–é…ç½®ï¼ˆçº¯CPUæ¨¡å¼ï¼‰
    
    è¿”å›:
    ä¼˜åŒ–åçš„é…ç½®å­—å…¸
    """
    import os
    import torch
    global _CONFIG_ALREADY_APPLIED
    
    # ğŸ›¡ï¸ å†…å­˜å®‰å…¨é…ç½® - é¿å…Windows KMeanså†…å­˜æ³„æ¼
    # KMeansç›¸å…³åº“å¿…é¡»ä½¿ç”¨å•çº¿ç¨‹ä»¥é¿å…å†…å­˜æ³„æ¼
    if 'OMP_NUM_THREADS' not in os.environ:
        os.environ['OMP_NUM_THREADS'] = '1'  # ä¿æŒ1ï¼Œé¿å…KMeanså†…å­˜æ³„æ¼
    
    # ğŸ”§ PyTorch CPUæ¨¡å¼ä¼˜åŒ–
    torch.set_num_threads(4)  # PyTorch CPUè®¡ç®—çº¿ç¨‹æ•°
    
    # ğŸ›¡ï¸ MKLä¿æŒå•çº¿ç¨‹ï¼ˆKMeansä¾èµ–MKLï¼‰
    if 'MKL_NUM_THREADS' not in os.environ:
        os.environ['MKL_NUM_THREADS'] = '1'  # ä¿æŒ1ï¼Œé¿å…MKLç›¸å…³å†…å­˜æ³„æ¼
    
    # ğŸ”§ æ£€æµ‹å®é™…è®¡ç®—æ¨¡å¼
    compute_mode = "çº¯CPUæ¨¡å¼"
    if torch.cuda.is_available():
        # å³ä½¿GPUå¯ç”¨ï¼Œä¹Ÿå¯èƒ½åœ¨CPUæ¨¡å¼ä¸‹è¿è¡Œ
        device_info = f"GPUå¯ç”¨ä½†ä½¿ç”¨{compute_mode}"
    else:
        device_info = compute_mode
    
    # åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è¾“å‡ºé…ç½®ä¿¡æ¯ï¼Œé¿å…é‡å¤
    if not _CONFIG_ALREADY_APPLIED:
        print_once("å·²åº”ç”¨å†…å­˜å®‰å…¨ + æ€§èƒ½å¹³è¡¡é…ç½®")
        print(f"  â€¢ OMPçº¿ç¨‹æ•°: {os.environ.get('OMP_NUM_THREADS')} (é¿å…KMeanså†…å­˜æ³„æ¼)")
        print(f"  â€¢ PyTorchçº¿ç¨‹æ•°: 4 ({device_info})")
        print(f"  â€¢ MKLçº¿ç¨‹æ•°: {os.environ.get('MKL_NUM_THREADS')} (é¿å…å†…å­˜æ³„æ¼)")
        print(f"  â€¢ GeoShapleyå¹¶è¡Œåº¦: 20æ ¸å¿ƒ (å¤šè¿›ç¨‹CPUå¹¶è¡Œ)")
        print(f"  ç­–ç•¥: å•è¿›ç¨‹å†…ä¿å®ˆï¼Œå¤šè¿›ç¨‹é—´æ¿€è¿›")
        print("å·²å¯ç”¨GeoShapleyå†…å­˜ä¼˜åŒ–é…ç½®")
        _CONFIG_ALREADY_APPLIED = True
    
    return MEMORY_OPTIMIZED_GEOSHAPLEY_PARAMS

def optimize_kernel_computation(model):
    """
    ä¼˜åŒ–æ ¸å‡½æ•°è®¡ç®—ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    
    å‚æ•°:
    model: ST-GPRæ¨¡å‹
    """
    if hasattr(model, 'covar_module'):
        # å¯ç”¨æ‡’æƒ°è®¡ç®—
        model.covar_module.lazily_evaluate_kernels = True
        
        # å‡å°‘ç¼“å­˜å¤§å°
        if hasattr(model.covar_module, 'max_root_decomposition_size'):
            model.covar_module.max_root_decomposition_size = 50
    
    return model 