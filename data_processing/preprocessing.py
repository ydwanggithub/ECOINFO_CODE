#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—ï¼šå¤„ç†H3èšåˆåçš„æ¤è¢«å¥åº·æ•°æ®

ä¸»è¦åŠŸèƒ½ï¼š
1. ç»Ÿä¸€åˆ—åæ ¼å¼
2. æå–åŸºç¡€ç‰¹å¾
3. æ•°æ®è´¨é‡æ£€æŸ¥
4. å‡†å¤‡STGPR+GeoShapleyæ¡†æ¶è¾“å…¥

ä¼˜åŒ–åçš„ç‰¹å¾ä½“ç³»ï¼ˆ14ä¸ªç‰¹å¾ï¼‰ï¼š
- ç©ºé—´ä¿¡æ¯ï¼šlatitude, longitude (2ä¸ª)
- æ°”å€™ç‰¹å¾ï¼štemperature, precipitation (2ä¸ªï¼Œå»æ‰pet)
- äººç±»æ´»åŠ¨ç‰¹å¾ï¼šnightlight, population_density, road_density, mining_density (4ä¸ª)
- åœ°å½¢ç‰¹å¾ï¼šelevation, slope (2ä¸ªï¼Œå»æ‰aspect)
- åœŸåœ°è¦†ç›–ç‰¹å¾ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼šforest_area_percent, cropland_area_percent, impervious_area_percent (3ä¸ªï¼Œå»æ‰grassland/shrubland/bareland)
- æ—¶é—´ç‰¹å¾ï¼šyear (1ä¸ª)

è®¡ç®—æ•ˆç‡æå‡ï¼šGeoShapleyå¤æ‚åº¦ä»O(2^19)é™ä½åˆ°O(2^14)ï¼Œçº¦97%çš„è®¡ç®—é‡å‡å°‘

å¤„ç†çš„æ•°æ®ç»“æ„ï¼ˆ37åˆ—ï¼‰ï¼š
- ç©ºé—´æ ‡è¯†ï¼šh3_index, latitude, longitude, hex_id_res5
- æ—¶é—´æ ‡è¯†ï¼šyear
- æ°”å€™ç‰¹å¾ï¼štemperature, precipitation, pet
- æ¤è¢«æŒ‡æ ‡ï¼šgpp, lai, fpar, evi
- äººç±»æ´»åŠ¨ç‰¹å¾ï¼šnightlight, population_density, road_density, mining_density
- åœŸåœ°è¦†ç›–ç‰¹å¾ï¼ˆé¢ç§¯ï¼‰ï¼šforest_area, cropland_area, grassland_area, shrubland_area, impervious_area, bareland_area
- åœŸåœ°è¦†ç›–ç‰¹å¾ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼šforest_area_percent, cropland_area_percent, grassland_area_percent, shrubland_area_percent, impervious_area_percent, bareland_area_percent
- åœ°å½¢ç‰¹å¾ï¼šelevation, slope, aspect
- ç›®æ ‡å˜é‡ï¼šVHI
- å…¶ä»–ï¼štotal_area_km2, has_valid_data, .geo

ä½œè€…: Yuandong Wang (wangyuandong@gnnu.edu.cn)
æ—¥æœŸ: 2025.07.26
"""

import os
import numpy as np
import pandas as pd
import warnings
from typing import Optional, Tuple, Dict, List

def load_complete_dataset(data_dir: str = None, resolutions: List[str] = None, 
                        verbose: bool = True) -> Dict[str, pd.DataFrame]:
    """
    ç›´æ¥åŠ è½½2000-2024å¹´å®Œæ•´æ•°æ®é›†ï¼ˆåŒ…å«ARIMAå¤–æ¨æ•°æ®ï¼‰
    
    è¿™æ˜¯è®ºæ–‡ä»£ç çš„ä¸»è¦æ•°æ®åŠ è½½æ–¹å¼ï¼Œè¯»è€…å¯ä»¥æ¸…æ¥šçœ‹åˆ°ä½¿ç”¨çš„æ˜¯å®Œæ•´25å¹´æ•°æ®é›†
    
    å‚æ•°:
    data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹ï¼‰
    resolutions: è¦åŠ è½½çš„åˆ†è¾¨ç‡åˆ—è¡¨ï¼Œå¦‚['res5', 'res6', 'res7']
    verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
    dict: åˆ†è¾¨ç‡ä¸ºé”®ï¼ŒDataFrameä¸ºå€¼çš„å­—å…¸
    """
    if verbose:
        print("=== âš¡ åŠ è½½2000-2024å¹´å®Œæ•´æ•°æ®é›† ===")
        print("ğŸ“Š åŒ…å«25å¹´æ•°æ®: 2000-2020è§‚æµ‹æ•°æ® + 2021-2024 ARIMAå¤–æ¨æ•°æ®")
    
    # å¦‚æœæœªæŒ‡å®šæ•°æ®ç›®å½•ï¼Œä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹
    if data_dir is None:
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆdata_processingçš„çˆ¶ç›®å½•ï¼‰
        current_dir = os.path.dirname(os.path.abspath(__file__))  # data_processingç›®å½•
        project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
        data_dir = os.path.join(project_root, 'data')
    
    # å¦‚æœæœªæŒ‡å®šåˆ†è¾¨ç‡ï¼Œåˆ™åŠ è½½æ‰€æœ‰å¯ç”¨çš„
    if resolutions is None:
        resolutions = ['res5', 'res6', 'res7']
    
    data_by_resolution = {}
    
    # æ–‡ä»¶åæ¨¡å¼ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
    file_patterns = {
        'res5': 'ALL_DATA_with_VHI_PCA_res5.csv',
        'res6': 'ALL_DATA_with_VHI_PCA_res6.csv',
        'res7': 'ALL_DATA_with_VHI_PCA_res7.csv'
    }
    
    if verbose:
        print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    
    for resolution in resolutions:
        if resolution not in file_patterns:
            if verbose:
                print(f"âš ï¸  è·³è¿‡æœªæ”¯æŒçš„åˆ†è¾¨ç‡: {resolution}")
            continue
            
        file_path = os.path.join(data_dir, file_patterns[resolution])
        
        if not os.path.exists(file_path):
            if verbose:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
        
        try:
            if verbose:
                print(f"ğŸ“– åŠ è½½ {resolution} æ•°æ®: {file_patterns[resolution]}")
            
            # è¯»å–æ•°æ®
            df = pd.read_csv(file_path)
            
            # æ•°æ®éªŒè¯
            required_columns = [
                'year', 'latitude', 'longitude', 'h3_index', 'VHI',
                'temperature', 'precipitation', 'elevation', 'slope',
                'nightlight', 'population_density', 'road_density', 'mining_density',
                'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'
            ]
            
            missing_cols = [col for col in required_columns if col not in df.columns]
            if missing_cols:
                print(f"âš ï¸  {resolution} æ•°æ®ç¼ºå¤±åˆ—: {missing_cols}")
            
            # æ—¶é—´èŒƒå›´éªŒè¯
            year_range = (df['year'].min(), df['year'].max())
            if verbose:
                print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {year_range[0]}-{year_range[1]} ({year_range[1]-year_range[0]+1}å¹´)")
                print(f"   ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
                print(f"   ğŸ¯ ç‰¹å¾æ•°é‡: {len([col for col in df.columns if col not in ['year', 'h3_index', 'VHI', 'latitude', 'longitude']])}ä¸ª")
            
            # ç¡®è®¤åŒ…å«å®Œæ•´25å¹´æ•°æ®
            if year_range != (2000, 2024):
                if verbose:
                    print(f"âš ï¸  æ³¨æ„: {resolution} æ•°æ®æ—¶é—´èŒƒå›´ä¸æ˜¯é¢„æœŸçš„2000-2024å¹´")
            
            data_by_resolution[resolution] = df
            
            if verbose:
                print(f"âœ… {resolution} æ•°æ®åŠ è½½æˆåŠŸ")
        
        except Exception as e:
            if verbose:
                print(f"âŒ åŠ è½½ {resolution} æ•°æ®å¤±è´¥: {str(e)}")
    
    if verbose:
        print(f"\nğŸ“‹ æ€»ç»“: æˆåŠŸåŠ è½½ {len(data_by_resolution)} ä¸ªåˆ†è¾¨ç‡çš„æ•°æ®é›†")
        for res, df in data_by_resolution.items():
            print(f"  â€¢ {res}: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")
    
    return data_by_resolution

def load_and_check_data(file_path: str, verbose: bool = True) -> pd.DataFrame:
    """è¯»å–æ•°æ®å¹¶è¿›è¡Œåˆå§‹æ£€æŸ¥"""
    if verbose:
        print(f"è¯»å–æ•°æ®æ–‡ä»¶: {file_path}")
    
    df = pd.read_csv(file_path)
    
    if verbose:
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")
    
    # æ£€æŸ¥ç©ºå€¼
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0 and verbose:
        print(f"å‘ç°ç©ºå€¼çš„åˆ—: {null_counts[null_counts > 0].to_dict()}")
    
    return df

def standardize_h3_index(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """æ ‡å‡†åŒ–H3ç´¢å¼•åˆ—"""
    if verbose:
        print("æ ‡å‡†åŒ–H3ç´¢å¼•...")
    
    # 1. æ£€æŸ¥hex_idåˆ—å¹¶é‡å‘½åä¸ºh3_index
    hex_id_columns = [col for col in df.columns if 'hex_id' in col.lower()]
    
    if 'h3_index' not in df.columns and hex_id_columns:
        # ä¼˜å…ˆé€‰æ‹©ä¸å¸¦åç¼€çš„hex_id
        if 'hex_id' in hex_id_columns:
            df['h3_index'] = df['hex_id']
            if verbose:
                print(f"  ä½¿ç”¨hex_idåˆ—ä½œä¸ºh3_index")
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„hex_idåˆ—
            hex_id_col = hex_id_columns[0]
            df['h3_index'] = df[hex_id_col]
            if verbose:
                print(f"  ä½¿ç”¨{hex_id_col}åˆ—ä½œä¸ºh3_index")
    
    # 2. ç¡®ä¿h3_indexæ˜¯å­—ç¬¦ä¸²ç±»å‹
    if 'h3_index' in df.columns:
        df['h3_index'] = df['h3_index'].astype(str)
        if verbose:
            valid_count = df['h3_index'].notna().sum()
            print(f"  h3_indexåˆ—çŠ¶æ€: {valid_count}/{len(df)} ä¸ªæœ‰æ•ˆå€¼")
    
    # 3. åˆ›å»ºå¤‡ä»½åˆ—
    if 'h3_index' in df.columns and 'original_h3_index' not in df.columns:
        df['original_h3_index'] = df['h3_index'].copy()
        if verbose:
            print("  å·²åˆ›å»ºoriginal_h3_indexå¤‡ä»½åˆ—")
    
    return df

def standardize_landcover_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–åœŸåœ°è¦†ç›–ç‰¹å¾å‘½å
    
    ä¼˜åŒ–ååªå¤„ç†ä¿ç•™çš„3ä¸ªåœŸåœ°è¦†ç›–ç‰¹å¾ï¼ˆå»æ‰grassland/shrubland/barelandï¼‰
    """
    if verbose:
        print("æ ‡å‡†åŒ–åœŸåœ°è¦†ç›–ç‰¹å¾...")
    
    # å®šä¹‰é‡å‘½åæ˜ å°„ï¼ˆä¼˜åŒ–ååªä¿ç•™3ä¸ªç‰¹å¾ï¼‰
    landcover_mapping = {
        'forest_area': 'forest_area_percent',
        'cropland_area': 'cropland_area_percent', 
        'crop_area': 'cropland_area_percent',
        'impervious_area': 'impervious_area_percent',
        'imperv_area': 'impervious_area_percent',
        # ç§»é™¤ä»¥ä¸‹æ˜ å°„ï¼ˆä¼˜åŒ–ç­–ç•¥ï¼‰:
        # 'grassland_area': 'grassland_area_percent',
        # 'grass_area': 'grassland_area_percent',
        # 'shrubland_area': 'shrubland_area_percent',
        # 'shrub_area': 'shrubland_area_percent',
        # 'bareland_area': 'bareland_area_percent',
        # 'bare_area': 'bareland_area_percent'
    }
    
    renamed_cols = 0
    for old_name, new_name in landcover_mapping.items():
        if old_name in df.columns and new_name not in df.columns:
            df = df.rename(columns={old_name: new_name})
            renamed_cols += 1
            if verbose:
                print(f"  é‡å‘½å: {old_name} -> {new_name}")
        elif old_name in df.columns and new_name in df.columns:
            # å¦‚æœä¸¤ä¸ªåˆ—éƒ½å­˜åœ¨ï¼Œåˆå¹¶å®ƒä»¬
            if verbose:
                print(f"  åˆå¹¶åˆ—: {old_name} -> {new_name}")
            # ä½¿ç”¨éç©ºå€¼å¡«å……
            mask = df[new_name].isna() & df[old_name].notna()
            if mask.any():
                df.loc[mask, new_name] = df.loc[mask, old_name]
            # åˆ é™¤æ—§åˆ—
            df = df.drop(columns=[old_name])
    
    if verbose and renamed_cols > 0:
        print(f"  æ€»å…±é‡å‘½åäº† {renamed_cols} ä¸ªåœŸåœ°è¦†ç›–åˆ—")
        print(f"  âœ… ä¼˜åŒ–ï¼šåªä¿ç•™forest/cropland/impervious 3ä¸ªåœŸåœ°è¦†ç›–ç‰¹å¾")
    
    return df

def convert_data_types(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """è½¬æ¢æ•°æ®ç±»å‹"""
    if verbose:
        print("è½¬æ¢æ•°æ®ç±»å‹...")
    
    # 1. ç¡®ä¿H3ç›¸å…³åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
    string_columns = ['h3_index', 'original_h3_index', 'hex_id']
    for col in string_columns:
        if col in df.columns:
            non_null_values = df[col].dropna()
            if len(non_null_values) > 0:
                df[col] = df[col].apply(lambda x: str(x) if not pd.isna(x) else x)
                if verbose:
                    print(f"  å°†åˆ— {col} è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹")
    
    # 2. ç¡®ä¿ç»çº¬åº¦æ˜¯æ•°å€¼ç±»å‹
    coord_columns = ['latitude', 'longitude']
    for col in coord_columns:
        if col in df.columns:
            try:
                if df[col].dtype != 'float64':
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    if verbose:
                        print(f"  å°†åˆ— {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
            except Exception as e:
                if verbose:
                    print(f"  è­¦å‘Š: æ— æ³•å°† {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹: {e}")
    
    # 3. ç¡®ä¿æ•°å€¼ç‰¹å¾åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼ˆä¼˜åŒ–åå»æ‰petå’Œaspectï¼‰
    numeric_features = [
        'year', 'temperature', 'precipitation',  # å»æ‰pet
        'gpp', 'lai', 'fpar', 'evi',  # æ¤è¢«æŒ‡æ ‡ï¼ˆåŸå§‹æ•°æ®å¯èƒ½åŒ…å«ï¼Œä½†åç»­ä¼šè¢«æ’é™¤ï¼‰
        'nightlight', 'population_density', 'road_density', 'mining_density',
        'elevation', 'slope',  # å»æ‰aspect
        'VHI', 'total_area_km2'
    ]
    
    # æ·»åŠ åœŸåœ°è¦†ç›–ç™¾åˆ†æ¯”åˆ—ï¼ˆä¼˜åŒ–ååªæœ‰3ä¸ªï¼‰
    landcover_features = [
        'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'
    ]
    numeric_features.extend(landcover_features)
    
    for col in numeric_features:
        if col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                if verbose and df[col].isna().sum() > 0:
                    print(f"  è½¬æ¢ {col} æ—¶äº§ç”Ÿäº† {df[col].isna().sum()} ä¸ªNaNå€¼")
            except Exception as e:
                if verbose:
                    print(f"  è­¦å‘Š: è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    return df

def handle_missing_values(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """å¤„ç†ç¼ºå¤±å€¼"""
    if verbose:
        print("å¤„ç†ç¼ºå¤±å€¼...")
    
    # 1. æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹
    null_ratios = df.isnull().sum() / len(df)
    high_null_cols = null_ratios[null_ratios > 0.9].index.tolist()
    
    if high_null_cols and verbose:
        print(f"  ç©ºå€¼æ¯”ä¾‹è¶…è¿‡90%çš„åˆ—: {high_null_cols}")
    
    # 2. å¯¹æ•°å€¼åˆ—ç”¨å‡å€¼å¡«å……ç©ºå€¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['h3_index', 'original_h3_index'] and df[col].isnull().any():
            if null_ratios[col] <= 0.9:  # åªå¤„ç†ç©ºå€¼æ¯”ä¾‹ä¸å¤ªé«˜çš„åˆ—
                mean_val = df[col].mean()
                df[col] = df[col].fillna(mean_val)
                if verbose:
                    print(f"  ç”¨å‡å€¼ {mean_val:.4f} å¡«å…… {col} çš„ç©ºå€¼")
    
    return df

def extract_basic_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """æå–åŸºç¡€ç‰¹å¾ï¼Œæ’é™¤ä¸éœ€è¦çš„åˆ—"""
    if verbose:
        print("æå–åŸºç¡€ç‰¹å¾...")
    
    # å®šä¹‰è¦ä¿ç•™çš„åŸºç¡€ç‰¹å¾ç±»åˆ«ï¼ˆæ›´æ–°ï¼šå»æ‰5ä¸ªç‰¹å¾ï¼‰
    basic_features = {
        # æ—¶é—´æ ‡è¯†
        'temporal': ['year'],
        
        # æ°”å€™ç‰¹å¾ï¼ˆä»3ä¸ªå‡å°‘åˆ°2ä¸ªï¼‰
        'climate': ['temperature', 'precipitation'],
        
        # åœ°å½¢ç‰¹å¾ï¼ˆä»3ä¸ªå‡å°‘åˆ°2ä¸ªï¼‰
        'terrain': ['elevation', 'slope'],
        
        # äººç±»æ´»åŠ¨ç‰¹å¾ï¼ˆä¿æŒ4ä¸ªä¸å˜ï¼‰
        'human': ['nightlight', 'population_density', 'road_density', 'mining_density'],
        
        # åœŸåœ°è¦†ç›–ç‰¹å¾ï¼ˆä»6ä¸ªå‡å°‘åˆ°3ä¸ªï¼‰
        'landcover': [
            'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'
        ],
        
        # ç›®æ ‡å˜é‡
        'target': ['VHI'],
        
        # ä½ç½®ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ†æï¼Œä½†ä¸ä½œä¸ºæ¨¡å‹ç‰¹å¾ï¼‰
        'location': ['h3_index', 'latitude', 'longitude']
    }
    
    # æ”¶é›†æ‰€æœ‰è¦ä¿ç•™çš„åˆ—
    keep_columns = []
    for category, features in basic_features.items():
        available_features = [f for f in features if f in df.columns]
        keep_columns.extend(available_features)
        if verbose:
            print(f"  {category}: {len(available_features)} ä¸ªç‰¹å¾ - {available_features}")
    
    # æ’é™¤çš„ç‰¹å¾ï¼ˆé¿å…æ•°æ®æ³„éœ²å’Œä¼˜åŒ–çš„ç‰¹å¾ï¼‰
    exclude_features = [
        'gpp', 'lai', 'fpar', 'evi',  # åŸå§‹æ¤è¢«æŒ‡æ ‡
        'total_area', 'total_area_km2',  # é¢ç§¯ä¿¡æ¯
        'has_valid_data', '.geo',  # å…ƒæ•°æ®
        'original_h3_index',  # å¤‡ä»½åˆ—
        # ğŸ”´ æ–°å¢ï¼šæ˜ç¡®æ’é™¤çš„ä¼˜åŒ–ç‰¹å¾
        'pet',  # æ½œåœ¨è’¸æ•£å‘
        'aspect',  # å¡å‘
        'grassland_area_percent',  # è‰åœ°è¦†ç›–ç™¾åˆ†æ¯”
        'shrubland_area_percent',  # çŒæœ¨è¦†ç›–ç™¾åˆ†æ¯”
        'bareland_area_percent'  # è£¸åœ°è¦†ç›–ç™¾åˆ†æ¯”
    ]
    
    # ä»ä¿ç•™åˆ—è¡¨ä¸­ç§»é™¤è¦æ’é™¤çš„ç‰¹å¾
    keep_columns = [col for col in keep_columns if col not in exclude_features]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æœªåˆ†ç±»çš„æ•°å€¼åˆ—
    all_numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    unclassified_cols = [col for col in all_numeric_cols if col not in keep_columns and col not in exclude_features]
    
    if unclassified_cols and verbose:
        print(f"  å‘ç°æœªåˆ†ç±»çš„æ•°å€¼åˆ—: {unclassified_cols}")
        # å¯ä»¥é€‰æ‹©æ˜¯å¦åŒ…å«è¿™äº›åˆ—
        # keep_columns.extend(unclassified_cols)
    
    # æå–åŸºç¡€ç‰¹å¾
    basic_df = df[keep_columns].copy()
    
    if verbose:
        print(f"  æœ€ç»ˆåŸºç¡€ç‰¹å¾æ•°æ®é›†: {basic_df.shape[0]} è¡Œ Ã— {basic_df.shape[1]} åˆ—")
        print(f"  ä¿ç•™çš„åˆ—: {list(basic_df.columns)}")
    
    return basic_df

def preprocess_for_basic_features(file_path: str, save_path: Optional[str] = None, 
                                 verbose: bool = True) -> pd.DataFrame:
    """
    å®Œæ•´çš„åŸºç¡€ç‰¹å¾é¢„å¤„ç†æµç¨‹
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…ç”¨äºå¤„ç†åŸå§‹æ•°æ®ã€‚ä¸»è¦å·¥ä½œæµç¨‹è¯·ä½¿ç”¨load_complete_dataset()å‡½æ•°ã€‚
    
    å‚æ•°:
    file_path (str): åŸå§‹CSVæ–‡ä»¶è·¯å¾„
    save_path (str): ä¿å­˜å¤„ç†åæ•°æ®çš„è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
    DataFrame: å¤„ç†åçš„åŸºç¡€ç‰¹å¾æ•°æ®
    """
    if verbose:
        print("=" * 50)
        print("å¼€å§‹åŸºç¡€ç‰¹å¾é¢„å¤„ç†æµç¨‹")
        print("ğŸ’¡ æç¤º: å¯¹äºå¸¸è§„ä½¿ç”¨ï¼Œå»ºè®®ä½¿ç”¨load_complete_dataset()å‡½æ•°")
        print("=" * 50)
    
    # æ­¥éª¤1: è¯»å–æ•°æ®
    df = load_and_check_data(file_path, verbose)
    
    # æ­¥éª¤2: H3ç´¢å¼•æ ‡å‡†åŒ–
    df = standardize_h3_index(df, verbose)
    
    # æ­¥éª¤3: åœŸåœ°è¦†ç›–ç‰¹å¾æ ‡å‡†åŒ–
    df = standardize_landcover_features(df, verbose)
    
    # æ­¥éª¤4: æ•°æ®ç±»å‹è½¬æ¢
    df = convert_data_types(df, verbose)
    
    # æ­¥éª¤5: ç¼ºå¤±å€¼å¤„ç†
    df = handle_missing_values(df, verbose)
    
    # æ­¥éª¤6: æå–åŸºç¡€ç‰¹å¾
    basic_df = extract_basic_features(df, verbose)
    
    # æ­¥éª¤7: è·³è¿‡æ—¶é—´å¤–æ¨ (ç°åœ¨ä½¿ç”¨é¢„å¤„ç†å¥½çš„å®Œæ•´æ•°æ®é›†)
    if verbose:
        print(f"\nğŸ’¡ æ•°æ®å¤„ç†å®Œæˆï¼Œæ—¶é—´èŒƒå›´: {basic_df['year'].min()}-{basic_df['year'].max()}")
    
    # æ­¥éª¤8: æœ€ç»ˆæ£€æŸ¥
    if verbose:
        print("\næœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"  æ•°æ®å½¢çŠ¶: {basic_df.shape}")
        print(f"  æ—¶é—´èŒƒå›´: {basic_df['year'].min()}-{basic_df['year'].max()} ({basic_df['year'].nunique()}å¹´)")
        print(f"  ç©ºé—´èŒƒå›´: {basic_df['h3_index'].nunique()}ä¸ªH3ç½‘æ ¼")
        print(f"  ç©ºå€¼æ€»æ•°: {basic_df.isnull().sum().sum()}")
        print(f"  æ•°å€¼åˆ—æ•°: {len(basic_df.select_dtypes(include=[np.number]).columns)}")
        
        # VHIç›®æ ‡å˜é‡æ£€æŸ¥
        if 'VHI' in basic_df.columns:
            vhi_stats = basic_df.groupby('year')['VHI'].agg(['count', 'mean', lambda x: x.isna().sum()])
            vhi_stats.columns = ['total_records', 'mean_vhi', 'null_count']
            print(f"  ğŸ¯ VHIåˆ†å¸ƒ:")
            for year, row in vhi_stats.iterrows():
                print(f"    {year}: {row['total_records']}æ¡è®°å½•, å‡å€¼={row['mean_vhi']:.3f}, ç©ºå€¼={row['null_count']}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾çš„åŸºæœ¬ç»Ÿè®¡
        print("\nåŸºç¡€ç‰¹å¾ç»Ÿè®¡:")
        for col in basic_df.columns:
            if col not in ['h3_index', 'latitude', 'longitude']:
                if pd.api.types.is_numeric_dtype(basic_df[col]):
                    print(f"  {col}: å‡å€¼={basic_df[col].mean():.4f}, æ ‡å‡†å·®={basic_df[col].std():.4f}")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    if save_path:
        try:
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            basic_df.to_csv(save_path, index=False)
            if verbose:
                print(f"\nå¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            if verbose:
                print(f"\nä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
    
    if verbose:
        print("=" * 50)
        print("åŸºç¡€ç‰¹å¾é¢„å¤„ç†å®Œæˆ")
        print("=" * 50)
    
    return basic_df

def process_single_file(file_path: str, verbose: bool = True) -> pd.DataFrame:
    """
    å¤„ç†å•ä¸ªæ•°æ®æ–‡ä»¶ï¼šæ ‡å‡†åŒ–åˆ—åã€æå–åŸºç¡€ç‰¹å¾ã€æ•°æ®æ¸…ç†
    
    å‚æ•°:
    file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
    DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
    """
    if verbose:
        print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(file_path)
    
    if verbose:
        print(f"  åŸå§‹æ•°æ®: {len(df)}è¡Œ Ã— {df.shape[1]}åˆ—")
    
    # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
    df_processed = standardize_column_names(df)
    df_processed = extract_basic_features(df_processed, verbose=verbose)
    df_processed = clean_data(df_processed, verbose=verbose)
    
    if verbose:
        print(f"  å¤„ç†å: {len(df_processed)}è¡Œ Ã— {df_processed.shape[1]}åˆ—")
        year_range = (df_processed['year'].min(), df_processed['year'].max())
        print(f"  æ—¶é—´èŒƒå›´: {year_range[0]}-{year_range[1]}")
    
    return df_processed

def load_data_files(data_dir: str, resolutions: List[str] = None, 
                   verbose: bool = True, force_reprocess: bool = False) -> Dict[str, pd.DataFrame]:
    """
    åŠ è½½æŒ‡å®šåˆ†è¾¨ç‡çš„æ•°æ®æ–‡ä»¶
    
    ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä»é¢„å¤„ç†åçš„æ–‡ä»¶è¯»å–ï¼Œé¿å…é‡å¤é¢„å¤„ç†
    
    å‚æ•°:
    data_dir: æ•°æ®ç›®å½•è·¯å¾„
    resolutions: è¦åŠ è½½çš„åˆ†è¾¨ç‡åˆ—è¡¨ï¼Œå¦‚['res5', 'res6', 'res7']ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰å¯ç”¨çš„
    verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰çš„é¢„å¤„ç†æ–‡ä»¶ï¼‰
    
    è¿”å›:
    dict: åˆ†è¾¨ç‡ä¸ºé”®ï¼ŒDataFrameä¸ºå€¼çš„å­—å…¸
    """
    if verbose:
        print("=== ğŸš€ åŠ è½½æ•°æ®æ–‡ä»¶ ===")
        if not force_reprocess:
            print("ğŸ“ˆ ä¼˜å…ˆä½¿ç”¨2000-2024å¹´å®Œæ•´æ•°æ®é›†")
    
    # ä½¿ç”¨æ–°çš„load_complete_datasetå‡½æ•°
    return load_complete_dataset(data_dir=data_dir, resolutions=resolutions, verbose=verbose)

def load_processed_data_files(resolutions: List[str] = None, verbose: bool = True) -> Dict[str, pd.DataFrame]:
    """
    ç›´æ¥ä»é¢„å¤„ç†åçš„æ•°æ®æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•é¢„å¤„ç†
    
    è¿™æ˜¯æœ€å¿«çš„æ•°æ®åŠ è½½æ–¹å¼ï¼Œé€‚åˆæ¨¡å‹å·²ç»è®­ç»ƒå¥½ï¼Œåªéœ€è¦å¿«é€ŸåŠ è½½æ•°æ®çš„åœºæ™¯
    
    å‚æ•°:
    resolutions: è¦åŠ è½½çš„åˆ†è¾¨ç‡åˆ—è¡¨ï¼Œå¦‚['res5', 'res6', 'res7']ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰å¯ç”¨çš„
    verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
    dict: åˆ†è¾¨ç‡ä¸ºé”®ï¼ŒDataFrameä¸ºå€¼çš„å­—å…¸
    """
    if verbose:
        print("=== âš¡ ç›´æ¥åŠ è½½é¢„å¤„ç†æ•°æ®æ–‡ä»¶ ===")
        print("ğŸ¯ è·³è¿‡æ‰€æœ‰é¢„å¤„ç†æ­¥éª¤ï¼Œç›´æ¥è¯»å–å·²å¤„ç†çš„æ•°æ®")
    
    # å¦‚æœæœªæŒ‡å®šåˆ†è¾¨ç‡ï¼Œåˆ™å°è¯•åŠ è½½æ‰€æœ‰å¯ç”¨çš„
    if resolutions is None:
        resolutions = ['res5', 'res6', 'res7']
    
    data_by_resolution = {}
    
    # é¢„å¤„ç†æ•°æ®å­˜å‚¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))  # data_processingç›®å½•
    processed_data_dir = os.path.join(current_dir, 'data')
    
    if verbose:
        print(f"ğŸ“‚ é¢„å¤„ç†æ•°æ®ç›®å½•: {processed_data_dir}")
    
    for res in resolutions:
        processed_filename = f'ALL_DATA_with_VHI_PCA_{res}_processed.csv'
        processed_file_path = os.path.join(processed_data_dir, processed_filename)
        
        try:
            if not os.path.exists(processed_file_path):
                if verbose:
                    print(f"âš ï¸ {res}: é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {processed_filename}")
                continue
            
            if verbose:
                print(f"ğŸ“– åŠ è½½ {res}: {processed_filename}")
            
            # ç›´æ¥è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(processed_file_path)
            
            # åŸºæœ¬éªŒè¯
            if len(df) == 0:
                if verbose:
                    print(f"  âŒ {res}: æ–‡ä»¶ä¸ºç©º")
                continue
            
            # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = ['h3_index', 'latitude', 'longitude', 'year', 'VHI']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                if verbose:
                    print(f"  âš ï¸ {res}: ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
            
            # æ•°æ®æ‘˜è¦
            file_size = os.path.getsize(processed_file_path) / (1024 * 1024)  # MB
            memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
            
            if 'year' in df.columns:
                year_range = (df['year'].min(), df['year'].max())
                has_extrapolated_data = df['year'].max() >= 2021
                time_status = "å«å¤–æ¨æ•°æ®" if has_extrapolated_data else "ä»…å†å²æ•°æ®"
            else:
                year_range = ("æœªçŸ¥", "æœªçŸ¥")
                time_status = "æ— å¹´ä»½ä¿¡æ¯"
            
            if verbose:
                print(f"  âœ… æˆåŠŸ: {df.shape[0]:,}è¡Œ Ã— {df.shape[1]}åˆ—")
                print(f"    ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.1f}MB")
                print(f"    ğŸ’¾ å†…å­˜å ç”¨: {memory_mb:.1f}MB")
                print(f"    ğŸ“… æ—¶é—´èŒƒå›´: {year_range[0]}-{year_range[1]} ({time_status})")
            
            data_by_resolution[res] = df
            
        except Exception as e:
            if verbose:
                print(f"âŒ {res}: åŠ è½½å¤±è´¥: {e}")
            continue
    
    if verbose:
        total_loaded = len(data_by_resolution)
        print(f"\nâœ… å¿«é€ŸåŠ è½½å®Œæˆ: {total_loaded}/{len(resolutions)} ä¸ªåˆ†è¾¨ç‡")
        
        if total_loaded == 0:
            print("ğŸ’¡ æç¤º: å¦‚æœé¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´çš„æ•°æ®åŠ è½½æµç¨‹ç”Ÿæˆé¢„å¤„ç†æ–‡ä»¶")
            print("       æˆ–ä½¿ç”¨ load_data_files() å‡½æ•°è¿›è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†")
    
    return data_by_resolution

def prepare_features_for_stgpr(df: pd.DataFrame, target: str = 'VHI') -> Tuple[pd.DataFrame, pd.Series]:
    """
    ä¸ºST-GPRæ¨¡å‹å‡†å¤‡ç‰¹å¾ï¼Œä¼˜åŒ–åä½¿ç”¨14ä¸ªç‰¹å¾ï¼ˆä»19ä¸ªå‡å°‘ï¼‰
    
    ä¼˜åŒ–ç­–ç•¥ï¼šå»æ‰5ä¸ªç›¸å¯¹ä¸é‡è¦çš„ç‰¹å¾ä»¥å¤§å¹…æå‡GeoShapleyè®¡ç®—æ•ˆç‡
    
    ä¼˜åŒ–åçš„ST-GPRæ¨¡å‹ä½¿ç”¨14ä¸ªç‰¹å¾ï¼š
    - ç©ºé—´ä¿¡æ¯: latitude, longitude (2ä¸ª)
    - ç¯å¢ƒç‰¹å¾ (11ä¸ª):
      - æ°”å€™å› ç´ : temperature, precipitation (2ä¸ªï¼Œå»æ‰pet)
      - äººç±»æ´»åŠ¨å› ç´ : nightlight, road_density, mining_density, population_density (4ä¸ª)
      - åœ°å½¢å› ç´ : elevation, slope (2ä¸ªï¼Œå»æ‰aspect)
      - åœŸåœ°è¦†ç›–å› ç´ : forest_area_percent, cropland_area_percent, impervious_area_percent (3ä¸ªï¼Œå»æ‰grassland/shrubland/bareland)
    - æ—¶é—´ä¿¡æ¯: year (1ä¸ª)
    
    è®¡ç®—æ•ˆç‡æå‡ï¼šGeoShapleyå¤æ‚åº¦ä»O(2^19)é™ä½åˆ°O(2^14)ï¼Œçº¦97%çš„è®¡ç®—é‡å‡å°‘
    
    å‚æ•°:
    df: è¾“å…¥æ•°æ®æ¡†
    target: ç›®æ ‡å˜é‡åç§°
    
    è¿”å›:
    tuple: (ç‰¹å¾çŸ©é˜µX, ç›®æ ‡å˜é‡y)
    """
    print("ğŸ¯ ä¸ºST-GPRæ¨¡å‹å‡†å¤‡ä¼˜åŒ–åçš„åŸºç¡€ç‰¹å¾ï¼ˆ14ä¸ªç‰¹å¾ï¼‰...")
    
    # æŒ‰ç…§ä¼˜åŒ–åè®¾è®¡ä¸¥æ ¼å®šä¹‰åŸºç¡€ç‰¹å¾åˆ—è¡¨
    base_features = []
    
    # ä½ç½®ç‰¹å¾ (å¿…é¡»åŒ…å«)
    location_cols = ['latitude', 'longitude']
    for col in location_cols:
        if col in df.columns:
            base_features.append(col)
        else:
            print(f"âš ï¸ ç¼ºå°‘å…³é”®ä½ç½®ç‰¹å¾ '{col}'")
    
    # ç¯å¢ƒç‰¹å¾ï¼šä¼˜åŒ–åçš„11ä¸ªåŸºç¡€ç‰¹å¾
    # æŒ‰é¡ºåºï¼šæ°”å€™ã€äººç±»æ´»åŠ¨ã€åœ°å½¢ã€åœŸåœ°è¦†ç›–
    env_features = [
        # æ°”å€™å› ç´  (2ä¸ªï¼Œå»æ‰pet)
        'temperature', 'precipitation',
        # äººç±»æ´»åŠ¨å› ç´  (4ä¸ªï¼Œä¿æŒä¸å˜)
        'nightlight', 'road_density', 'mining_density', 'population_density',
        # åœ°å½¢å› ç´  (2ä¸ªï¼Œå»æ‰aspect)
        'elevation', 'slope',
        # åœŸåœ°è¦†ç›–å› ç´  (3ä¸ªï¼Œå»æ‰grassland/shrubland/bareland)
        'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'
    ]
    
    # æ£€æŸ¥å¹¶æ·»åŠ å¯ç”¨çš„ç¯å¢ƒç‰¹å¾
    available_env_features = []
    missing_env_features = []
    removed_features = ['pet', 'aspect', 'grassland_area_percent', 'shrubland_area_percent', 'bareland_area_percent']
    
    for feature in env_features:
        if feature in df.columns:
            base_features.append(feature)
            available_env_features.append(feature)
        else:
            missing_env_features.append(feature)
    
    if missing_env_features:
        print(f"âš ï¸ è­¦å‘Š: ç¼ºå°‘ä»¥ä¸‹ç¯å¢ƒç‰¹å¾: {', '.join(missing_env_features)}")
    
    print(f"âœ… ä¼˜åŒ–ç§»é™¤çš„ç‰¹å¾: {', '.join(removed_features)}")
    print(f"ğŸ“ˆ GeoShapleyè®¡ç®—æ•ˆç‡æå‡: çº¦97%çš„è®¡ç®—é‡å‡å°‘")
    
    # å¹´ä»½ (å¿…é¡»åŒ…å«)
    if 'year' in df.columns:
        base_features.append('year')
    else:
        print("âš ï¸ è­¦å‘Š: ç¼ºå°‘å¹´ä»½åˆ— 'year'")
    
    # æ£€æŸ¥ç‰¹å¾æ•°é‡ - ä¼˜åŒ–åæœŸæœ›14ä¸ªç‰¹å¾ï¼šç»çº¬åº¦ã€11ä¸ªç¯å¢ƒç‰¹å¾ã€å¹´ä»½
    expected_features = 14
    if len(base_features) < expected_features:  
        print(f"âš ï¸ è­¦å‘Š: ç‰¹å¾æ•°é‡({len(base_features)})å°‘äºé¢„æœŸ({expected_features})ï¼Œç¼ºå°‘çš„ç‰¹å¾å¯èƒ½ä¼šå½±å“æ¨¡å‹æ€§èƒ½")
    elif len(base_features) == expected_features:
        print(f"âœ… ç‰¹å¾æ•°é‡æ­£ç¡®: {len(base_features)}ä¸ªç‰¹å¾")
    else:
        print(f"âš ï¸ ç‰¹å¾æ•°é‡({len(base_features)})è¶…è¿‡é¢„æœŸ({expected_features})")
        
    print(f"ğŸ”§ ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(base_features)}")
    print(f"ğŸ“‹ ç‰¹å¾åˆ—è¡¨: {base_features}")

    # æ„å»ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡
    X = df[base_features].copy()
    y = df[target].copy() if target in df.columns else pd.Series()

    # å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼
    for col in X.columns:
        if X[col].isna().any():
            print(f"ğŸ”§ ç‰¹å¾'{col}'å­˜åœ¨ç¼ºå¤±å€¼ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
            X[col] = X[col].fillna(X[col].median())
    
    print(f"ğŸ“Š æœ€ç»ˆç‰¹å¾çŸ©é˜µ: {X.shape[0]}è¡Œ Ã— {X.shape[1]}åˆ—")
    
    # æ·»åŠ ç‰¹å¾ç±»åˆ«ä¿¡æ¯ï¼ˆæ›´æ–°ä¸ºä¼˜åŒ–åçš„åˆ†ç±»ï¼‰
    feature_categories = {}
    # æŒ‰ä¼˜åŒ–åè®¾è®¡å®šä¹‰ç‰¹å¾ç±»åˆ«
    for col in X.columns:
        if col in ['latitude', 'longitude']:
            category = 'Spatial'
        elif col in ['temperature', 'precipitation']:  # æ›´æ–°ï¼šå»æ‰pet
            category = 'Climate'
        elif col in ['nightlight', 'road_density', 'mining_density', 'population_density']:
            category = 'Human'
        elif col in ['elevation', 'slope']:  # æ›´æ–°ï¼šå»æ‰aspect
            category = 'Terrain'
        elif col in ['forest_area_percent', 'cropland_area_percent', 'impervious_area_percent']:  # æ›´æ–°ï¼šå»æ‰3ä¸ª
            category = 'Land Cover'
        elif col == 'year':
            category = 'Temporal'
        else:
            # ğŸ”´ ä¸å†é»˜è®¤å½’ç±»ï¼Œè€Œæ˜¯è®°å½•å¹¶è­¦å‘Š
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•åˆ†ç±»ç‰¹å¾ '{col}'ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿåœ¨ä¼˜åŒ–åçš„ST-GPRæ¨¡å‹ä¸­")
            print(f"   ä¼˜åŒ–åçš„ST-GPRæ¨¡å‹åº”è¯¥åªåŒ…å«14ä¸ªé¢„å®šä¹‰çš„ç‰¹å¾")
            category = 'Unknown'  # ä½¿ç”¨Unknownè€Œä¸æ˜¯Spatialï¼Œæ›´æ˜ç¡®è¡¨ç¤ºæœ‰é—®é¢˜
        
        feature_categories[col] = category
    
    # å°†ç‰¹å¾ç±»åˆ«åˆ†ç»„ï¼ˆæ›´æ–°ä¸ºä¼˜åŒ–åçš„åˆ†ç»„ï¼‰
    feature_categories_grouped = {
        'Climate': [f for f in X.columns if feature_categories.get(f) == 'Climate'],
        'Human': [f for f in X.columns if feature_categories.get(f) == 'Human'],
        'Terrain': [f for f in X.columns if feature_categories.get(f) == 'Terrain'],
        'Land Cover': [f for f in X.columns if feature_categories.get(f) == 'Land Cover'],
        'Spatial': [f for f in X.columns if feature_categories.get(f) == 'Spatial'],
        'Temporal': [f for f in X.columns if feature_categories.get(f) == 'Temporal']
    }
    
    # ç‰¹å¾ç®€å†™æ˜ å°„ - æ›´æ–°ä¸ºä¼˜åŒ–åçš„ç‰¹å¾
    feature_abbreviations = {
        'latitude': 'LAT', 'longitude': 'LONG',
        'temperature': 'TEMP', 'precipitation': 'PREC',  # å»æ‰pet
        'nightlight': 'NIGH', 'road_density': 'RD', 'mining_density': 'MD', 'population_density': 'PD',
        'elevation': 'ELEV', 'slope': 'SLOP',  # å»æ‰aspect
        'forest_area_percent': 'FAP', 'cropland_area_percent': 'CAP', 'impervious_area_percent': 'IAP',  # å»æ‰GAPã€SAPã€BAP
        'year': 'YEAR'
    }
    
    # å­˜å‚¨ç‰¹å¾å…ƒä¿¡æ¯
    try:
        X.attrs['feature_categories'] = feature_categories
        X.attrs['feature_categories_grouped'] = feature_categories_grouped
        X.attrs['feature_names'] = list(base_features)
        X.attrs['feature_abbreviations'] = feature_abbreviations
        print("âœ… ç‰¹å¾å…ƒä¿¡æ¯å·²é™„åŠ åˆ°DataFrame")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•å°†ç‰¹å¾ä¿¡æ¯é™„åŠ åˆ°DataFrame: {e}")
    
    # ğŸ”¥ æ‰“å°ä¼˜åŒ–æ•ˆæœæ‘˜è¦
    print(f"\nğŸ“ˆ ç‰¹å¾ä¼˜åŒ–æ•ˆæœæ‘˜è¦:")
    print(f"  â€¢ ç‰¹å¾æ•°é‡: 19 â†’ 14 (-5ä¸ªç‰¹å¾)")
    print(f"  â€¢ GeoShapleyå¤æ‚åº¦: 2^19 â†’ 2^14 (çº¦97%è®¡ç®—é‡å‡å°‘)")
    print(f"  â€¢ ç§»é™¤çš„ç‰¹å¾: pet, aspect, grassland/shrubland/barelandè¦†ç›–ç‡")
    print(f"  â€¢ ä¿ç•™æ ¸å¿ƒç‰¹å¾: æ°”å€™2ä¸ª, äººç±»æ´»åŠ¨4ä¸ª, åœ°å½¢2ä¸ª, åœŸåœ°è¦†ç›–3ä¸ª, æ—¶ç©º3ä¸ª")
    
    return X, y

# å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒä¸åŸæœ‰ä»£ç çš„å…¼å®¹æ€§
def preprocess_data(df: pd.DataFrame, verbose: bool = True, filepath: Optional[str] = None, 
                   resolution: Optional[str] = None, overwrite_original: bool = False) -> pd.DataFrame:
    """
    é¢„å¤„ç†æ•°æ®çš„å…¼å®¹æ€§åŒ…è£…å‡½æ•°
    
    å‚æ•°:
    df: è¾“å…¥æ•°æ®æ¡†
    verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    filepath: å¦‚æœæä¾›ï¼Œä¼šå°†å¤„ç†åçš„æ•°æ®ä¿å­˜åˆ°è¯¥è·¯å¾„
    resolution: åˆ†è¾¨ç‡çº§åˆ«ï¼ˆres5, res6, res7ï¼‰
    overwrite_original: æ˜¯å¦è¦†ç›–åŸå§‹æ–‡ä»¶ï¼Œé»˜è®¤ä¸ºFalse
    
    è¿”å›:
    DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
    """
    if verbose:
        print(f"  ğŸ”§ æ•°æ®é¢„å¤„ç† - åˆå§‹å½¢çŠ¶: {df.shape}")
    
    # ä½¿ç”¨æ–°çš„é¢„å¤„ç†æµç¨‹
    # 1. H3ç´¢å¼•æ ‡å‡†åŒ–
    df = standardize_h3_index(df, verbose)
    
    # 2. åœŸåœ°è¦†ç›–ç‰¹å¾æ ‡å‡†åŒ–
    df = standardize_landcover_features(df, verbose)
    
    # 3. æ•°æ®ç±»å‹è½¬æ¢
    df = convert_data_types(df, verbose)
    
    # 4. ç¼ºå¤±å€¼å¤„ç†
    df = handle_missing_values(df, verbose)
    
    if verbose:
        print(f"   âœ… é¢„å¤„ç†å®Œæˆ: {df.shape}")
        
    return df

# ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œä¿ç•™ä¸€äº›åŸæœ‰çš„å‡½æ•°ç­¾å
def get_data_summary(data_by_resolution: Dict[str, pd.DataFrame], verbose: bool = True) -> Dict:
    """è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯"""
    summary = {}
    
    for res, df in data_by_resolution.items():
        summary[res] = {
            'shape': df.shape,
            'columns': list(df.columns),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'null_counts': df.isnull().sum().to_dict()
        }
        
        if verbose:
            print(f"{res}: {df.shape[0]:,}è¡Œ Ã— {df.shape[1]}åˆ—")
    
    return summary

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„æ•°æ®
    resolutions = ['res7', 'res6', 'res5']
    
    for res in resolutions:
        # ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # é¡¹ç›®æ ¹ç›®å½•
        input_file = os.path.join(project_root, 'data', f'ALL_DATA_with_VHI_PCA_{res}.csv')
        
        # ä¿®å¤ï¼šå°†è¾“å‡ºæ–‡ä»¶ä¿å­˜åˆ°data_processing/dataç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))  # data_processingç›®å½•
        output_dir = os.path.join(current_dir, 'data')
        output_file = os.path.join(output_dir, f'ALL_DATA_with_VHI_PCA_{res}_basic_features.csv')
        
        if os.path.exists(input_file):
            print(f"\nå¤„ç† {res} åˆ†è¾¨ç‡æ•°æ®...")
            basic_features_df = preprocess_for_basic_features(
                input_file, 
                output_file, 
                verbose=True
            )
        else:
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}") 