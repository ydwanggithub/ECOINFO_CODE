#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
STGPR+GeoShapley æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’å¯è§£é‡Šæ€§åˆ†ææ¡†æ¶ - ä¸»æ–‡ä»¶

æœ¬æ¨¡å—å®ç°äº†åŸºäºSTGPRæ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’ä¸GeoShapleyå¯è§£é‡Šæ€§åˆ†æçš„å»ºæ¨¡å’Œåˆ†æåŠŸèƒ½ä¸»æµç¨‹ï¼Œ
ç”¨äºæ¢ç©¶ä¸˜é™µå±±åœ°æ¤è¢«å¥åº·å¯¹ç¯å¢ƒå˜åŒ–çš„æ»åå“åº”ç‰¹å¾åŠåœ°å½¢è°ƒèŠ‚æœºåˆ¶ã€‚

ä½œè€…: Yuandong Wang (wangyuandong@gnnu.edu.cn)
æ—¥æœŸ: 2025.07.26
"""

# å¿½ç•¥tqdmçš„IProgressè­¦å‘Š
import warnings
warnings.filterwarnings("ignore", message="IProgress not found")

# é¦–å…ˆè®¾ç½®ç¯å¢ƒå˜é‡å’Œè·¯å¾„
from model_analysis.stgpr_config import setup_environment, configure_python_path, get_config, PROJECT_INFO, DATA_CONFIG

# è®¾ç½®ç¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
setup_environment()

# é…ç½®Pythonè·¯å¾„
configure_python_path()

# ä»å·¥å…·æ¨¡å—å¯¼å…¥å‡½æ•°ï¼ˆå»é™¤å†—ä½™å¯¼å…¥ï¼‰
from model_analysis.stgpr_utils import (
    clean_pycache,
    check_module_availability,
    create_train_evaluate_wrapper,
    ensure_dir_exists,
    prepare_features_for_stgpr,
    sample_data_for_testing,
    explain_stgpr_predictions,
    perform_spatiotemporal_sampling  # ç»Ÿä¸€ä»stgpr_utilså¯¼å…¥
)

# å¯¼å…¥å…¶ä»–å¿…è¦çš„æ¨¡å—
import os
import sys
import time
import traceback
import numpy as np
import pandas as pd
import argparse
import torch
import json

# ä»model_analysiså¯¼å…¥å¯è§†åŒ–æ¨¡å—
from model_analysis.stgpr_visualization import create_additional_visualizations

# æ¸…ç†ç¼“å­˜
clean_pycache()

# è·å–é…ç½®
CONFIG = get_config()
RANDOM_SEED = CONFIG['random_seed']
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

# æ‰“å°é¡¹ç›®ä¿¡æ¯
print(f"=== {PROJECT_INFO['name']} ===")
print(f"{PROJECT_INFO['description']}")
print(f"ç‰ˆæœ¬: {PROJECT_INFO['version']}\n")

def main(data_dir=None, output_dir=None, plots_to_create=None, use_parallel=False, 
         n_processes=4, data_resolutions=None, use_hyperopt=True, 
         max_hyperopt_evals=10, skip_validation=False, use_processed_data=True):
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒSTGPR+GeoShapleyå¯è§£é‡Šæ€§åˆ†æ
    
    å‚æ•°:
    use_processed_data: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®æ–‡ä»¶ï¼ˆé»˜è®¤Trueï¼Œå¤§å¹…æå‡åŠ è½½é€Ÿåº¦ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    start_time = time.time()
    
    print(f"\n====== å¼€å§‹STGPR+GeoShapleyå¯è§£é‡Šæ€§åˆ†æ ======")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\nğŸ“‹ å½“å‰é…ç½®:")
    print(f"  â€¢ éšæœºç§å­: 42")
    print(f"  â€¢ åŸºç¡€è¯±å¯¼ç‚¹æ•°é‡: {CONFIG['model']['num_inducing_points']}")
    print(f"  â€¢ è®­ç»ƒè¿­ä»£æ¬¡æ•°: {CONFIG['model']['num_iterations']}")
    print(f"  â€¢ æ‰¹å¤„ç†å¤§å°: {CONFIG['model']['batch_size']}")
    print(f"  â€¢ GeoShapleyè¿›ç¨‹æ•°: {CONFIG['geoshapley']['n_jobs']}")
    print(f"  â€¢ èƒŒæ™¯æ•°æ®ç‚¹: è‡ªåŠ¨è®¡ç®—ï¼ˆâ‰¥âˆšç‰¹å¾æ•°ï¼Œå‘ä¸Šå–æ•´ï¼‰")
    if use_processed_data:
        print(f"  â€¢ æ•°æ®åŠ è½½ä¼˜åŒ–: âœ… å·²å¯ç”¨ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ•°æ®ï¼‰")
    else:
        print(f"  â€¢ æ•°æ®åŠ è½½ä¼˜åŒ–: âŒ å·²ç¦ç”¨ï¼ˆå¼ºåˆ¶é‡æ–°é¢„å¤„ç†ï¼‰")
    print()
    
    # æ£€æŸ¥æ¨¡å—å¯ç”¨æ€§
    print("\n=== ğŸ” æ¨¡å—å¯ç”¨æ€§æ£€æŸ¥ ===")
    modules_status = check_module_availability()
    print("=== æ¨¡å—æ£€æŸ¥å®Œæˆ ===\n")
    
    # è·å–è®­ç»ƒå‡½æ•°åŒ…è£…å™¨
    train_evaluate_stgpr_model = create_train_evaluate_wrapper()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = output_dir or DATA_CONFIG['default_output_dir']
    ensure_dir_exists(output_dir)
    
    # è®¾ç½®ç›®æ ‡å˜é‡
    target_column = DATA_CONFIG['target_column']
    
    # è®¾ç½®æ•°æ®ç›®å½•
    if data_dir is None:
        data_dir = DATA_CONFIG['default_data_dir']
    
    # ğŸš€ ä¼˜åŒ–çš„æ•°æ®åŠ è½½æµç¨‹
    print("\n=== 1. ğŸ“Š åŠ è½½å®Œæ•´æ•°æ®é›† (2000-2024å¹´) ===")
    
    # å¯¼å…¥æ•°æ®åŠ è½½æ¨¡å—
    from data_processing.preprocessing import load_complete_dataset, load_data_files, load_processed_data_files, get_data_summary
    
    # ç¡®å®šè¦å¤„ç†çš„åˆ†è¾¨ç‡
    resolutions_to_process = data_resolutions if data_resolutions else DATA_CONFIG['default_resolutions']
    
    all_dfs = {}
    
    # ğŸ¯ ä¸»è¦ç­–ç•¥ï¼šç›´æ¥åŠ è½½2000-2024å¹´å®Œæ•´æ•°æ®é›†
    print("ğŸ¯ ä½¿ç”¨å®Œæ•´æ•°æ®é›† (åŒ…å«2000-2020è§‚æµ‹æ•°æ® + 2021-2024 ARIMAå¤–æ¨æ•°æ®)")
    
    try:
        print("\nğŸ“Š åŠ è½½å®Œæ•´æ•°æ®é›†...")
        all_dfs = load_complete_dataset(
            data_dir=data_dir,
            resolutions=resolutions_to_process,
            verbose=True
        )
        
        if all_dfs:
            print("âœ… å®Œæ•´æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
            
            # æ˜¾ç¤ºæ•°æ®é›†ç‰¹å¾
            print("\nğŸ“‹ æ•°æ®é›†ç‰¹å¾ç¡®è®¤:")
            for res, df in all_dfs.items():
                year_range = (df['year'].min(), df['year'].max())
                print(f"  â€¢ {res}: {len(df):,}è¡Œ | æ—¶é—´: {year_range[0]}-{year_range[1]} | ç½‘æ ¼: {df['h3_index'].nunique():,}ä¸ª")
            
        else:
            print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            
    except Exception as e:
        print(f"âš ï¸ ä¸»è¦åŠ è½½æ–¹æ³•å¤±è´¥: {e}")
        all_dfs = {}
    
    # å¤‡ç”¨ç­–ç•¥ï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
    if not all_dfs:
        print("\nğŸ”„ ä½¿ç”¨å¤‡ç”¨æ•°æ®åŠ è½½æ–¹æ³•...")
        try:
            all_dfs = load_data_files(
                data_dir=data_dir,
                resolutions=resolutions_to_process,
                verbose=True,
                force_reprocess=False
            )
        except Exception as e:
            print(f"âŒ å¤‡ç”¨åŠ è½½æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            return
    
    # ğŸ”„ å¤‡é€‰ç­–ç•¥ï¼šå¦‚æœä¸»è¦æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨ä¼˜åŒ–åŠ è½½æ–¹å¼
    if not all_dfs and use_processed_data:
        print("\nğŸ”„ å¤‡é€‰æ–¹æ¡ˆ1ï¼šæ™ºèƒ½æ•°æ®åŠ è½½")
        
        try:
            # å°è¯•ç›´æ¥ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½
            backup_dfs = load_processed_data_files(
                resolutions=resolutions_to_process,
                verbose=True
            )
            
            if backup_dfs:
                all_dfs.update(backup_dfs)
                print("âœ… å¤‡é€‰æ–¹æ¡ˆ1æˆåŠŸ")
            
            # å¯¹äºä»ç¼ºå¤±çš„åˆ†è¾¨ç‡ï¼Œä½¿ç”¨æ™ºèƒ½åŠ è½½
            missing_resolutions = [res for res in resolutions_to_process if res not in all_dfs]
            if missing_resolutions:
                print(f"ğŸ”„ å¤„ç†ç¼ºå¤±åˆ†è¾¨ç‡: {missing_resolutions}")
                smart_load_dfs = load_data_files(
                    data_dir=data_dir, 
                    resolutions=missing_resolutions, 
                    force_reprocess=False,
                    verbose=True
                )
                all_dfs.update(smart_load_dfs)
                
        except Exception as e:
            print(f"âŒ å¤‡é€‰æ–¹æ¡ˆ1å¤±è´¥: {e}")
    
    # ğŸ”„ å¤‡é€‰ç­–ç•¥2ï¼šå¼ºåˆ¶é‡æ–°é¢„å¤„ç†
    if not all_dfs and not use_processed_data:
        print("\nğŸ”„ å¤‡é€‰æ–¹æ¡ˆ2ï¼šå¼ºåˆ¶é‡æ–°é¢„å¤„ç†")
        
        try:
            all_dfs = load_data_files(
                data_dir=data_dir, 
                resolutions=resolutions_to_process, 
                force_reprocess=True,
                verbose=True
            )
            print("âœ… å¤‡é€‰æ–¹æ¡ˆ2æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å¤‡é€‰æ–¹æ¡ˆ2å¤±è´¥: {e}")
    
    # âŒ æœ€ç»ˆå›é€€ï¼šä¼ ç»ŸåŠ è½½æ–¹å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    if not all_dfs:
        print("\nâŒ æ‰€æœ‰ä¼˜åŒ–æ–¹å¼å¤±è´¥ï¼Œä½¿ç”¨æœ€åçš„å…¼å®¹æ€§å›é€€...")
        
        file_patterns = DATA_CONFIG['file_patterns']
        for res in resolutions_to_process:
            if res in file_patterns:
                file_path = os.path.join(data_dir, file_patterns[res])
                if os.path.exists(file_path):
                    try:
                        # ç›´æ¥è¯»å–CSVæ–‡ä»¶ä½œä¸ºæœ€åçš„æ‰‹æ®µ
                        df = pd.read_csv(file_path)
                        all_dfs[res] = df
                        print(f"âœ“ å…¼å®¹æ€§åŠ è½½ {res}: {df.shape}")
                    except Exception as load_error:
                        print(f"âŒ å…¼å®¹æ€§åŠ è½½ {res} å¤±è´¥: {load_error}")
    
    # æ£€æŸ¥æœ€ç»ˆç»“æœ
    if not all_dfs:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("  1. data/ç›®å½•ä¸­æ˜¯å¦å­˜åœ¨ ALL_DATA_with_VHI_PCA_{res}.csv æ–‡ä»¶")
        print("  2. æ–‡ä»¶æƒé™æ˜¯å¦æ­£ç¡®")
        print("  3. æ–‡ä»¶æ ¼å¼æ˜¯å¦å®Œæ•´")
        return None
    
    # æ˜¾ç¤ºæœ€ç»ˆåŠ è½½ç»“æœ
    try:
        print(f"\nğŸ‰ æ•°æ®åŠ è½½æœ€ç»ˆç»“æœ:")
        print(f"  â€¢ æˆåŠŸåŠ è½½: {len(all_dfs)}/{len(resolutions_to_process)} ä¸ªåˆ†è¾¨ç‡")
        
        # æ˜¾ç¤ºåŠ è½½æ•ˆç‡ç»Ÿè®¡
        load_time = time.time() - start_time
        total_rows = sum(len(df) for df in all_dfs.values())
        print(f"  â€¢ åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
        print(f"  â€¢ æ€»æ•°æ®é‡: {total_rows:,}è¡Œ")
        
        # æ˜¾ç¤ºæ¯ä¸ªåˆ†è¾¨ç‡çš„æ—¶é—´èŒƒå›´
        print(f"\nğŸ“… æ•°æ®æ—¶é—´èŒƒå›´éªŒè¯:")
        for res, df in all_dfs.items():
            if 'year' in df.columns:
                year_range = (df['year'].min(), df['year'].max())
                year_count = df['year'].nunique()
                print(f"  {res}: {year_range[0]}-{year_range[1]} ({year_count}å¹´)")
            else:
                print(f"  {res}: æ— å¹´ä»½ä¿¡æ¯")
        
        # è·å–æ•°æ®æ‘˜è¦
        data_summary = get_data_summary(all_dfs, verbose=True)
        print("âœ… æ•°æ®åŠ è½½ä¸éªŒè¯å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ è·å–æ•°æ®æ‘˜è¦å¤±è´¥: {e}")

    # ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œè®­ç»ƒ
    dfs = all_dfs
    
    # ä¸ºSHAPåˆ†æå‡†å¤‡é‡‡æ ·æ•°æ®
    shap_dfs = {}
    
    # ğŸ¯ å¹³è¡¡ä¼˜åŒ–ï¼šé€‚åº¦å¢åŠ ç½‘æ ¼æ•°é‡ï¼Œæå‡ç©ºé—´ä»£è¡¨æ€§ï¼Œæ§åˆ¶è®¡ç®—æ—¶é—´åœ¨åˆç†èŒƒå›´
    # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šres5=100ç½‘æ ¼ï¼Œres6=50ç½‘æ ¼ï¼Œres7=200ç½‘æ ¼
    SHAP_SAMPLE_CONFIG = {
        'res5': {
            'spatial_coverage': 0.45,  # 45%ç©ºé—´è¦†ç›–ç‡ï¼ˆ99ä¸ªç½‘æ ¼ï¼‰
            'temporal_coverage': 1.0,  # 100%æ—¶é—´è¦†ç›–ç‡ï¼ˆ25å¹´ï¼Œ2000-2024ï¼‰
            'min_networks': 50,        # æœ€å°‘50ä¸ªç½‘æ ¼
            'description': 'å°æ•°æ®é›†ï¼Œ45%ç©ºé—´è¦†ç›–+25å¹´å…¨è¦†ç›–ï¼ˆ2000-2024ï¼‰ï¼Œ~15åˆ†é’Ÿ'
        },
        'res6': {
            'spatial_coverage': 0.151,  # 15.1%ç©ºé—´è¦†ç›–ç‡ï¼ˆ200ä¸ªç½‘æ ¼ï¼‰
            'temporal_coverage': 1.0,   # 100%æ—¶é—´è¦†ç›–ç‡ï¼ˆ25å¹´ï¼Œ2000-2024ï¼‰
            'min_networks': 100,        # æœ€å°‘100ä¸ªç½‘æ ¼
            'description': 'ä¸­ç­‰æ•°æ®é›†ï¼Œ15.1%ç©ºé—´è¦†ç›–+25å¹´å…¨è¦†ç›–ï¼ˆ2000-2024ï¼‰ï¼Œ~60åˆ†é’Ÿ'
        },
        'res7': {
            'spatial_coverage': 0.058,  # 5.8%ç©ºé—´è¦†ç›–ç‡ï¼ˆ500ä¸ªç½‘æ ¼ï¼‰
            'temporal_coverage': 1.0,   # 100%æ—¶é—´è¦†ç›–ç‡ï¼ˆ25å¹´ï¼Œ2000-2024ï¼‰
            'min_networks': 200,        # æœ€å°‘200ä¸ªç½‘æ ¼
            'description': 'å¤§æ•°æ®é›†ï¼Œ5.8%ç©ºé—´è¦†ç›–+25å¹´å…¨è¦†ç›–ï¼ˆ2000-2024ï¼‰ï¼Œ~90åˆ†é’Ÿ'
        }
    }
    
    for res, df in all_dfs.items():
        if res in SHAP_SAMPLE_CONFIG:
            config = SHAP_SAMPLE_CONFIG[res]
            
            # è®¡ç®—ç›®æ ‡æ ·æœ¬é‡ï¼ˆåŸºäºç©ºé—´è¦†ç›–ç‡å’Œæ—¶é—´è¦†ç›–ç‡ï¼‰
            total_h3 = df['h3_index'].nunique() if 'h3_index' in df.columns else len(df)
            total_years = df['year'].nunique() if 'year' in df.columns else 1
            
            target_h3 = int(total_h3 * config['spatial_coverage'])
            target_years = int(total_years * config['temporal_coverage'])
            
            # é¢„ä¼°ç›®æ ‡æ ·æœ¬é‡ï¼ˆä»…ç”¨äºä¼ é€’ç»™å‡½æ•°ï¼Œå®é™…æ ·æœ¬é‡ç”±å‡½æ•°è‡ªç„¶äº§ç”Ÿï¼‰
            avg_records_per_grid_per_year = len(df) / (total_h3 * total_years) if total_h3 > 0 and total_years > 0 else 1
            estimated_samples = int(target_h3 * target_years * avg_records_per_grid_per_year)
            
            print(f"\nğŸ”§ {res} SHAPé‡‡æ ·ç­–ç•¥:")
            print(f"    â€¢ åŸå§‹æ•°æ®: {len(df):,}è¡Œ ({total_h3}ç½‘æ ¼ Ã— {total_years}å¹´)")
            print(f"    â€¢ ç©ºé—´è¦†ç›–: {config['spatial_coverage']*100:.0f}% ({target_h3}ç½‘æ ¼)")
            print(f"    â€¢ æ—¶é—´è¦†ç›–: {config['temporal_coverage']*100:.0f}% ({target_years}å¹´)")
            print(f"    â€¢ é¢„ä¼°æ ·æœ¬: ~{estimated_samples:,}ä¸ª")
            print(f"    â€¢ ç­–ç•¥è¯´æ˜: {config['description']}")
            
            # ä½¿ç”¨æ”¹è¿›çš„æ—¶ç©ºåˆ†å±‚é‡‡æ ·ï¼ˆæ ·æœ¬é‡è‡ªç„¶äº§ç”Ÿï¼‰
            shap_dfs[res] = perform_spatiotemporal_sampling(
                df, estimated_samples,  # ä¼ å…¥é¢„ä¼°å€¼ï¼Œä½†å‡½æ•°å†…éƒ¨ä¼šè‡ªç„¶äº§ç”Ÿå®é™…æ ·æœ¬é‡
                h3_col='h3_index', year_col='year',
                spatial_coverage=config['spatial_coverage'],  # ğŸš€ ä¼ å…¥é…ç½®çš„ç©ºé—´è¦†ç›–ç‡
                random_state=42
            )
            
            # éªŒè¯é‡‡æ ·ç»“æœçš„æ—¶ç©ºè¦†ç›–
            actual_samples = len(shap_dfs[res])
            actual_networks = shap_dfs[res]['h3_index'].nunique() if 'h3_index' in shap_dfs[res].columns else 0
            actual_years = shap_dfs[res]['year'].nunique() if 'year' in shap_dfs[res].columns else 0
            
            # è®¡ç®—å®é™…è¦†ç›–ç‡
            actual_spatial_coverage = actual_networks / total_h3 * 100 if total_h3 > 0 else 0
            actual_temporal_coverage = actual_years / total_years * 100 if total_years > 0 else 0
            
            print(f"    âœ… é‡‡æ ·ç»“æœ: {actual_samples:,}ä¸ªæ ·æœ¬")
            print(f"    ğŸ“ ç©ºé—´è¦†ç›–: {actual_networks}/{total_h3} = {actual_spatial_coverage:.1f}%")
            print(f"    ğŸ“… æ—¶é—´è¦†ç›–: {actual_years}/{total_years} = {actual_temporal_coverage:.1f}%")
            print(f"    ğŸ“Š å®é™…é‡‡æ ·ç‡: {actual_samples/len(df)*100:.2f}%")
            
            # è¯„ä¼°ä»£è¡¨æ€§
            if actual_spatial_coverage >= 20 and actual_temporal_coverage >= 90:
                print(f"    ğŸ¯ æ—¶ç©ºä»£è¡¨æ€§: âœ… ä¼˜ç§€")
            elif actual_spatial_coverage >= 10 and actual_temporal_coverage >= 70:
                print(f"    ğŸ¯ æ—¶ç©ºä»£è¡¨æ€§: âš ï¸ è‰¯å¥½")
            else:
                print(f"    ğŸ¯ æ—¶ç©ºä»£è¡¨æ€§: âŒ éœ€è¦æ”¹è¿›")
        else:
            # é»˜è®¤é…ç½®
            default_samples = min(1000, len(df))
            shap_dfs[res] = perform_spatiotemporal_sampling(
                df, default_samples,
                h3_col='h3_index', year_col='year',
                spatial_coverage=0.05,  # é»˜è®¤5%ç©ºé—´è¦†ç›–ç‡
                random_state=42
            )
            print(f"{res}: ä½¿ç”¨é»˜è®¤é…ç½®ï¼ŒSHAPåˆ†æå°†ä½¿ç”¨{len(shap_dfs[res])}ä¸ªé‡‡æ ·æ ·æœ¬")
    
    # åˆ›å»ºç»“æœå­—å…¸
    results = {}
    
    # ğŸ”´ æ–°å¢ï¼šéªŒè¯æ‰€æœ‰ç‰¹å¾æ˜¯å¦èƒ½è¢«æ­£ç¡®åˆ†ç±»
    print("\n=== ğŸ” ç‰¹å¾åˆ†ç±»éªŒè¯ ===")
    from model_analysis.core import validate_all_features_categorized
    
    for res, df in dfs.items():
        print(f"\n{res}åˆ†è¾¨ç‡ç‰¹å¾éªŒè¯:")
        # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç›®æ ‡å˜é‡å’Œéæ¨¡å‹ç‰¹å¾ï¼‰
        non_feature_cols = [target_column, 'h3_index', 'original_h3_index', '.geo']
        feature_cols = [col for col in df.columns if col not in non_feature_cols]
        
        # éªŒè¯ç‰¹å¾
        is_valid, validation_result = validate_all_features_categorized(feature_cols)
        
        # ğŸ”¥ ä¼˜åŒ–çš„éªŒè¯ç»“æœå¤„ç†
        feature_set_type = validation_result.get('feature_set_type', 'æœªçŸ¥')
        optimization_status = validation_result.get('optimization_status', 'æœªçŸ¥')
        
        if not is_valid:
            # æ ¹æ®ç‰¹å¾é›†ç±»å‹ç»™å‡ºä¸åŒçš„å¤„ç†å»ºè®®
            if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
                print(f"âš ï¸  {res}ä¼˜åŒ–ç‰¹å¾é›†éœ€è¦è°ƒæ•´")
                print(f"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥å¹¶è°ƒæ•´ç‰¹å¾é¢„å¤„ç†ï¼Œç¡®ä¿åŒ…å«14ä¸ªæ ¸å¿ƒç‰¹å¾")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ„å¤–çš„ç§»é™¤ç‰¹å¾
                optimized_removed_present = validation_result.get('optimized_removed_present', [])
                if optimized_removed_present:
                    print(f"ğŸ”§ å‘ç°è¢«ä¼˜åŒ–ç§»é™¤çš„ç‰¹å¾ä»å­˜åœ¨ï¼Œå»ºè®®ç§»é™¤ä»¥ä¿æŒä¼˜åŒ–æ•ˆæœ")
                
            elif feature_set_type == "å®Œæ•´ç‰¹å¾é›†":
                print(f"âŒ {res}å®Œæ•´ç‰¹å¾é›†éªŒè¯æœªé€šè¿‡!")
                print(f"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç‰¹å¾åç§°å¹¶ä¿®æ­£åå†è¿è¡Œæ¨¡å‹")
            else:
                print(f"â“ {res}ç‰¹å¾é›†ç±»å‹æœªçŸ¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
                print(f"ğŸ’¡ å»ºè®®ï¼šç¡®è®¤ç‰¹å¾æ•°é‡æ˜¯å¦ç¬¦åˆé¢„æœŸï¼ˆ14ä¸ªä¼˜åŒ–ç‰¹å¾æˆ–19ä¸ªå®Œæ•´ç‰¹å¾ï¼‰")
            
            # æ ¹æ®å¤±è´¥ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­
            failed_features = validation_result.get('failed', [])
            if failed_features:
                print("ğŸš¨ å‘ç°æ— æ³•åˆ†ç±»çš„ç‰¹å¾ï¼Œå»ºè®®ä¿®æ­£åé‡æ–°è¿è¡Œ")
                if skip_validation:
                    print("âš ï¸ è·³è¿‡éªŒè¯æ¨¡å¼å·²å¯ç”¨ï¼Œè‡ªåŠ¨ç»§ç»­...")
                else:
                    response = input("æ˜¯å¦ä»è¦ç»§ç»­ï¼Ÿ(y/n): ")
                    if response.lower() != 'y':
                        print("ç¨‹åºç»ˆæ­¢")
                        return None
            else:
                # å¦‚æœåªæ˜¯ä¼˜åŒ–ç‰¹å¾é›†çš„å°é—®é¢˜ï¼Œå¯ä»¥ç»§ç»­
                if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
                    print("âœ… ä¼˜åŒ–ç‰¹å¾é›†åŸºæœ¬æ­£å¸¸ï¼Œç»§ç»­è¿è¡Œ...")
                else:
                    if skip_validation:
                        print("âš ï¸ è·³è¿‡éªŒè¯æ¨¡å¼å·²å¯ç”¨ï¼Œè‡ªåŠ¨ç»§ç»­...")
                    else:
                        response = input("æ˜¯å¦ä»è¦ç»§ç»­ï¼Ÿ(y/n): ")
                        if response.lower() != 'y':
                            print("ç¨‹åºç»ˆæ­¢")
                            return None
        else:
            # éªŒè¯æˆåŠŸçš„æƒ…å†µ
            if feature_set_type == "GeoShapleyä¼˜åŒ–ç‰¹å¾é›†":
                print(f"ğŸ‰ {res}GeoShapleyä¼˜åŒ–ç‰¹å¾é›†éªŒè¯å®Œç¾é€šè¿‡!")
                print(f"âš¡ å·²å¯ç”¨ä¸‰é‡ä¼˜åŒ–ï¼šç‰¹å¾å‡å°‘ + ä½ç½®åˆå¹¶ + ç®—æ³•ä¼˜åŒ–")
            elif feature_set_type == "å®Œæ•´ç‰¹å¾é›†":
                print(f"âœ… {res}å®Œæ•´ç‰¹å¾é›†éªŒè¯é€šè¿‡!")
                print(f"ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿ19ä¸ªç‰¹å¾è¿›è¡Œå»ºæ¨¡")
            else:
                print(f"âœ… {res}ç‰¹å¾é›†éªŒè¯é€šè¿‡")
    
    print("=== ç‰¹å¾éªŒè¯å®Œæˆ ===\n")
    
    # å¯¹æ¯ä¸ªåˆ†è¾¨ç‡è®­ç»ƒä¸€ä¸ªæ—¶ç©ºé«˜æ–¯è¿‡ç¨‹æ¨¡å‹
    print("ğŸ¤– æ¨¡å‹è®­ç»ƒ:")
    for resolution in resolutions_to_process:
        print(f"\nğŸ“ˆ è®­ç»ƒ{resolution}æ¨¡å‹...")
        
        # æ ¹æ®æ•°æ®å¤§å°å’Œåˆ†è¾¨ç‡è‡ªåŠ¨ç¡®å®šè¯±å¯¼ç‚¹æ•°é‡
        base_inducing_points = CONFIG['model']['num_inducing_points']
        
        # è·å–åˆ†è¾¨ç‡ç‰¹å®šçš„é…ç½®
        res_config = CONFIG.get('resolution_specific', {}).get(resolution, {})
        inducing_points_factor = res_config.get('num_inducing_points_factor', 1.0)
        
        # è®¡ç®—å®é™…è¯±å¯¼ç‚¹æ•°é‡
        num_inducing_points = int(base_inducing_points * inducing_points_factor)
        
        # ç¡®ä¿è¯±å¯¼ç‚¹æ•°é‡ä¸è¶…è¿‡æ•°æ®é‡
        X, y = prepare_features_for_stgpr(dfs[resolution], target=target_column)
        num_inducing_points = min(num_inducing_points, X.shape[0])
        
        # è®¡ç®—å®é™…æ¯”ä¾‹
        actual_ratio = num_inducing_points / X.shape[0]
        
        print(f"  ğŸ“Š è¯±å¯¼ç‚¹ç­–ç•¥:")
        print(f"    â€¢ æ•°æ®é‡: {X.shape[0]:,}è¡Œ")
        print(f"    â€¢ è¯±å¯¼ç‚¹æ•°é‡: {num_inducing_points} ({actual_ratio:.1%})")
        print(f"    â€¢ é€‰æ‹©æ–¹æ³•: KMeansèšç±»ï¼ˆç‰¹å¾ç©ºé—´ä»£è¡¨æ€§ç‚¹ï¼‰")
        
        # ğŸ”§ æ™ºèƒ½GPUé€‰æ‹©ç­–ç•¥ - éµå¾ªé…ç½®æ–‡ä»¶è®¾ç½®
        use_gpu_for_training = False
        
        # ä»é…ç½®æ–‡ä»¶è·å–GPUåå¥½è®¾ç½®
        prefer_gpu = res_config.get('prefer_gpu', False)
        
        if torch.cuda.is_available() and prefer_gpu:
            use_gpu_for_training = True
            gpu_name = torch.cuda.get_device_name(0)
            print(f"  ğŸ® è®¡ç®—è®¾å¤‡: GPU ({gpu_name}) - æŒ‰é…ç½®å¯ç”¨GPUåŠ é€Ÿ")
        else:
            use_gpu_for_training = False
            if not torch.cuda.is_available():
                print(f"  ğŸ’» è®¡ç®—è®¾å¤‡: CPU (GPUä¸å¯ç”¨)")
            elif not prefer_gpu:
                print(f"  ğŸ’» è®¡ç®—è®¾å¤‡: CPU (é…ç½®ä¸ºCPUä¼˜å…ˆï¼Œç¡®ä¿GeoShapleyå…¼å®¹æ€§)")
            else:
                print(f"  ğŸ’» è®¡ç®—è®¾å¤‡: CPU (æœªçŸ¥åŸå› )")
        
        # è·å–åˆ†è¾¨ç‡ç‰¹å®šçš„è¶…å‚æ•°ä¼˜åŒ–é…ç½®
        actual_max_hyperopt_evals = res_config.get('max_hyperopt_evals', max_hyperopt_evals)
        print(f"  âš¡ ä¼˜åŒ–ç­–ç•¥: è¶…å‚æ•°è¯„ä¼°æ¬¡æ•°{actual_max_hyperopt_evals}æ¬¡")
        
        # è®­ç»ƒæ¨¡å‹
        model_output_dir = os.path.join(output_dir, resolution)
        ensure_dir_exists(model_output_dir)
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¹¶è®­ç»ƒ
        if modules_status['HAS_STGPR']:
            result = train_evaluate_stgpr_model(
                dfs[resolution],
                resolution=resolution,
                output_dir=model_output_dir,
                target=target_column,
                use_gpu=use_gpu_for_training,  # ğŸš€ æ™ºèƒ½GPUé€‰æ‹©
                use_hyperopt=use_hyperopt,
                num_inducing_points=num_inducing_points,
                max_hyperopt_evals=actual_max_hyperopt_evals
            )
            
            # ä¿å­˜ç»“æœ
            if result is not None:
                results[resolution] = result
    
            # ğŸ”§ ä¿®å¤ï¼šä¿å­˜æ¨¡å‹ç»“æœåˆ°æ–‡ä»¶ï¼ˆä»…ä¿å­˜å¯åºåˆ—åŒ–çš„éƒ¨åˆ†ï¼‰
            import pickle
            model_file = os.path.join(model_output_dir, f"{resolution}_model_results.pkl")
            
            try:
                # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœå‰¯æœ¬ï¼ˆæ’é™¤æ¨¡å‹å¯¹è±¡ï¼‰
                if result is not None:
                    serializable_result = {}
                    for key, value in result.items():
                        # è·³è¿‡åŒ…å«æ¨¡å‹å¯¹è±¡çš„é”®
                        if key in ['model', 'likelihood', 'mll', 'optimizer']:
                            print(f"    è·³è¿‡ä¸å¯åºåˆ—åŒ–çš„é¡¹: {key}")
                            continue
                        
                        # å°è¯•åºåˆ—åŒ–æ¯ä¸ªå€¼
                        try:
                            import copy
                            # æ·±æ‹·è´å¹¶æµ‹è¯•æ˜¯å¦å¯ä»¥åºåˆ—åŒ–
                            temp_value = copy.deepcopy(value)
                            pickle.dumps(temp_value)  # æµ‹è¯•åºåˆ—åŒ–
                            serializable_result[key] = temp_value
                        except Exception as e:
                            print(f"    è·³è¿‡ä¸å¯åºåˆ—åŒ–çš„é¡¹ {key}: {str(e)[:100]}")
                            continue
                    
                    # ä¿å­˜å¯åºåˆ—åŒ–çš„ç»“æœ
                    with open(model_file, 'wb') as f:
                        pickle.dump(serializable_result, f)
                    print(f"  ğŸ’¾ æ¨¡å‹ç»“æœå·²ä¿å­˜è‡³: {model_file}")
                    print(f"    ä¿å­˜çš„é¡¹ç›®: {list(serializable_result.keys())}")
                else:
                    print(f"  âš ï¸ æ¨¡å‹ç»“æœä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                    
            except Exception as save_error:
                print(f"  âš ï¸ ä¿å­˜æ¨¡å‹ç»“æœæ—¶å‡ºé”™: {str(save_error)}")
                print(f"  ğŸ’¡ æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œä½†ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼ˆä¸å½±å“åç»­åˆ†æï¼‰")
            
            # ç®€åŒ–çš„æ€§èƒ½æŠ¥å‘Š
            if result and 'metrics' in result:
                metrics = result['metrics']
                if 'r2' in metrics and 'rmse' in metrics:
                    print(f"  âœ“ è®­ç»ƒå®Œæˆ: æµ‹è¯•RÂ²={metrics['r2']:.3f}, RMSE={metrics['rmse']:.3f}")
                else:
                    print(f"  âœ“ è®­ç»ƒå®Œæˆï¼Œå¯ç”¨æŒ‡æ ‡: {list(metrics.keys())}")
                
                print(f"    è®­ç»ƒæ ·æœ¬: {len(result.get('y_train', []))}, æµ‹è¯•æ ·æœ¬: {len(result.get('y_test', []))}")
            else:
                print("  âš ï¸ æ— æ³•è·å–æ€§èƒ½æŒ‡æ ‡")
        else:
            print(f"  âŒ STGPRæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡{resolution}åˆ†è¾¨ç‡çš„è®­ç»ƒ")
    
    # æ£€æŸ¥æ¨¡å‹è®­ç»ƒç»“æœ
    successful_models = [res for res in results.keys() if results[res] is not None]
    if not successful_models:
        print("\né”™è¯¯: æ‰€æœ‰åˆ†è¾¨ç‡çš„æ¨¡å‹è®­ç»ƒå‡å¤±è´¥ã€‚")
        return
    
    # ä½¿ç”¨GeoShapleyè®¡ç®—ç‰¹å¾é‡è¦æ€§
    print("\n=== 2. ä½¿ç”¨GeoShapleyè®¡ç®—SHAPç‰¹å¾é‡è¦æ€§ ===")
    print("ğŸ¯ GeoShapleyæ ¸å¿ƒç‰¹æ€§:")
    print("  â€¢ å°†ç»çº¬åº¦(latitude, longitude)ä½œä¸ºè”åˆç‰¹å¾(GEO)å¤„ç†")
    print("  â€¢ åœ¨SHAPåˆ†å¸ƒå›¾ä¸­æ˜¾ç¤ºä¸ºå•ä¸€çš„'GEO'ç‰¹å¾")
    print("  â€¢ æ•æ‰ç©ºé—´æ•ˆåº”çš„æ•´ä½“è´¡çŒ®ï¼Œé¿å…ç»çº¬åº¦å½±å“åˆ†æ•£")
    print("  â€¢ èƒŒæ™¯æ•°æ®ç‚¹è‡ªåŠ¨è®¡ç®—ï¼šâˆšç‰¹å¾æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰")
    
    # å¯¹æ‰€æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ
    for res in successful_models:
        try:
            if results[res] is not None:
                print(f"\nğŸ” è®¡ç®—{res}åˆ†è¾¨ç‡çš„GeoShapleyå€¼...")
                
                # è·å–æ¨¡å‹å’Œè®­ç»ƒæ•°æ®
                X_train = results[res].get('X')
                if X_train is None:
                    print(f"  è­¦å‘Š: {res}ç¼ºå°‘ç‰¹å¾çŸ©é˜µXï¼Œæ— æ³•è®¡ç®—SHAPå€¼")
                    continue
                
                # ä½¿ç”¨ä¸“é—¨çš„SHAPé‡‡æ ·æ•°æ®
                print(f"  ğŸ”§ ä½¿ç”¨æ™ºèƒ½æ•°æ®ç­–ç•¥:")
                print(f"    â€¢ è®­ç»ƒæ•°æ®: {X_train.shape[0]}è¡Œ (ç”¨äºç”ŸæˆèƒŒæ™¯æ•°æ®)")
                
                # è·å–SHAPä¸“ç”¨çš„é‡‡æ ·æ•°æ®
                if res in shap_dfs:
                    shap_df = shap_dfs[res]
                    print(f"    â€¢ SHAPæ•°æ®: {len(shap_df)}è¡Œ (ä¸“é—¨é‡‡æ ·ï¼Œä¿æŒæ—¶ç©ºä»£è¡¨æ€§)")
                    
                    # ä»SHAPæ•°æ®ä¸­æå–ç‰¹å¾
                    X_shap, _ = prepare_features_for_stgpr(shap_df, target=target_column)
                    
                    print(f"    â€¢ ç‰¹å¾ä¸€è‡´æ€§æ£€æŸ¥: è®­ç»ƒ{X_train.shape[1]}åˆ— vs SHAP{X_shap.shape[1]}åˆ—")
                    
                    # ç¡®ä¿ç‰¹å¾åˆ—ä¸€è‡´
                    if list(X_train.columns) == list(X_shap.columns):
                        print(f"    âœ… ç‰¹å¾åˆ—å®Œå…¨ä¸€è‡´")
                        X_samples = X_shap
                    else:
                        print(f"    âš ï¸ ç‰¹å¾åˆ—ä¸ä¸€è‡´ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„åˆ—é¡ºåº")
                        X_samples = X_shap[X_train.columns]
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿X_samplesåŒ…å«h3_index
                    if 'h3_index' not in X_samples.columns and 'h3_index' in shap_df.columns:
                        print(f"    ğŸ”§ æ·»åŠ h3_indexåˆ°X_samples")
                        X_samples = X_samples.copy()
                        X_samples['h3_index'] = shap_df['h3_index'].values[:len(X_samples)]
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®å¯¹åº”å…³ç³»æ­£ç¡®
                    print(f"    ğŸ“ æ•°æ®å¯¹åº”æ£€æŸ¥:")
                    print(f"      â€¢ X_sampleså½¢çŠ¶: {X_samples.shape}")
                    print(f"      â€¢ åŸå§‹SHAPæ•°æ®å½¢çŠ¶: {shap_df.shape}")
                    print(f"      â€¢ h3_indexåˆ—å­˜åœ¨: {'h3_index' in X_samples.columns}")
                else:
                    print(f"    âš ï¸ æœªæ‰¾åˆ°{res}çš„SHAPä¸“ç”¨æ•°æ®ï¼Œå›é€€åˆ°è®­ç»ƒæ•°æ®é‡‡æ ·")
                    sample_size = min(200, len(X_train))
                    X_samples = X_train.sample(sample_size, random_state=RANDOM_SEED)
                
                print(f"    â€¢ æœ€ç»ˆSHAPæ ·æœ¬: {len(X_samples)}è¡Œ")
                
                # éªŒè¯ç»çº¬åº¦ç‰¹å¾
                X_samples = validate_geo_features(X_samples, res)
                
                # æ·»åŠ h3_indexåˆ—ï¼ˆå¦‚æœéœ€è¦ï¼‰- ä¿®å¤ï¼šæ¥æ”¶è¿”å›å€¼
                X_samples = add_h3_index_if_needed(X_samples, res, shap_dfs)
                
                # ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒæ•°æ®ä½œä¸ºèƒŒæ™¯æ•°æ®
                X_train_full = X_train
                
                # ğŸ”§ ç»Ÿä¸€CPUé€»è¾‘ï¼šæ‰€æœ‰åˆ†è¾¨ç‡éƒ½ä½¿ç”¨ç›¸åŒçš„æˆåŠŸæ–¹æ¡ˆ
                print(f"    â€¢ {res}ä½¿ç”¨ç»Ÿä¸€çš„CPUè®­ç»ƒ+CPU GeoShapleyæ–¹æ¡ˆ")
                
                # æ‰€æœ‰åˆ†è¾¨ç‡ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆéƒ½æ˜¯CPUè®­ç»ƒï¼‰
                geoshapley_model_dict = results[res]
                
                # è®¡ç®—SHAPè§£é‡Š
                compute_shap_explanations(results, res, X_samples, X_train_full, geoshapley_model_dict)
                
        except Exception as e:
            print(f"  {res}çš„SHAPåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results[res]['feature_importance_failed'] = True
            results[res]['feature_importance'] = None
    
    # æ˜¾ç¤ºæ¯ä¸ªåˆ†è¾¨ç‡çš„ç‰¹å¾é‡è¦æ€§
    print("\n=== 3. ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ ===")
    display_feature_importance(results, successful_models)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    print("\n=== 4. å¯è§†åŒ–åˆ†æ ===")
    create_visualizations(results, output_dir, plots_to_create)
    
    # å®Œæˆåˆ†æ
    elapsed_time = time.time() - start_time
    print(f"\n====== STGPR+GeoShapleyå¯è§£é‡Šæ€§åˆ†æå®Œæˆ ======")
    print(f"æ€»è€—æ—¶: {elapsed_time/60:.2f} åˆ†é’Ÿ")
    print(f"åˆ†æç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    # é‡æ–°ç”Ÿæˆä¿®å¤åçš„å¯è§†åŒ–å›¾è¡¨
    print("\nğŸ”§ é‡æ–°ç”Ÿæˆä¿®å¤åçš„å¯è§†åŒ–å›¾è¡¨...")
    try:
        from visualization.feature_plots import plot_feature_importance_comparison
        from visualization.shap_distribution_plots import plot_combined_shap_summary_distribution
        from visualization.geoshapley_spatial_top3 import plot_geoshapley_spatial_top3
        from visualization.regionkmeans_plot import plot_regionkmeans_feature_target_analysis
        
        # 1. é‡æ–°ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ¯”è¾ƒå›¾
        print("  ğŸ“Š é‡æ–°ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ¯”è¾ƒå›¾...")
        feature_importances_dict = {}
        for res in ['res5', 'res6', 'res7']:
            if res in results:
                feature_importances_dict[res] = results[res]['feature_importance']
        
        plot_feature_importance_comparison(
            feature_importances_dict, 
            output_dir=output_dir,
            results=results
        )
        
        # 2. é‡æ–°ç”ŸæˆSHAPç©ºé—´åˆ†å¸ƒå›¾
        print("  ğŸŒ é‡æ–°ç”ŸæˆGeoShapleyç©ºé—´åˆ†å¸ƒå›¾...")
        plot_geoshapley_spatial_top3(
            results,
            output_dir=output_dir
        )
        
        # 3. é‡æ–°ç”Ÿæˆèšç±»åˆ†æå›¾è¡¨
        print("  ğŸ“ˆ é‡æ–°ç”Ÿæˆèšç±»åˆ†æå›¾è¡¨...")
        from visualization.regionkmeans_plot import plot_regionkmeans_shap_clusters_by_resolution
        
        # å…ˆç”Ÿæˆèšç±»åˆ†æï¼Œè·å–cluster_results
        fig, cluster_results = plot_regionkmeans_shap_clusters_by_resolution(
            results, 
            output_dir=output_dir
        )
        
        # ç„¶åä½¿ç”¨cluster_resultsç”Ÿæˆç‰¹å¾è´¡çŒ®åˆ†æå›¾
        if cluster_results:
            plot_regionkmeans_feature_target_analysis(
                cluster_results,
                output_dir=output_dir
            )
        else:
            print("  âš ï¸ èšç±»ç»“æœä¸ºç©ºï¼Œè·³è¿‡ç‰¹å¾è´¡çŒ®åˆ†æå›¾")
        
        print("  âœ… æ‰€æœ‰ä¿®å¤åçš„å¯è§†åŒ–å›¾è¡¨å·²é‡æ–°ç”Ÿæˆ")

    except Exception as e:
        print(f"  âš ï¸ é‡æ–°ç”Ÿæˆå›¾è¡¨æ—¶å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def validate_geo_features(X_samples, res):
    """éªŒè¯åœ°ç†ç‰¹å¾"""
    geo_features = [col for col in X_samples.columns if col.lower() in ['latitude', 'longitude']]
    if len(geo_features) == 2:
        print(f"  ğŸŒ GeoShapleyåœ°ç†ç‰¹å¾éªŒè¯:")
        print(f"    â€¢ å‘ç°åœ°ç†ç‰¹å¾: {geo_features}")
        
        # æ£€æŸ¥ç»çº¬åº¦æ˜¯å¦åœ¨DataFrameçš„æœ€åä¸¤åˆ—ï¼ˆGeoShapleyè¦æ±‚ï¼‰
        last_two_cols = X_samples.columns[-2:].tolist()
        if set(geo_features) == set(last_two_cols):
            print(f"    âœ… ç»çº¬åº¦å·²åœ¨æœ€åä¸¤åˆ—ï¼Œç¬¦åˆGeoShapleyè¦æ±‚")
        else:
            print(f"    ğŸ”„ é‡æ–°æ’åˆ—ç‰¹å¾é¡ºåºï¼Œå°†ç»çº¬åº¦ç§»è‡³æœ€å...")
            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šéåœ°ç†ç‰¹å¾ + åœ°ç†ç‰¹å¾
            non_geo_cols = [col for col in X_samples.columns if col not in geo_features]
            new_column_order = non_geo_cols + geo_features
            X_samples = X_samples[new_column_order]
            print(f"    âœ… ç‰¹å¾é‡æ’å®Œæˆ: {len(non_geo_cols)}ä¸ªéåœ°ç†ç‰¹å¾ + 2ä¸ªåœ°ç†ç‰¹å¾")
    else:
        print(f"  âš ï¸ åœ°ç†ç‰¹å¾ä¸å®Œæ•´: {geo_features}")
        print(f"    GeoShapleyéœ€è¦ç»çº¬åº¦ä¸¤ä¸ªç‰¹å¾æ‰èƒ½æ­£ç¡®å¤„ç†è”åˆåœ°ç†ç‰¹å¾")
    
    return X_samples

def add_h3_index_if_needed(X_samples, res, shap_dfs):
    """æ·»åŠ h3_indexåˆ—ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
    # å¦‚æœX_sampleså·²ç»æœ‰h3_indexåˆ—ï¼Œç›´æ¥è¿”å›
    if 'h3_index' in X_samples.columns:
        print(f"  ğŸ“ X_sampleså·²åŒ…å«h3_indexåˆ—")
        return X_samples
    
    # å°è¯•ä»shap_dfsæ·»åŠ h3_index
    if res in shap_dfs and 'h3_index' in shap_dfs[res].columns:
        print(f"  ğŸ“ ä¸ºSHAPè®¡ç®—æ·»åŠ ç©ºé—´æ ‡è¯†ä¿¡æ¯...")
        try:
            shap_df_with_features = shap_dfs[res]
            
            if len(X_samples) == len(shap_df_with_features):
                X_samples = X_samples.copy()
                X_samples['h3_index'] = shap_df_with_features['h3_index'].values
                unique_h3_count = len(set(shap_df_with_features['h3_index']))
                print(f"    âœ… æˆåŠŸæ·»åŠ h3_indexåˆ—ï¼Œå…±{unique_h3_count}ä¸ªå”¯ä¸€H3å€¼")
            else:
                print(f"    âš ï¸ æ•°æ®é•¿åº¦ä¸åŒ¹é…: X_samples({len(X_samples)}) vs shap_df({len(shap_df_with_features)})")
                
                # ğŸ”¥ ä¿®å¤ï¼šå°è¯•åŸºäºç»çº¬åº¦è¿›è¡Œæœ€è¿‘é‚»åŒ¹é…
                if ('latitude' in X_samples.columns and 'longitude' in X_samples.columns and
                    'latitude' in shap_df_with_features.columns and 'longitude' in shap_df_with_features.columns):
                    
                    print(f"    ğŸ”„ å°è¯•åŸºäºç»çº¬åº¦è¿›è¡Œç©ºé—´åŒ¹é…...")
                    from sklearn.neighbors import NearestNeighbors
                    
                    # æ„å»ºKNNæ¨¡å‹
                    shap_coords = shap_df_with_features[['latitude', 'longitude']].values
                    X_coords = X_samples[['latitude', 'longitude']].values
                    
                    knn = NearestNeighbors(n_neighbors=1, metric='haversine')
                    knn.fit(np.radians(shap_coords))  # ä½¿ç”¨å¼§åº¦å•ä½
                    
                    # æ‰¾åˆ°æœ€è¿‘çš„ç‚¹
                    distances, indices = knn.kneighbors(np.radians(X_coords))
                    
                    # æ·»åŠ h3_index
                    X_samples = X_samples.copy()
                    X_samples['h3_index'] = shap_df_with_features.iloc[indices.flatten()]['h3_index'].values
                    
                    avg_distance = distances.mean() * 6371000  # è½¬æ¢ä¸ºç±³
                    print(f"    âœ… åŸºäºç©ºé—´åŒ¹é…æˆåŠŸæ·»åŠ h3_indexï¼Œå¹³å‡è·ç¦»: {avg_distance:.1f}ç±³")
                else:
                    print(f"    âŒ æ— æ³•è¿›è¡Œç©ºé—´åŒ¹é…ï¼Œç¼ºå°‘ç»çº¬åº¦åˆ—")
                    
        except Exception as e:
            print(f"    æ·»åŠ h3_indexæ—¶å‡ºé”™: {str(e)}")
    else:
        print(f"  âš ï¸ æ— æ³•æ·»åŠ h3_index: res={res}, shap_dfså­˜åœ¨={res in shap_dfs if shap_dfs else False}")
    
    return X_samples

def compute_shap_explanations(results, res, X_samples, X_train_full, geoshapley_model_dict):
    """è®¡ç®—SHAPè§£é‡Š"""
    try:
        # è·å–åŸå§‹çš„feature_namesï¼Œç¡®ä¿ä¸åŒ…å«h3_index
        feature_names = [col for col in X_train_full.columns if col not in ['h3_index']]
        
        print(f"  ğŸš€ å¼€å§‹GeoShapleyè®¡ç®—...")
        print(f"    â€¢ æ ·æœ¬æ•°é‡: {len(X_samples)}")
        print(f"    â€¢ ç‰¹å¾æ•°é‡: {len(feature_names)}")
        print(f"    â€¢ èƒŒæ™¯æ•°æ®: {len(X_train_full)}è¡Œ")
        print(f"    â€¢ èƒŒæ™¯æ•°æ®ç‚¹: è‡ªåŠ¨è®¡ç®—ï¼ˆåŸºäºç‰¹å¾æ•°é‡ï¼‰")
        print(f"    â€¢ åœ°ç†ç‰¹å¾å¤„ç†: ç»çº¬åº¦å°†è‡ªåŠ¨åˆå¹¶ä¸ºGEOç‰¹å¾")
        
        explanations = explain_stgpr_predictions(
            model_dict=geoshapley_model_dict,
            X_samples=X_samples,
            X_train=X_train_full,
            feature_names=feature_names,
            n_background=None,  # è®©å‡½æ•°è‡ªåŠ¨è®¡ç®—èƒŒæ™¯ç‚¹æ•°é‡
            res_level=res
        )
        
        # å¤„ç†è§£é‡Šç»“æœ
        if explanations:
            results[res]['explanations'] = explanations
            print(f"    âœ… GeoShapleyè®¡ç®—æˆåŠŸ")
            
            # æå–SHAPè§£é‡Šç»“æœ
            if 'local_explanations' in explanations and explanations['local_explanations']:
                local_exp = explanations['local_explanations']
                
                # æå–SHAPå€¼
                if 'shap_values' in local_exp:
                    results[res]['shap_values'] = local_exp['shap_values']
                    results[res]['feature_names'] = local_exp.get('feature_names', feature_names)
                    
                    # ğŸ”´ å…³é”®ä¿®å¤ï¼šç¡®ä¿X_sampleä¸SHAPå€¼ç»´åº¦ä¸€è‡´
                    # è·å–å®é™…è®¡ç®—SHAPå€¼çš„æ ·æœ¬æ•°é‡
                    n_shap_samples = local_exp['shap_values'].shape[0]
                    
                    # å¦‚æœåŸå§‹X_samplesæ¯”SHAPæ ·æœ¬å¤šï¼Œè¯´æ˜è¿›è¡Œäº†é‡‡æ ·
                    if len(X_samples) > n_shap_samples:
                        print(f"  ğŸ“Š æ£€æµ‹åˆ°é‡‡æ ·: åŸå§‹{len(X_samples)}è¡Œ â†’ SHAPè®¡ç®—{n_shap_samples}è¡Œ")
                        # éœ€è¦é‡æ–°é‡‡æ ·X_samplesä»¥åŒ¹é…SHAPå€¼
                        from model_analysis.stgpr_sampling import perform_spatiotemporal_sampling
                        
                        # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿é‡‡æ ·æ—¶ä¿ç•™h3_indexåˆ—
                        sampling_kwargs = {
                            'random_state': 42  # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰h3_indexåˆ—ç”¨äºé‡‡æ ·
                        if 'h3_index' in X_samples.columns:
                            sampling_kwargs['h3_col'] = 'h3_index'
                            print(f"    ğŸ—ºï¸ ä½¿ç”¨h3_indexè¿›è¡Œç©ºé—´åˆ†å±‚é‡‡æ ·")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰yearåˆ—ç”¨äºé‡‡æ ·
                        if 'year' in X_samples.columns:
                            sampling_kwargs['year_col'] = 'year'
                            print(f"    ğŸ“… ä½¿ç”¨yearè¿›è¡Œæ—¶é—´åˆ†å±‚é‡‡æ ·")
                        
                        try:
                            X_samples_matched = perform_spatiotemporal_sampling(
                                X_samples, n_shap_samples, 
                                spatial_coverage=0.05,  # ä½¿ç”¨5%é»˜è®¤è¦†ç›–ç‡è¿›è¡Œé‡é‡‡æ ·
                                **sampling_kwargs
                            )
                            
                            # éªŒè¯é‡‡æ ·ç»“æœ
                            if len(X_samples_matched) == n_shap_samples:
                                results[res]['X_sample'] = X_samples_matched
                                print(f"  âœ… X_sampleå·²è°ƒæ•´ä¸º{len(X_samples_matched)}è¡Œï¼Œä¸SHAPå€¼åŒ¹é…")
                                
                                # éªŒè¯h3_indexæ˜¯å¦ä¿ç•™
                                if 'h3_index' in X_samples.columns:
                                    if 'h3_index' in X_samples_matched.columns:
                                        unique_h3_before = len(set(X_samples['h3_index']))
                                        unique_h3_after = len(set(X_samples_matched['h3_index']))
                                        print(f"    ğŸ—ºï¸ h3_indexä¿ç•™å®Œæ•´: {unique_h3_before} â†’ {unique_h3_after}ä¸ªå”¯ä¸€å€¼")
                                    else:
                                        print(f"    âš ï¸ è­¦å‘Š: é‡‡æ ·åä¸¢å¤±äº†h3_indexåˆ—")
                            else:
                                print(f"  âŒ é‡‡æ ·ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ›{n_shap_samples}, å®é™…{len(X_samples_matched)}")
                                results[res]['X_sample'] = X_samples  # ä½¿ç”¨åŸå§‹æ•°æ®
                                
                        except Exception as sampling_error:
                            print(f"  âŒ é‡‡æ ·å¤±è´¥: {sampling_error}")
                            print(f"  ğŸ”„ å›é€€ï¼šä½¿ç”¨åŸå§‹X_samplesçš„å‰{n_shap_samples}è¡Œ")
                            X_samples_matched = X_samples.iloc[:n_shap_samples].copy()
                            results[res]['X_sample'] = X_samples_matched
                    else:
                        results[res]['X_sample'] = X_samples
                    
                    # åˆ›å»ºæŒ‰ç‰¹å¾åç§°ç´¢å¼•çš„SHAPå€¼å­—å…¸
                    shap_values_by_feature = {}
                    for i, feat in enumerate(local_exp['feature_names']):
                        shap_values_by_feature[feat] = local_exp['shap_values'][:, i]
                    results[res]['shap_values_by_feature'] = shap_values_by_feature
                
                # æå–GeoShapleyçš„ä¸‰éƒ¨åˆ†ç»“æœ
                if 'geoshap_original' in local_exp:
                    geoshap_data = local_exp['geoshap_original']
                    if all(k in geoshap_data for k in ['primary', 'geo', 'geo_intera']):
                        results[res]['geoshapley_values'] = {
                            'primary_effects': geoshap_data['primary'],
                            'geo_effect': geoshap_data['geo'],
                            'interaction_effects': geoshap_data['geo_intera']
                        }
                        print(f"  âœ… å·²ä¿å­˜GeoShapleyä¸‰éƒ¨åˆ†ç»“æœ")
                        
            # å¤„ç†SHAPäº¤äº’å€¼
            process_shap_interactions(results, res, explanations)
            
            # è®¡ç®—åŸºäºSHAPçš„ç‰¹å¾é‡è¦æ€§
            compute_shap_feature_importance(results, res, explanations, feature_names)
        else:
            print(f"    âŒ GeoShapleyè®¡ç®—å¤±è´¥ï¼Œæœªè¿”å›æœ‰æ•ˆç»“æœ")
            results[res]['feature_importance_failed'] = True
            results[res]['feature_importance'] = None
            
    except Exception as e:
        print(f"    âŒ GeoShapleyè®¡ç®—å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        results[res]['feature_importance_failed'] = True
        results[res]['feature_importance'] = None

def process_shap_interactions(results, res, explanations):
    """å¤„ç†SHAPäº¤äº’å€¼"""
    if 'local_explanations' in explanations and explanations['local_explanations'] is not None:
        local_expl = explanations['local_explanations']
        if 'shap_interaction_values' in local_expl:
            results[res]['shap_interaction_values'] = local_expl['shap_interaction_values']
            print(f"    âœ… SHAPäº¤äº’å€¼å·²ä¿å­˜ï¼Œå½¢çŠ¶: {local_expl['shap_interaction_values'].shape}")
            
            if 'interaction_sample_indices' in local_expl:
                results[res]['interaction_sample_indices'] = local_expl['interaction_sample_indices']

def compute_shap_feature_importance(results, res, explanations, feature_names):
    """è®¡ç®—åŸºäºSHAPçš„ç‰¹å¾é‡è¦æ€§"""
    if explanations is None:
        print(f"    âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§£é‡Šç»“æœ")
        results[res]['feature_importance_failed'] = True
        results[res]['feature_importance'] = None
        return
    
    if 'local_explanations' in explanations and explanations['local_explanations'] is not None:
        local_expl = explanations['local_explanations']
        
        if 'shap_values' in local_expl:
            # ç›´æ¥ä½¿ç”¨å·²ç»å¤„ç†å¥½çš„numpyæ•°ç»„ï¼ˆç”±stgpr_geoshapley.pyè¿”å›ï¼‰
            if isinstance(local_expl['shap_values'], np.ndarray):
                print(f"    ğŸ“Š SHAPå€¼ç»Ÿè®¡: å½¢çŠ¶{local_expl['shap_values'].shape}")
            else:
                print(f"    âŒ SHAPå€¼ä¸æ˜¯numpyæ•°ç»„: {type(local_expl['shap_values'])}")
                results[res]['feature_importance_failed'] = True
                results[res]['feature_importance'] = None
                return
            
            # ä½¿ç”¨GeoShapleyè¿”å›çš„ç‰¹å¾åï¼ˆå·²ç»åŒ…å«GEOï¼‰
            shap_feature_names = local_expl.get('feature_names', feature_names)
            
            # éªŒè¯SHAPå€¼è´¨é‡
            if local_expl['shap_values'].size > 0 and local_expl['shap_values'].ndim == 2:
                process_valid_shap_values(results, res, local_expl['shap_values'], shap_feature_names)
            else:
                print(f"    âŒ SHAPå€¼æ•°ç»„æ— æ•ˆæˆ–ä¸ºç©º")
                print(f"    å¤§å°: {local_expl['shap_values'].size}, ç»´åº¦: {local_expl['shap_values'].ndim}")
                results[res]['feature_importance_failed'] = True
                results[res]['feature_importance'] = None
        else:
            print(f"    âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„SHAPå€¼æ•°ç»„")
            results[res]['feature_importance_failed'] = True
            results[res]['feature_importance'] = None
    else:
        print(f"    âŒ æœªèƒ½ç”ŸæˆSHAPè§£é‡Š")
        results[res]['feature_importance_failed'] = True
        results[res]['feature_importance'] = None

def process_valid_shap_values(results, res, shap_values, shap_feature_names):
    """å¤„ç†æœ‰æ•ˆçš„SHAPå€¼"""
    shap_range = (shap_values.min(), shap_values.max())
    shap_std = shap_values.std()
    non_zero_ratio = np.count_nonzero(shap_values) / shap_values.size
    
    print(f"    â€¢ SHAPå€¼èŒƒå›´: [{shap_range[0]:.6f}, {shap_range[1]:.6f}]")
    print(f"    â€¢ SHAPå€¼æ ‡å‡†å·®: {shap_std:.6f}")
    print(f"    â€¢ éé›¶å€¼æ¯”ä¾‹: {non_zero_ratio:.1%}")
    
    if shap_std > 1e-6 and non_zero_ratio > 0.1:
        print(f"    âœ… SHAPå€¼è´¨é‡è‰¯å¥½ï¼ŒåŒ…å«æœ‰æ„ä¹‰çš„å˜å¼‚")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
        mean_abs_shap = np.abs(shap_values).mean(axis=0)
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if len(mean_abs_shap) != len(shap_feature_names):
            min_len = min(len(mean_abs_shap), len(shap_feature_names))
            mean_abs_shap = mean_abs_shap[:min_len]
            shap_feature_names = shap_feature_names[:min_len]
        
        # ç›´æ¥ä½¿ç”¨åŸå§‹SHAPå€¼ï¼ˆä¸SHAPåˆ†å¸ƒå›¾ä¿æŒä¸€è‡´ï¼‰
        importance_values = mean_abs_shap
        print(f"    ğŸ“Š ä½¿ç”¨åŸå§‹SHAPå€¼ï¼ˆä¸SHAPåˆ†å¸ƒå›¾ä¿æŒä¸€è‡´ï¼‰")
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§æ’å
        shap_feature_importance = [(shap_feature_names[i], float(importance)) 
                                  for i, importance in enumerate(importance_values)]
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        shap_feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        results[res]['feature_importance'] = shap_feature_importance
        
        # æ˜¾ç¤ºåŸºäºSHAPå€¼çš„ç‰¹å¾é‡è¦æ€§
        print(f"\n    ğŸ“Š åŸºäºSHAPå€¼çš„ç‰¹å¾é‡è¦æ€§:")
        print(f"    {'ç‰¹å¾åç§°':<20} {'é‡è¦æ€§å€¼':<10} {'ç›¸å¯¹ç™¾åˆ†æ¯”':<10}")
        print(f"    {'-'*40}")
        
        # è®¡ç®—æ€»é‡è¦æ€§ï¼ˆç”¨äºç™¾åˆ†æ¯”è®¡ç®—ï¼‰
        total_importance = sum(imp for _, imp in shap_feature_importance)
        
        # æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
        for i, (feat, imp) in enumerate(shap_feature_importance[:10]):
            percentage = (imp / total_importance * 100) if total_importance > 0 else 0
            print(f"    {feat:<20} {imp:<10.4f} {percentage:<10.1f}%")
        
        if len(shap_feature_importance) > 10:
            print(f"    ... è¿˜æœ‰ {len(shap_feature_importance) - 10} ä¸ªç‰¹å¾")
        
        # ä¸ºå¯è§†åŒ–æ·»åŠ å¿…è¦çš„æ•°æ®
        results[res]['shap_values'] = shap_values
        results[res]['feature_names'] = shap_feature_names  # ä½¿ç”¨GeoShapleyè¿”å›çš„ç‰¹å¾å
        
        # åˆ›å»ºSHAPå€¼å­—å…¸ - ä½¿ç”¨GeoShapleyè¿”å›çš„ç‰¹å¾åï¼ˆåŒ…å«GEOï¼‰
        shap_values_by_feature = {}
        for i, feature in enumerate(shap_feature_names):
            if i < shap_values.shape[1]:
                shap_values_by_feature[feature] = shap_values[:, i]
        results[res]['shap_values_by_feature'] = shap_values_by_feature
        
        print(f"\n    ğŸ† å‰5ä¸ªé‡è¦ç‰¹å¾ï¼ˆåŸºäºSHAPå€¼ï¼‰:")
        for i, (feat, imp) in enumerate(shap_feature_importance[:5]):
            print(f"      {i+1}. {feat}: {imp:.6f}")
    else:
        print(f"    âš ï¸ SHAPå€¼å˜å¼‚å¤ªå°æˆ–é›¶å€¼å¤ªå¤š")
        results[res]['feature_importance_failed'] = True
        results[res]['feature_importance'] = None

def display_feature_importance(results, successful_models):
    """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ"""
    print("ğŸ“Š åŸºäºGeoShapleyçš„ç‰¹å¾é‡è¦æ€§åˆ†æ:")
    print("  â€¢ ç»çº¬åº¦å·²ä½œä¸ºè”åˆGEOç‰¹å¾å¤„ç†")
    print("  â€¢ ç‰¹å¾é‡è¦æ€§åŸºäºSHAPå€¼è®¡ç®—")
    print("  â€¢ æ’åºæŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½")
    
    # å¯¼å…¥ç‰¹å¾åˆ†ç±»å‡½æ•°
    from model_analysis.core import categorize_feature
    
    feature_importances_dict = {}
    
    for res in successful_models:
        print(f"\nğŸ¯ {res} ç‰¹å¾é‡è¦æ€§æ’å:")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾é‡è¦æ€§å¤±è´¥çš„æ ‡è®°
        if results[res].get('feature_importance_failed', False):
            print(f"  âŒ {res} ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥")
            print(f"  âš ï¸ åŸå› ï¼šSHAPå€¼è®¡ç®—æœªæˆåŠŸå®Œæˆ")
            print(f"  ğŸ’¡ å»ºè®®ï¼š")
            print(f"    1. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®")
            print(f"    2. ç¡®ä¿æ¨¡å‹è®­ç»ƒæˆåŠŸ")
            print(f"    3. éªŒè¯ç‰¹å¾æ•°é‡æ˜¯å¦åŒ¹é…")
            print(f"    4. è€ƒè™‘ç¦ç”¨ç‰¹å¾é¢„ç­›é€‰")
            continue
            
        if 'feature_importance' in results[res] and results[res]['feature_importance']:
            # è·å–ç‰¹å¾é‡è¦æ€§
            feature_importance = results[res]['feature_importance']
            
            # å»é‡å¤„ç†
            unique_features = {}
            for feat, imp in feature_importance:
                std_feat = feat.lower() if isinstance(feat, str) else str(feat).lower()
                if std_feat not in unique_features or imp > unique_features[std_feat][1]:
                    unique_features[std_feat] = (feat, imp)
            
            # è½¬å›åˆ—è¡¨å¹¶æ’åº
            feature_importance = [(feat, imp) for _, (feat, imp) in unique_features.items()]
            feature_importance.sort(key=lambda x: x[1], reverse=True)
            
            # æ›´æ–°ç»“æœå­—å…¸
            feature_importances_dict[res] = feature_importance
            
            # æ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾
            print(f"{'æ’å':<4} {'ç‰¹å¾åç§°':<25} {'é‡è¦æ€§':<10} {'ç±»åˆ«':<15}")
            print("-" * 60)
            
            # è®¡ç®—ç‰¹å¾ç±»åˆ«ä¿¡æ¯
            feature_categories = {}
            for feature, importance in feature_importance:
                category = categorize_feature(feature)
                feature_categories[feature] = category
            
            # ä¿å­˜ç‰¹å¾ç±»åˆ«ä¿¡æ¯åˆ°ç»“æœä¸­
            results[res]['feature_categories'] = feature_categories
            
            for i, (feature, importance) in enumerate(feature_importance, 1):
                category = feature_categories.get(feature, 'æœªåˆ†ç±»')
                print(f"{i:<4} {feature:<25} {importance:<10.4f} {category:<15}")
            
            # ç»Ÿè®¡å„ç±»åˆ«ç‰¹å¾æ•°é‡
            category_counts = {}
            for feature, importance in feature_importance:
                category = feature_categories.get(feature, 'æœªåˆ†ç±»')
                if category not in category_counts:
                    category_counts[category] = []
                category_counts[category].append((feature, importance))
            
            print(f"\nğŸ“ˆ {res} ç‰¹å¾ç±»åˆ«ç»Ÿè®¡:")
            for category, features in category_counts.items():
                avg_importance = sum(imp for _, imp in features) / len(features)
                print(f"  {category}: {len(features)}ä¸ªç‰¹å¾, å¹³å‡é‡è¦æ€§: {avg_importance:.4f}")
        else:
            print(f"  âŒ {res} æœªæ‰¾åˆ°ç‰¹å¾é‡è¦æ€§æ•°æ®")
    
    # è·¨åˆ†è¾¨ç‡ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
    if len(feature_importances_dict) > 1:
        cross_resolution_comparison(feature_importances_dict)

def cross_resolution_comparison(feature_importances_dict):
    """è·¨åˆ†è¾¨ç‡ç‰¹å¾é‡è¦æ€§å¯¹æ¯”"""
    print(f"\nğŸ” è·¨åˆ†è¾¨ç‡ç‰¹å¾é‡è¦æ€§å¯¹æ¯”:")
    
    # æ”¶é›†æ‰€æœ‰ç‰¹å¾
    all_features = set()
    for res_features in feature_importances_dict.values():
        for feat, _ in res_features:
            all_features.add(feat)
    
    print(f"  æ€»å…±å‘ç° {len(all_features)} ä¸ªä¸åŒç‰¹å¾")
    
    # æ‰¾å‡ºåœ¨æ‰€æœ‰åˆ†è¾¨ç‡ä¸­éƒ½é‡è¦çš„ç‰¹å¾ï¼ˆå‰5åï¼‰
    consistent_important_features = set()
    for res, features in feature_importances_dict.items():
        top_5_features = set(feat for feat, _ in features[:5])
        if not consistent_important_features:
            consistent_important_features = top_5_features
        else:
            consistent_important_features &= top_5_features
    
    if consistent_important_features:
        print(f"  ğŸ† åœ¨æ‰€æœ‰åˆ†è¾¨ç‡ä¸­éƒ½æ’åå‰5çš„ç‰¹å¾:")
        for feat in consistent_important_features:
            print(f"    â€¢ {feat}")
    else:
        print(f"  â„¹ï¸ æ²¡æœ‰åœ¨æ‰€æœ‰åˆ†è¾¨ç‡ä¸­éƒ½æ’åå‰5çš„ç‰¹å¾")
        
    # æ˜¾ç¤ºå„åˆ†è¾¨ç‡çš„æœ€é‡è¦ç‰¹å¾
    print(f"  ğŸ“Š å„åˆ†è¾¨ç‡æœ€é‡è¦ç‰¹å¾:")
    for res, features in feature_importances_dict.items():
        if features:
            top_feature, top_importance = features[0]
            print(f"    {res}: {top_feature} ({top_importance:.4f})")

def create_visualizations(results, output_dir, plots_to_create):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    vis_success = False
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿GeoShapleyç»“æœè¢«ä¿å­˜åˆ°ç»“æœæ–‡ä»¶ä¸­
    print("\nğŸ”§ æ£€æŸ¥å¹¶ä¿å­˜GeoShapleyç»“æœ...")
    for res in results:
        if res not in ['res5', 'res6', 'res7']:
            continue
            
        res_result = results[res]
        
        # æ£€æŸ¥å†…å­˜ä¸­æ˜¯å¦æœ‰GeoShapleyæ•°æ®
        has_geoshapley = any(key in res_result for key in [
            'geoshapley_values', 'shap_values_by_feature', 'feature_importance'
        ])
        
        if has_geoshapley:
            print(f"  âœ… {res}: æ£€æµ‹åˆ°å®Œæ•´çš„GeoShapleyæ•°æ®")
            
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
            output_res_dir = os.path.join(output_dir, res)
            os.makedirs(output_res_dir, exist_ok=True)
            
            # ğŸ”¥ ä¿å­˜å…³é”®çš„GeoShapleyæ•°æ®å’Œç©ºé—´ä¿¡æ¯
            geoshapley_data = {}
            
            # ä¿å­˜GeoShapleyæ ¸å¿ƒæ•°æ®
            for key in ['geoshapley_values', 'shap_values_by_feature', 'feature_importance', 
                       'shap_values', 'feature_names', 'X_sample']:
                if key in res_result:
                    geoshapley_data[key] = res_result[key]
            
            # ğŸ”¥ ç¡®ä¿ä¿å­˜å®Œæ•´çš„ç©ºé—´æ•°æ®
            for spatial_key in ['df', 'raw_data', 'data']:
                if spatial_key in res_result and res_result[spatial_key] is not None:
                    geoshapley_data[spatial_key] = res_result[spatial_key]
                    print(f"    ğŸ“ ä¿å­˜ç©ºé—´æ•°æ®å­—æ®µ: {spatial_key}")
                    break  # åªä¿å­˜ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å®Œæ•´ç©ºé—´æ•°æ®
            
            # ä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
            geoshapley_file = os.path.join(output_res_dir, f'{res}_geoshapley_data.pkl')
            try:
                import pickle
                with open(geoshapley_file, 'wb') as f:
                    pickle.dump(geoshapley_data, f)
                print(f"    ğŸ’¾ GeoShapleyæ•°æ®å·²ä¿å­˜: {geoshapley_file}")
                
                # éªŒè¯ä¿å­˜çš„æ•°æ®
                saved_keys = list(geoshapley_data.keys())
                print(f"    ğŸ“‹ ä¿å­˜çš„æ•°æ®å­—æ®µ: {saved_keys}")
                
                # ç‰¹åˆ«æ£€æŸ¥ç©ºé—´ç›¸å…³å­—æ®µ
                spatial_info = []
                if 'X_sample' in geoshapley_data:
                    X_sample = geoshapley_data['X_sample']
                    if hasattr(X_sample, 'columns'):
                        spatial_cols = [col for col in X_sample.columns if col in ['latitude', 'longitude', 'h3_index']]
                        if spatial_cols:
                            spatial_info.append(f"X_sampleåŒ…å«: {spatial_cols}")
                
                for spatial_key in ['df', 'raw_data', 'data']:
                    if spatial_key in geoshapley_data:
                        spatial_df = geoshapley_data[spatial_key]
                        if hasattr(spatial_df, 'columns'):
                            spatial_cols = [col for col in spatial_df.columns if col in ['latitude', 'longitude', 'h3_index']]
                            if spatial_cols:
                                spatial_info.append(f"{spatial_key}åŒ…å«: {spatial_cols}")
                                break
                
                if spatial_info:
                    print(f"    ğŸ—ºï¸ ç©ºé—´ä¿¡æ¯: {'; '.join(spatial_info)}")
                else:
                    print(f"    âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°ç©ºé—´ä¿¡æ¯å­—æ®µ")
                    
            except Exception as e:
                print(f"    âŒ ä¿å­˜GeoShapleyæ•°æ®å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸ {res}: æœªæ£€æµ‹åˆ°GeoShapleyæ•°æ®")
    
    # åœ¨åˆ›å»ºå¯è§†åŒ–ä¹‹å‰å…ˆè¿›è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    try:
        from check_visualization_data import run_comprehensive_check
        print("\nğŸ” æ‰§è¡Œå¯è§†åŒ–æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")
        data_check_passed = run_comprehensive_check(results)
        if not data_check_passed:
            print("\nâš ï¸ è­¦å‘Š: æ•°æ®æ£€æŸ¥å‘ç°é—®é¢˜ï¼ŒæŸäº›å›¾è¡¨å¯èƒ½æ— æ³•ç”Ÿæˆ")
    except ImportError:
        print("â„¹ï¸ æœªæ‰¾åˆ°æ•°æ®æ£€æŸ¥æ¨¡å—ï¼Œè·³è¿‡æ£€æŸ¥")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®æ£€æŸ¥æ—¶å‡ºé”™: {e}")
    
    try:
        # å°è¯•å¯¼å…¥æ–°æ¥å£æ¨¡å—
        from model_analysis import stgpr_visualization
        
        # å‡†å¤‡æ¨¡å‹ç»“æœç”¨äºå¯è§†åŒ–
        print("\nä½¿ç”¨stgpr_visualizationæ¨¡å—å‡†å¤‡æ•°æ®å¹¶åˆ›å»ºå¯è§†åŒ–...")
        
        # ğŸ”¥ ç›´æ¥ä¼ é€’å®Œæ•´çš„resultsï¼Œç¡®ä¿GeoShapleyæ•°æ®ä¸ä¸¢å¤±
        model_results = stgpr_visualization.prepare_stgpr_results_for_visualization(results, output_dir)
        
        # ğŸ”¥ éªŒè¯æ•°æ®ä¼ é€’æ˜¯å¦æˆåŠŸ
        print("\nğŸ” éªŒè¯GeoShapleyæ•°æ®ä¼ é€’...")
        for res in model_results:
            if res not in ['res5', 'res6', 'res7']:
                continue
                
            res_data = model_results[res]
            has_shap = any(key in res_data for key in [
                'shap_values_by_feature', 'geoshapley_values', 'feature_importance'
            ])
            
            if has_shap:
                print(f"  âœ… {res}: å¯è§†åŒ–æ•°æ®åŒ…å«SHAPä¿¡æ¯")
                
                # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                if 'shap_values_by_feature' in res_data:
                    n_features = len(res_data['shap_values_by_feature'])
                    print(f"    ğŸ“Š shap_values_by_feature: {n_features}ä¸ªç‰¹å¾")
                
                if 'feature_importance' in res_data:
                    n_importance = len(res_data['feature_importance'])
                    print(f"    ğŸ† feature_importance: {n_importance}ä¸ªç‰¹å¾")
                    
                if 'geoshapley_values' in res_data:
                    print(f"    ğŸ—ºï¸ geoshapley_values: ä¸‰éƒ¨åˆ†ç»“æ„å¯ç”¨")
            else:
                print(f"  âŒ {res}: å¯è§†åŒ–æ•°æ®ç¼ºå°‘SHAPä¿¡æ¯ï¼")
        
        # åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        stgpr_visualization.create_all_visualizations(model_results, output_dir)
        
        print("âœ“ ä½¿ç”¨stgpr_visualizationæ¨¡å—åˆ›å»ºçš„å¯è§†åŒ–å›¾è¡¨å·²å®Œæˆ")
        vis_success = True
    except ImportError:
        print("âš  è­¦å‘Š: æœªèƒ½å¯¼å…¥stgpr_visualizationæ¨¡å—")
        print("å°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    except Exception as e:
        print(f"âš  è­¦å‘Š: ä½¿ç”¨stgpr_visualizationæ¨¡å—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        print("å°†å°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    # åªæœ‰åœ¨stgpr_visualizationå¤±è´¥æ—¶æ‰ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    if not vis_success:
        try:
            create_additional_visualizations(
                results, 
                extended_results_by_resolution=None,
                output_dir=output_dir, 
                plots_to_create=plots_to_create
            )
            print("âœ“ ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åˆ›å»ºçš„å¯è§†åŒ–å›¾è¡¨å·²å®Œæˆ")
        except Exception as e:
            print(f"âš  è­¦å‘Š: ä¼ ç»Ÿå¯è§†åŒ–æ–¹å¼å¤±è´¥: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser(description='STGPR+GeoShapley æ—¶ç©ºé«˜æ–¯è¿‡ç¨‹å›å½’å¯è§£é‡Šæ€§åˆ†ææ¡†æ¶')
        parser.add_argument('--output_dir', type=str, default='output', help='è¾“å‡ºç›®å½•')
        parser.add_argument('--resolutions', type=str, nargs='+', choices=['res5', 'res6', 'res7'], 
                            help='è¦å¤„ç†çš„åˆ†è¾¨ç‡ï¼Œä¾‹å¦‚ "res5 res6 res7"')
        parser.add_argument('--skip_validation', action='store_true', help='è·³è¿‡ç‰¹å¾éªŒè¯è¯¢é—®ï¼Œè‡ªåŠ¨ç»§ç»­')
        parser.add_argument('--force_reprocess', action='store_true', help='å¼ºåˆ¶é‡æ–°é¢„å¤„ç†æ•°æ®ï¼ˆé»˜è®¤ä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶ï¼‰')
        
        # æ·»åŠ ä¸€ä¸ªé€šç”¨å‚æ•°æ¥æ•è·Jupyterå‘é€çš„ç‰¹æ®Šå‚æ•°
        parser.add_argument('--f', type=str, help='Jupyter notebook kernel file (è‡ªåŠ¨å¿½ç•¥)', default=None)
        
        # ä½¿ç”¨parse_known_args()è€Œä¸æ˜¯parse_args()ï¼Œå¿½ç•¥æœªçŸ¥å‚æ•°
        args, unknown = parser.parse_known_args()
        
        # åªåœ¨æœ‰ä¸æ˜¯ä»¥--få¼€å¤´çš„æœªçŸ¥å‚æ•°æ—¶æ‰è¾“å‡ºæç¤º
        unknown_non_jupyter = [arg for arg in unknown if not arg.startswith('--f=')]
        if unknown_non_jupyter:
            print(f"æ³¨æ„ï¼šå¿½ç•¥æœªçŸ¥å‚æ•°: {unknown_non_jupyter}")
        
        # å¯åŠ¨ä¸»å‡½æ•°
        data_resolutions = args.resolutions if args.resolutions else None
        
        main(
            output_dir=args.output_dir,
            data_resolutions=data_resolutions,
            use_parallel=False,
            use_hyperopt=True,
            max_hyperopt_evals=10,
            skip_validation=args.skip_validation,
            use_processed_data=not args.force_reprocess  # åè½¬é€»è¾‘ï¼šé»˜è®¤ä½¿ç”¨é¢„å¤„ç†æ•°æ®
        )
    except SystemExit as e:
        # æ•è·argparseäº§ç”Ÿçš„SystemExitå¼‚å¸¸
        if e.code != 0:
            print(f"å‚æ•°è§£æé”™è¯¯ (ä»£ç : {e.code})ï¼Œä½†ä¼šç»§ç»­æ‰§è¡Œä¸»ç¨‹åº")
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ‰§è¡Œmainå‡½æ•°
            main(
                output_dir='output',
                data_resolutions=None,
                use_parallel=False,
                use_hyperopt=True,
                max_hyperopt_evals=10,
                skip_validation=False,
                use_processed_data=True
            )
    except Exception as e:
        print(f"æ‰§è¡Œæ—¶å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()