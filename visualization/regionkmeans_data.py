#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—ï¼šæä¾›å¯¹SHAPæ•°æ®çš„é¢„å¤„ç†ï¼Œç”¨äºç©ºé—´çº¦æŸèšç±»
"""
import os
import numpy as np
import pandas as pd
# ä½¿ç”¨æ˜ç¡®çš„å¯¼å…¥æ–¹å¼é¿å…Jupyterç¯å¢ƒæ£€æµ‹è­¦å‘Š
import tqdm.std
tqdm = tqdm.std.tqdm
from sklearn.neighbors import KNeighborsRegressor

# å°è¯•å¯¼å…¥h3åº“ï¼Œæ”¯æŒå¤šç§ç‰ˆæœ¬
try:
    import h3
    H3_AVAILABLE = True
    print("dataæ¨¡å—: æˆåŠŸå¯¼å…¥h3åº“")
except ImportError:
    try:
        # å°è¯•ä½¿ç”¨h3ronpyä½œä¸ºæ›¿ä»£
        from h3ronpy import h3
        H3_AVAILABLE = True
        print("dataæ¨¡å—: ä½¿ç”¨h3ronpyä½œä¸ºh3åº“æ›¿ä»£")
    except ImportError:
        H3_AVAILABLE = False
        print("dataæ¨¡å—: æœªèƒ½å¯¼å…¥h3åº“ï¼Œéƒ¨åˆ†H3åŠŸèƒ½å°†ä¸å¯ç”¨")


def get_full_grid_with_shap(results, res, top_features):
    """
    è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®å¹¶æ˜ å°„SHAPå€¼
    
    å‚æ•°:
    - results: ç»“æœå­—å…¸
    - res: åˆ†è¾¨ç‡
    - top_features: é¡¶éƒ¨ç‰¹å¾åˆ—è¡¨
    
    è¿”å›:
    - full_grid_data: åŒ…å«å®Œæ•´ç½‘æ ¼å’ŒSHAPå€¼çš„æ•°æ®
    """
    # é¦–å…ˆå°è¯•è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®
    full_data = None
    
    # æ–¹æ³•1ï¼šä»dfå­—æ®µè·å–
    if 'df' in results and results['df'] is not None:
        full_data = results['df']
        print(f"  {res}: ä»dfè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•2ï¼šä»raw_dataè·å–
    elif 'raw_data' in results and results['raw_data'] is not None:
        full_data = results['raw_data']
        print(f"  {res}: ä»raw_dataè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•3ï¼šå°è¯•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶
    else:
        try:
            data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
            data_file = os.path.join(data_dir, f"ALL_DATA_with_VHI_PCA_{res}.csv")
            if os.path.exists(data_file):
                full_data = pd.read_csv(data_file)
                print(f"  {res}: ä»æ–‡ä»¶åŠ è½½å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
        except Exception as e:
            print(f"  {res}: æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {e}")
    
    if full_data is None:
        return None
    
    # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
    required_cols = ['h3_index', 'latitude', 'longitude']
    if not all(col in full_data.columns for col in required_cols):
        print(f"  {res}: æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—")
        return None
    
    # è·å–å”¯ä¸€çš„H3ç½‘æ ¼
    full_h3_grid = full_data[['h3_index', 'latitude', 'longitude']].drop_duplicates(subset=['h3_index']).copy()
    
    # å¦‚æœæœ‰VHIï¼Œä¹Ÿä¿ç•™
    if 'VHI' in full_data.columns:
        # æŒ‰h3_indexèšåˆVHI
        vhi_by_h3 = full_data.groupby('h3_index')['VHI'].mean().reset_index()
        full_h3_grid = full_h3_grid.merge(vhi_by_h3, on='h3_index', how='left')
    
    print(f"  {res}: å®Œæ•´ç½‘æ ¼åŒ…å« {len(full_h3_grid)} ä¸ªH3ç½‘æ ¼")
    
    # è·å–é‡‡æ ·çš„SHAPå€¼
    shap_values = results.get('shap_values')
    X_sample = results.get('X_sample') if 'X_sample' in results else results.get('X')
    
    if shap_values is None or X_sample is None:
        print(f"  {res}: ç¼ºå°‘SHAPå€¼æˆ–é‡‡æ ·æ•°æ®")
        return None
    
    # ç¡®ä¿X_sampleæ˜¯DataFrame
    if not isinstance(X_sample, pd.DataFrame):
        # å¦‚æœæœ‰feature_namesï¼Œä½¿ç”¨å®ƒä»¬
        if 'feature_names' in results:
            X_sample = pd.DataFrame(X_sample, columns=results['feature_names'])
        else:
            X_sample = pd.DataFrame(X_sample)
    
    # å‡†å¤‡SHAPç‰¹å¾æ•°æ®
    if isinstance(shap_values, np.ndarray):
        # è·å–ç‰¹å¾å
        if 'feature_names' in results:
            shap_feature_names = results['feature_names']
        else:
            shap_feature_names = X_sample.columns.tolist()
        
        # åˆ›å»ºSHAP DataFrame
        shap_df = pd.DataFrame(shap_values, columns=shap_feature_names[:shap_values.shape[1]])
    else:
        shap_df = shap_values
    
    # åªä¿ç•™top_features
    top_shap_df = shap_df[top_features]
    
    # å¦‚æœX_sampleæœ‰h3_indexï¼Œä½¿ç”¨å®ƒæ¥æ˜ å°„
    if 'h3_index' in X_sample.columns:
        # åˆ›å»ºé‡‡æ ·æ•°æ®çš„h3_indexæ˜ å°„
        sample_data = pd.concat([
            X_sample[['h3_index']].reset_index(drop=True),
            top_shap_df.reset_index(drop=True)
        ], axis=1)
        
        # æŒ‰h3_indexèšåˆ
        sample_data_agg = sample_data.groupby('h3_index').mean().reset_index()
        
        # åˆå¹¶åˆ°å®Œæ•´ç½‘æ ¼
        full_grid_with_shap = full_h3_grid.merge(
            sample_data_agg,
            on='h3_index',
            how='left'
        )
        
        # å¯¹ç¼ºå¤±çš„SHAPå€¼è¿›è¡Œæ’å€¼
        missing_mask = full_grid_with_shap[top_features[0]].isna()
        if missing_mask.any():
            print(f"  {res}: {missing_mask.sum()}ä¸ªç½‘æ ¼ç¼ºå°‘SHAPå€¼ï¼Œä½¿ç”¨KNNæ’å€¼")
            
            # å‡†å¤‡å·²çŸ¥å’ŒæœªçŸ¥çš„åæ ‡
            known_mask = ~missing_mask
            known_coords = full_grid_with_shap.loc[known_mask, ['latitude', 'longitude']].values
            unknown_coords = full_grid_with_shap.loc[missing_mask, ['latitude', 'longitude']].values
            
            if len(known_coords) > 0 and len(unknown_coords) > 0:
                # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡ŒKNNæ’å€¼
                n_neighbors = min(10, len(known_coords))
                knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')
                
                for feat in top_features:
                    known_values = full_grid_with_shap.loc[known_mask, feat].values
                    knn.fit(known_coords, known_values)
                    predicted_values = knn.predict(unknown_coords)
                    full_grid_with_shap.loc[missing_mask, feat] = predicted_values
            else:
                # å¡«å……0
                for feat in top_features:
                    full_grid_with_shap.loc[missing_mask, feat] = 0
    
    else:
        # å¦‚æœæ²¡æœ‰h3_indexï¼Œä½¿ç”¨ç©ºé—´KNNåŒ¹é…
        print(f"  {res}: ä½¿ç”¨ç©ºé—´KNNå°†SHAPå€¼æ˜ å°„åˆ°å®Œæ•´ç½‘æ ¼")
        
        if 'latitude' in X_sample.columns and 'longitude' in X_sample.columns:
            sample_coords = X_sample[['latitude', 'longitude']].values
            grid_coords = full_h3_grid[['latitude', 'longitude']].values
            
            # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡ŒKNNé¢„æµ‹
            n_neighbors = min(5, len(sample_coords))
            knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')
            
            for feat in top_features:
                if feat in top_shap_df.columns:
                    feat_values = top_shap_df[feat].values
                    knn.fit(sample_coords, feat_values)
                    predicted_values = knn.predict(grid_coords)
                    full_h3_grid[feat] = predicted_values
            
            full_grid_with_shap = full_h3_grid
        else:
            print(f"  {res}: æ— æ³•è¿›è¡Œç©ºé—´åŒ¹é…ï¼Œç¼ºå°‘åæ ‡ä¿¡æ¯")
            return None
    
    return {
        'shap_features': full_grid_with_shap[top_features],
        'coords_df': full_grid_with_shap[['h3_index', 'latitude', 'longitude']],
        'top_features': top_features,
        'target_values': full_grid_with_shap['VHI'].values if 'VHI' in full_grid_with_shap else None,
        'full_grid': True  # æ ‡è®°è¿™æ˜¯å®Œæ•´ç½‘æ ¼æ•°æ®
    }


def get_full_h3_grid_data_for_clustering(res_data, resolution):
    """
    è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®ï¼Œç¡®ä¿ç©ºé—´è¦†ç›–è¿ç»­æ€§
    (ä¸¥æ ¼å­¦ä¹ geoshapley_spatial_top3.pyçš„å®ç°)
    
    å‚æ•°:
    - res_data: åˆ†è¾¨ç‡æ•°æ®
    - resolution: åˆ†è¾¨ç‡æ ‡è¯†
    
    è¿”å›:
    - full_h3_data: å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®DataFrame
    """
    # å°è¯•ä»å¤šä¸ªæ¥æºè·å–å®Œæ•´æ•°æ®
    full_data = None
    
    # æ–¹æ³•1ï¼šä»dfå­—æ®µè·å–ï¼ˆé€šå¸¸åŒ…å«å®Œæ•´æ•°æ®ï¼‰
    if 'df' in res_data and res_data['df'] is not None:
        full_data = res_data['df']
        print(f"  {resolution}: ä»dfè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•2ï¼šä»raw_dataè·å–
    elif 'raw_data' in res_data and res_data['raw_data'] is not None:
        full_data = res_data['raw_data']
        print(f"  {resolution}: ä»raw_dataè·å–å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
    
    # æ–¹æ³•3ï¼šå°è¯•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶
    else:
        try:
            import os
            data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
            data_file = os.path.join(data_dir, f"ALL_DATA_with_VHI_PCA_{resolution}.csv")
            if os.path.exists(data_file):
                full_data = pd.read_csv(data_file)
                print(f"  {resolution}: ä»æ–‡ä»¶åŠ è½½å®Œæ•´æ•°æ® ({len(full_data)}è¡Œ)")
        except Exception as e:
            print(f"  {resolution}: æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {e}")
    
    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ— æ³•è·å–å®Œæ•´æ•°æ®ï¼ŒåŸºäºé‡‡æ ·æ•°æ®ç”Ÿæˆå¯†é›†ç½‘æ ¼
    if full_data is None:
        print(f"  {resolution}: æ— æ³•è·å–å®Œæ•´æ•°æ®ï¼ŒåŸºäºé‡‡æ ·æ•°æ®ç”Ÿæˆå¯†é›†ç½‘æ ¼...")
        
        # ä»é‡‡æ ·æ•°æ®è·å–è¾¹ç•Œ
        X_sample = res_data.get('X_sample') if 'X_sample' in res_data else res_data.get('X')
        if X_sample is None or 'latitude' not in X_sample.columns or 'longitude' not in X_sample.columns:
            print(f"  {resolution}: æ— æ³•è·å–é‡‡æ ·æ•°æ®çš„ç»çº¬åº¦ä¿¡æ¯")
            return None
        
        # è®¡ç®—ç ”ç©¶åŒºåŸŸè¾¹ç•Œ
        lat_min, lat_max = X_sample['latitude'].min(), X_sample['latitude'].max()
        lon_min, lon_max = X_sample['longitude'].min(), X_sample['longitude'].max()
        
        # æ·»åŠ è¾¹ç•Œç¼“å†²åŒº
        lat_buffer = (lat_max - lat_min) * 0.1  # 10%ç¼“å†²åŒº
        lon_buffer = (lon_max - lon_min) * 0.1
        
        lat_min -= lat_buffer
        lat_max += lat_buffer
        lon_min -= lon_buffer
        lon_max += lon_buffer
        
        print(f"    ç ”ç©¶åŒºåŸŸè¾¹ç•Œ: çº¬åº¦ [{lat_min:.4f}, {lat_max:.4f}], ç»åº¦ [{lon_min:.4f}, {lon_max:.4f}]")
        
        # ğŸ”§ ç”Ÿæˆå¯†é›†çš„è§„åˆ™ç½‘æ ¼ä»¥ç¡®ä¿ç©ºé—´è¿ç»­æ€§
        # æ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´ç½‘æ ¼å¯†åº¦
        if resolution == 'res7':  # å¾®è§‚å°ºåº¦
            lat_step = 0.01  # çº¦1km
            lon_step = 0.01
        elif resolution == 'res6':  # ä¸­è§‚å°ºåº¦
            lat_step = 0.02  # çº¦2km
            lon_step = 0.02
        else:  # res5 - å®è§‚å°ºåº¦
            lat_step = 0.05  # çº¦5km
            lon_step = 0.05
        
        # ç”Ÿæˆç½‘æ ¼ç‚¹
        lats = np.arange(lat_min, lat_max + lat_step, lat_step)
        lons = np.arange(lon_min, lon_max + lon_step, lon_step)
        
        # åˆ›å»ºç½‘æ ¼
        lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')
        lat_flat = lat_grid.flatten()
        lon_flat = lon_grid.flatten()
        
        # åˆ›å»ºH3ç´¢å¼•ï¼ˆä¼ªç´¢å¼•ï¼Œç”¨äºæ ‡è¯†ï¼‰
        h3_indices = [f"{resolution}_grid_{i}" for i in range(len(lat_flat))]
        
        # åˆ›å»ºå®Œæ•´ç½‘æ ¼DataFrame
        full_h3_grid = pd.DataFrame({
            'h3_index': h3_indices,
            'latitude': lat_flat,
            'longitude': lon_flat
        })
        
        print(f"  {resolution}: ç”Ÿæˆå¯†é›†ç½‘æ ¼ ({len(full_h3_grid)}ä¸ªç½‘æ ¼ç‚¹, æ­¥é•¿: {lat_step}Â°)")
        return full_h3_grid
    
    # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
    required_cols = ['h3_index', 'latitude', 'longitude']
    if not all(col in full_data.columns for col in required_cols):
        print(f"  {resolution}: æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {[col for col in required_cols if col not in full_data.columns]}")
        return None
    
    # è·å–å”¯ä¸€çš„H3ç½‘æ ¼
    h3_grid = full_data.drop_duplicates(subset=['h3_index'])[['h3_index', 'latitude', 'longitude']].copy()
    print(f"  {resolution}: å”¯ä¸€H3ç½‘æ ¼æ•°: {len(h3_grid)}")
    
    # ğŸ”§ ä¿®å¤ï¼šres5ä¸å¢åŠ ç½‘æ ¼å¯†åº¦ï¼Œåªä½¿ç”¨çœŸæ­£çš„H3ç½‘æ ¼
    if resolution != 'res5' and len(h3_grid) < 500:  # res5ä¿æŒåŸæœ‰220ä¸ªç½‘æ ¼ï¼Œå…¶ä»–åˆ†è¾¨ç‡æ‰å¢åŠ å¯†åº¦
        print(f"  {resolution}: H3ç½‘æ ¼è¿‡äºç¨€ç–({len(h3_grid)}ä¸ª)ï¼Œå¢åŠ ç½‘æ ¼å¯†åº¦...")
        
        # è®¡ç®—è¾¹ç•Œ
        lat_min, lat_max = h3_grid['latitude'].min(), h3_grid['latitude'].max()
        lon_min, lon_max = h3_grid['longitude'].min(), h3_grid['longitude'].max()
        
        # ç”Ÿæˆæ›´å¯†é›†çš„ç½‘æ ¼
        if resolution == 'res7':
            lat_step = 0.008
            lon_step = 0.008
        elif resolution == 'res6':
            lat_step = 0.015
            lon_step = 0.015
        else:  # res5 - ä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨ä¿ç•™
            lat_step = 0.03
            lon_step = 0.03
        
        # ç”Ÿæˆå¯†é›†ç½‘æ ¼
        lats = np.arange(lat_min, lat_max + lat_step, lat_step)
        lons = np.arange(lon_min, lon_max + lon_step, lon_step)
        lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')
        lat_flat = lat_grid.flatten()
        lon_flat = lon_grid.flatten()
        
        # åˆå¹¶åŸæœ‰ç½‘æ ¼å’Œæ–°ç”Ÿæˆç½‘æ ¼
        additional_h3_indices = [f"{resolution}_dense_{i}" for i in range(len(lat_flat))]
        dense_grid = pd.DataFrame({
            'h3_index': additional_h3_indices,
            'latitude': lat_flat,
            'longitude': lon_flat
        })
        
        # åˆå¹¶ç½‘æ ¼
        h3_grid = pd.concat([h3_grid, dense_grid], ignore_index=True)
        print(f"  {resolution}: å¢åŠ å¯†é›†ç½‘æ ¼åæ€»æ•°: {len(h3_grid)}ä¸ª")
    elif resolution == 'res5':
        print(f"  {resolution}: ä¿æŒåŸæœ‰{len(h3_grid)}ä¸ªçœŸæ­£çš„H3ç½‘æ ¼ï¼Œä¸å¢åŠ å¯†åº¦")
    
    return h3_grid


def enhanced_spatial_interpolation_for_clustering(sample_coords, sample_shap, grid_coords, method='idw'):
    """
    å¢å¼ºçš„ç©ºé—´æ’å€¼æ–¹æ³•ï¼Œç¡®ä¿ç©ºé—´è¿ç»­æ€§
    (ä¸¥æ ¼å­¦ä¹ geoshapley_spatial_top3.pyçš„å®ç°)
    
    å‚æ•°:
    - sample_coords: é‡‡æ ·ç‚¹åæ ‡
    - sample_shap: é‡‡æ ·ç‚¹SHAPå€¼
    - grid_coords: ç½‘æ ¼ç‚¹åæ ‡
    - method: æ’å€¼æ–¹æ³• ('idw', 'rbf', 'knn')
    
    è¿”å›:
    - grid_shap: æ’å€¼åçš„ç½‘æ ¼SHAPå€¼
    """
    from scipy.spatial.distance import cdist
    
    if len(sample_coords) < 3:
        # æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨ç®€å•KNN
        from sklearn.neighbors import KNeighborsRegressor
        knn = KNeighborsRegressor(n_neighbors=min(len(sample_coords), 3), weights='distance')
        knn.fit(sample_coords, sample_shap)
        return knn.predict(grid_coords)
    
    if method == 'idw':
        # åè·ç¦»æƒé‡æ’å€¼
        distances = cdist(grid_coords, sample_coords)
        # é¿å…é™¤é›¶
        distances = np.maximum(distances, 1e-10)
        
        # è®¡ç®—æƒé‡ï¼ˆp=2ä¸ºæ ‡å‡†IDWï¼‰
        weights = 1.0 / (distances ** 2)
        
        # å½’ä¸€åŒ–æƒé‡
        weights_sum = weights.sum(axis=1, keepdims=True)
        weights_norm = weights / weights_sum
        
        # è®¡ç®—æ’å€¼å€¼
        grid_shap = (weights_norm * sample_shap).sum(axis=1)
        
    elif method == 'rbf':
        # å¾„å‘åŸºå‡½æ•°æ’å€¼
        try:
            from scipy.interpolate import RBFInterpolator
            rbf = RBFInterpolator(sample_coords, sample_shap, kernel='linear')
            grid_shap = rbf(grid_coords)
        except:
            # RBFå¤±è´¥ï¼Œå›é€€åˆ°IDW
            return enhanced_spatial_interpolation_for_clustering(sample_coords, sample_shap, grid_coords, method='idw')
    
    else:  # knn
        # KNNæ’å€¼
        from sklearn.neighbors import KNeighborsRegressor
        n_neighbors = min(min(10, len(sample_coords)), max(3, len(sample_coords) // 2))
        knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')
        knn.fit(sample_coords, sample_shap)
        grid_shap = knn.predict(grid_coords)
    
    return grid_shap


def map_shap_to_full_grid_for_clustering(shap_values_by_feature, X_sample, full_h3_grid, feature_name):
    """
    å°†é‡‡æ ·è®¡ç®—çš„SHAPå€¼æ˜ å°„åˆ°å®Œæ•´çš„H3ç½‘æ ¼ï¼Œä¼˜åŒ–æ’å€¼ç®—æ³•é¿å…å­¤ç«‹ç‚¹
    (ä¸¥æ ¼å­¦ä¹ geoshapley_spatial_top3.pyçš„å®ç°)
    
    å‚æ•°:
    - shap_values_by_feature: ç‰¹å¾SHAPå€¼å­—å…¸
    - X_sample: é‡‡æ ·æ•°æ®
    - full_h3_grid: å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®
    - feature_name: ç‰¹å¾åç§°
    
    è¿”å›:
    - full_shap_values: å®Œæ•´ç½‘æ ¼çš„SHAPå€¼
    """
    if feature_name not in shap_values_by_feature:
        return None
    
    # è·å–é‡‡æ ·çš„SHAPå€¼
    sample_shap = np.array(shap_values_by_feature[feature_name])
    
    # ç¡®ä¿X_sampleçš„è¡Œæ•°ä¸SHAPå€¼æ•°é‡åŒ¹é…
    if len(X_sample) != len(sample_shap):
        if len(X_sample) > len(sample_shap):
            print(f"    {feature_name}: è°ƒæ•´X_sampleè¡Œæ•° {len(X_sample)} â†’ {len(sample_shap)}ï¼ˆåŒ¹é…SHAPå€¼æ•°é‡ï¼‰")
            X_sample = X_sample.iloc[:len(sample_shap)]
        else:
            print(f"    {feature_name}: è°ƒæ•´SHAPå€¼æ•°é‡ {len(sample_shap)} â†’ {len(X_sample)}ï¼ˆåŒ¹é…X_sampleè¡Œæ•°ï¼‰")
            sample_shap = sample_shap[:len(X_sample)]
    
    # ğŸ¯ ä¼˜å…ˆä½¿ç”¨h3_indexè¿›è¡Œç›´æ¥æ˜ å°„
    if 'h3_index' in X_sample.columns:
        print(f"    {feature_name}: ä½¿ç”¨h3_indexç›´æ¥æ˜ å°„")
        
        # åˆ›å»ºh3_indexåˆ°SHAPå€¼çš„æ˜ å°„
        sample_h3_shap = pd.DataFrame({
            'h3_index': X_sample['h3_index'],
            'shap_value': sample_shap
        })
        
        # ğŸ”¥ æ™ºèƒ½èšåˆé¿å…è¿‡åº¦å¹³å‡åŒ–å¯¼è‡´çš„å­¤ç«‹ç‚¹
        def smart_aggregate_shap(group):
            values = group['shap_value'].values
            if len(values) == 1:
                return values[0]
            elif len(values) == 2:
                # ä¸¤ä¸ªå€¼ï¼šä½¿ç”¨ä¸­ä½æ•°
                return np.median(values)
            elif len(values) <= 3:
                # å°‘é‡å€¼ï¼šä½¿ç”¨ä¸­ä½æ•°ä¿æŒåˆ†å¸ƒç‰¹å¾
                return np.median(values)
            else:
                # å¤šä¸ªå€¼ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªä»¥ä¿æŒåŸå§‹åˆ†å¸ƒç‰¹å¾
                np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
                return np.random.choice(values)
        
        sample_h3_shap = sample_h3_shap.groupby('h3_index').apply(smart_aggregate_shap).reset_index()
        sample_h3_shap.columns = ['h3_index', 'shap_value']
        
        print(f"    {feature_name}: æ™ºèƒ½èšåˆå®Œæˆï¼Œé¿å…è¿‡åº¦å¹³å‡åŒ–")
        
        # åˆå¹¶åˆ°å®Œæ•´ç½‘æ ¼
        full_grid_with_shap = full_h3_grid.merge(
            sample_h3_shap, 
            on='h3_index', 
            how='left'
        )
        
        # ğŸ”§ ä¼˜åŒ–æ’å€¼ç­–ç•¥
        missing_mask = full_grid_with_shap['shap_value'].isna()
        if missing_mask.any():
            missing_count = missing_mask.sum()
            total_count = len(full_grid_with_shap)
            missing_ratio = missing_count / total_count
            
            print(f"    {feature_name}: {missing_count}/{total_count}ä¸ªç½‘æ ¼ç¼ºå°‘SHAPå€¼({missing_ratio:.1%})ï¼Œä½¿ç”¨å¢å¼ºæ’å€¼")
            
            # ä½¿ç”¨å¢å¼ºçš„ç©ºé—´æ’å€¼
            known_coords = full_grid_with_shap.loc[~missing_mask, ['latitude', 'longitude']].values
            known_shap = full_grid_with_shap.loc[~missing_mask, 'shap_value'].values
            unknown_coords = full_grid_with_shap.loc[missing_mask, ['latitude', 'longitude']].values
            
            if len(known_coords) > 0 and len(unknown_coords) > 0:
                # æ ¹æ®ç¼ºå¤±æ¯”ä¾‹é€‰æ‹©æ’å€¼æ–¹æ³•
                if missing_ratio > 0.5:
                    # ç¼ºå¤±è¾ƒå¤šï¼Œä½¿ç”¨IDW
                    method = 'idw'
                elif missing_ratio > 0.2:
                    # ç¼ºå¤±ä¸­ç­‰ï¼Œä½¿ç”¨RBF
                    method = 'rbf'
                else:
                    # ç¼ºå¤±è¾ƒå°‘ï¼Œä½¿ç”¨KNN
                    method = 'knn'
                
                predicted_shap = enhanced_spatial_interpolation_for_clustering(
                    known_coords, known_shap, unknown_coords, method=method
                )
                
                # ğŸ”¥ ä¸ºæ’å€¼ç»“æœæ·»åŠ å—æ§å˜å¼‚æ€§ï¼Œé¿å…è¿‡åº¦å¹³æ»‘åŒ–
                if len(predicted_shap) > 0 and np.std(known_shap) > 0:
                    # æ·»åŠ å°‘é‡å˜å¼‚æ€§ï¼ŒåŸºäºåŸå§‹æ•°æ®çš„æ ‡å‡†å·®
                    np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
                    noise_scale = np.std(known_shap) * 0.05  # 5%çš„å˜å¼‚æ€§
                    noise = np.random.normal(0, noise_scale, len(predicted_shap))
                    predicted_shap = predicted_shap + noise
                    print(f"    {feature_name}: æ’å€¼æ·»åŠ {noise_scale:.4f}å˜å¼‚æ€§ï¼Œä¿æŒè‡ªç„¶åˆ†å¸ƒ")
                
                # å¡«å……ç¼ºå¤±å€¼
                full_grid_with_shap.loc[missing_mask, 'shap_value'] = predicted_shap
                print(f"    {feature_name}: ä½¿ç”¨{method.upper()}æ’å€¼å®Œæˆ")
            else:
                # ğŸ”¥ é¿å…å¡«å……0å€¼å¯¼è‡´å­¤ç«‹ç‚¹ï¼Œä½¿ç”¨å·²çŸ¥å€¼çš„ç»Ÿè®¡ä¿¡æ¯
                if len(known_shap) > 0:
                    # ä½¿ç”¨å·²çŸ¥SHAPå€¼çš„ä¸­ä½æ•°å¡«å……ï¼Œé¿å…äº§ç”Ÿä¸è‡ªç„¶çš„0å€¼å­¤ç«‹ç‚¹
                    fill_value = np.median(known_shap)
                    full_grid_with_shap.loc[missing_mask, 'shap_value'] = fill_value
                    print(f"    {feature_name}: ä½¿ç”¨ä¸­ä½æ•°å¡«å…… ({fill_value:.4f})ï¼Œé¿å…0å€¼å­¤ç«‹ç‚¹")
                else:
                    # æœ€åçš„å›é€€é€‰é¡¹ï¼šä½¿ç”¨æ•´ä½“SHAPå€¼çš„ä¸­ä½æ•°
                    overall_median = np.median(sample_shap) if len(sample_shap) > 0 else 0
                    full_grid_with_shap.loc[missing_mask, 'shap_value'] = overall_median
                    print(f"    {feature_name}: ä½¿ç”¨æ•´ä½“ä¸­ä½æ•°å¡«å…… ({overall_median:.4f})")
        
        return full_grid_with_shap
    
    # å¦‚æœæ²¡æœ‰h3_indexï¼Œä½¿ç”¨å¢å¼ºçš„ç©ºé—´åŒ¹é…
    else:
        print(f"    {feature_name}: ä½¿ç”¨å¢å¼ºç©ºé—´æ’å€¼ï¼ˆX_sampleç¼ºå°‘h3_indexåˆ—ï¼‰")
        
        # ç¡®ä¿X_sampleæœ‰ç»çº¬åº¦
        if 'latitude' not in X_sample.columns or 'longitude' not in X_sample.columns:
            print(f"    {feature_name}: X_sampleç¼ºå°‘ç»çº¬åº¦ï¼Œæ— æ³•è¿›è¡Œç©ºé—´åŒ¹é…")
            return None
        
        # ç¡®ä¿æ ·æœ¬æ•°é‡åŒ¹é…
        sample_coords = X_sample[['latitude', 'longitude']].values[:len(sample_shap)]
        grid_coords = full_h3_grid[['latitude', 'longitude']].values
        
        # ä½¿ç”¨å¢å¼ºæ’å€¼
        grid_shap = enhanced_spatial_interpolation_for_clustering(
            sample_coords, sample_shap, grid_coords, method='rbf'
        )
        
        # ğŸ”¥ ä¸ºæ’å€¼ç»“æœæ·»åŠ å—æ§å˜å¼‚æ€§ï¼Œé¿å…è¿‡åº¦å¹³æ»‘åŒ–
        if len(grid_shap) > 0 and np.std(sample_shap) > 0:
            np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
            noise_scale = np.std(sample_shap) * 0.03  # 3%çš„å˜å¼‚æ€§ï¼ˆæ¯”h3è·¯å¾„ç¨å°ï¼‰
            noise = np.random.normal(0, noise_scale, len(grid_shap))
            grid_shap = grid_shap + noise
            print(f"    {feature_name}: æ’å€¼æ·»åŠ {noise_scale:.4f}å˜å¼‚æ€§ï¼Œä¿æŒè‡ªç„¶åˆ†å¸ƒ")
        
        # åˆ›å»ºç»“æœDataFrame
        full_grid_with_shap = full_h3_grid.copy()
        full_grid_with_shap['shap_value'] = grid_shap
        
        print(f"    {feature_name}: å¢å¼ºç©ºé—´æ’å€¼å®Œæˆ")
        return full_grid_with_shap


def generate_full_grid_data_for_clustering(res_data, res):
    """
    ä¸¥æ ¼å­¦ä¹ geoshapley_spatial_top3.pyå®ç°ï¼šç”Ÿæˆå®Œæ•´ç½‘æ ¼æ•°æ®ç”¨äºèšç±»
    
    å‚æ•°:
    - res_data: åŸå§‹åˆ†è¾¨ç‡æ•°æ®
    - res: åˆ†è¾¨ç‡æ ‡è¯†ç¬¦
    
    è¿”å›:
    - enhanced_data: åŒ…å«å®Œæ•´ç½‘æ ¼SHAPæ•°æ®çš„å¢å¼ºç»“æœ
    """
    print(f"    ğŸ”§ ä¸º{res}ç”Ÿæˆå®Œæ•´ç½‘æ ¼èšç±»æ•°æ®ï¼ˆå­¦ä¹ geoshapley_spatial_top3.pyï¼‰...")
    
    try:
        # 1. è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®ï¼ˆå­¦ä¹ get_full_h3_grid_dataå‡½æ•°ï¼‰
        full_h3_grid = get_full_h3_grid_data_for_clustering(res_data, res)
        if full_h3_grid is None:
            print(f"    âŒ {res}æ— æ³•è·å–å®Œæ•´H3ç½‘æ ¼")
            return res_data
        
        # 2. è·å–åŸå§‹SHAPæ•°æ®
        shap_values_by_feature = res_data.get('shap_values_by_feature', {})
        X_sample = res_data.get('X_sample') if 'X_sample' in res_data else res_data.get('X')
        
        if not shap_values_by_feature or X_sample is None:
            print(f"    âŒ {res}ç¼ºå°‘SHAPæ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ’å€¼")
            return res_data
        
        print(f"    ğŸ“Š åŸå§‹æ•°æ®: {len(X_sample)}ä¸ªé‡‡æ ·ç‚¹ï¼Œ{len(shap_values_by_feature)}ä¸ªSHAPç‰¹å¾")
        print(f"    ğŸ”² ç›®æ ‡ç½‘æ ¼: {len(full_h3_grid)}ä¸ªå®Œæ•´H3ç½‘æ ¼")
        
        # 3. å¯¹11ä¸ªä¸»æ•ˆåº”ç‰¹å¾è¿›è¡Œé«˜è´¨é‡æ’å€¼ï¼ˆå­¦ä¹ map_shap_to_full_gridå‡½æ•°ï¼‰
        enhanced_shap_values_by_feature = {}
        
        # ğŸ¯ å®šä¹‰11ä¸ªä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾ï¼ˆç”¨æˆ·ç¡®è®¤çš„æ­£ç¡®ç»„æˆï¼‰
        target_features = {
            'temperature', 'precipitation',  # 2ä¸ªæ°”å€™ç‰¹å¾
            'nightlight', 'road_density', 'mining_density', 'population_density',  # 4ä¸ªäººç±»æ´»åŠ¨
            'elevation', 'slope',  # 2ä¸ªåœ°å½¢ç‰¹å¾
            'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'  # 3ä¸ªåœŸåœ°è¦†ç›–
        }
        
        print(f"    ğŸ¯ å¼€å§‹å¯¹11ä¸ªä¸»æ•ˆåº”ç‰¹å¾è¿›è¡Œé«˜è´¨é‡æ’å€¼...")
        
        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ’å€¼
        successful_interpolations = 0
        for feat_name in target_features:
            if feat_name in shap_values_by_feature:
                try:
                    # ä½¿ç”¨å­¦ä¹ çš„æ˜ å°„å‡½æ•°
                    full_grid_with_shap = map_shap_to_full_grid_for_clustering(
                        {feat_name: shap_values_by_feature[feat_name]}, 
                        X_sample, 
                        full_h3_grid, 
                        feat_name
                    )
                    
                    if full_grid_with_shap is not None:
                        enhanced_shap_values_by_feature[feat_name] = full_grid_with_shap['shap_value'].values
                        successful_interpolations += 1
                        print(f"    âœ“ {feat_name}: é«˜è´¨é‡æ’å€¼æˆåŠŸ ({len(enhanced_shap_values_by_feature[feat_name])}ä¸ªç½‘æ ¼)")
                    else:
                        print(f"    âŒ {feat_name}: æ’å€¼å¤±è´¥")
                        
                except Exception as e:
                    print(f"    âš ï¸ {feat_name}: æ’å€¼å¼‚å¸¸: {e}")
                    # ä½¿ç”¨ä¸­ä½æ•°å¡«å……ä½œä¸ºåå¤‡
                    shap_vals = shap_values_by_feature[feat_name]
                    median_val = np.median(shap_vals) if len(shap_vals) > 0 else 0
                    enhanced_shap_values_by_feature[feat_name] = np.full(len(full_h3_grid), median_val)
                    print(f"    â†³ ä½¿ç”¨ä¸­ä½æ•°å¡«å……: {median_val:.4f}")
            else:
                print(f"    âŒ ç¼ºå°‘ç‰¹å¾: {feat_name}")
                # ä½¿ç”¨é›¶å€¼å¡«å……
                enhanced_shap_values_by_feature[feat_name] = np.zeros(len(full_h3_grid))
        
        print(f"    ğŸ“ˆ æ’å€¼æ€»ç»“: {successful_interpolations}/11ä¸ªç‰¹å¾æˆåŠŸæ’å€¼")
        
        # 4. åˆ›å»ºå¢å¼ºçš„X_sample
        enhanced_X_sample = full_h3_grid.copy()
        
        # æ·»åŠ VHIæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        original_df = res_data.get('df')
        if original_df is not None and 'VHI' in original_df.columns:
            vhi_by_h3 = original_df.groupby('h3_index')['VHI'].mean().reset_index()
            enhanced_X_sample = enhanced_X_sample.merge(vhi_by_h3, on='h3_index', how='left')
            # å¡«å……ç¼ºå¤±çš„VHIå€¼
            if enhanced_X_sample['VHI'].isna().any():
                enhanced_X_sample['VHI'].fillna(enhanced_X_sample['VHI'].mean(), inplace=True)
            print(f"    âœ“ æ·»åŠ VHIç›®æ ‡å€¼")
        
        # 5. åˆ›å»ºå¢å¼ºçš„ç»“æœæ•°æ®
        enhanced_res_data = res_data.copy()
        enhanced_res_data['enhanced_shap_values_by_feature'] = enhanced_shap_values_by_feature
        enhanced_res_data['enhanced_X_sample'] = enhanced_X_sample
        
        print(f"    âœ… {res}å®Œæ•´ç½‘æ ¼æ•°æ®ç”ŸæˆæˆåŠŸ:")
        print(f"      â€¢ ç½‘æ ¼æ•°é‡: {len(enhanced_X_sample)}")
        print(f"      â€¢ ç¯å¢ƒç‰¹å¾: {len(enhanced_shap_values_by_feature)}ä¸ª")
        print(f"      â€¢ æ•°æ®å¢å¼ºå€æ•°: {len(enhanced_X_sample)/len(X_sample):.1f}x")
        print(f"      â€¢ æ’å€¼æ–¹æ³•: å­¦ä¹ è‡ªgeoshapley_spatial_top3.py")
        
        return enhanced_res_data
        
    except Exception as e:
        print(f"    âŒ {res}å®Œæ•´ç½‘æ ¼æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return res_data


def preprocess_data_for_clustering(results_by_resolution, top_n):
    """
    é¢„å¤„ç†æ•°æ®ç”¨äºç©ºé—´èšç±»
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼SHAPå€¼è¿›è¡Œèšç±»åˆ†æ
    2. å¦‚æœæ’å€¼ä¸å¯ç”¨ï¼Œå›é€€åˆ°ä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®
    3. ç¡®ä¿ä¸å…¶ä»–SHAPå›¾è¡¨ä¿æŒä¸€è‡´çš„æ•°æ®åŸºç¡€
    
    å‚æ•°:
    - results_by_resolution: æŒ‰åˆ†è¾¨ç‡ç»„ç»‡çš„ç»“æœå­—å…¸
    - top_n: æ‹Ÿç”¨äºèšç±»çš„é¡¶çº§SHAPç‰¹å¾æ•°é‡
    
    è¿”å›:
    - processed: é¢„å¤„ç†åçš„èšç±»æ•°æ®å­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªåˆ†è¾¨ç‡çš„ { 'shap_features', 'coords_df', 'top_features', 'target_values' }
    """
    if not results_by_resolution:
        return None
    
    print("  ğŸ”§ å°è¯•ä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®è¿›è¡Œèšç±»é¢„å¤„ç†...")
    
    # å°è¯•ä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®
    enhanced_results = {}
    
    for res in ['res7', 'res6', 'res5']:
        if res not in results_by_resolution:
            continue
            
        print(f"\n  ğŸ“Š å¤„ç†{res}çš„å®Œæ•´ç½‘æ ¼èšç±»åˆ†æ...")
        
        # è·å–åŸå§‹æ•°æ®
        res_data = results_by_resolution[res]
        shap_values_by_feature = res_data.get('shap_values_by_feature', {})
        X_sample = res_data.get('X_sample') if 'X_sample' in res_data else res_data.get('X')
        
        if not shap_values_by_feature or X_sample is None:
            print(f"    âš ï¸ {res}ç¼ºå°‘SHAPæ•°æ®ï¼Œä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
            enhanced_results[res] = res_data
            continue
        
        # è·å–å®Œæ•´çš„H3ç½‘æ ¼æ•°æ®
        try:
            from .geoshapley_spatial_top3 import get_full_h3_grid_data
            full_h3_grid = get_full_h3_grid_data(res_data, res)
            if full_h3_grid is None:
                print(f"    âš ï¸ {res}æ— æ³•è·å–å®Œæ•´H3ç½‘æ ¼ï¼Œä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
                enhanced_results[res] = res_data
                continue
        except ImportError:
            print(f"    âš ï¸ æ— æ³•å¯¼å…¥å®Œæ•´ç½‘æ ¼åŠŸèƒ½ï¼Œä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
            enhanced_results[res] = res_data
            continue
        
        # ğŸ”‡ ç§»é™¤å†—ä½™çš„æ’å€¼å¯¼å…¥å°è¯•ï¼Œä½¿ç”¨ç°æœ‰çš„åŠ¨æ€æ’å€¼
        # å®é™…çš„æ’å€¼åŠŸèƒ½ç”±å…¶ä»–æ¨¡å—å¤„ç†ï¼Œè¿™é‡Œçš„å¯¼å…¥æ€»æ˜¯å¤±è´¥ä½†ä¸å½±å“èšç±»åˆ†æ
        interpolated_shap_data = None  # è·³è¿‡é¢„æ’å€¼ï¼Œç›´æ¥ä½¿ç”¨å¼ºåˆ¶æ’å€¼
        
        if interpolated_shap_data is None:
            print(f"    âŒ {res}æ’å€¼å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
            enhanced_results[res] = res_data
            continue
        
        # åˆ›å»ºå¢å¼ºçš„ç»“æœæ•°æ®
        enhanced_res_data = res_data.copy()
        
        # ä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®
        enhanced_res_data['enhanced_X_sample'] = interpolated_shap_data['X_sample']
        enhanced_res_data['enhanced_shap_values_by_feature'] = {}
        
        # æ„å»ºå¢å¼ºçš„SHAPå€¼å­—å…¸
        feature_names = interpolated_shap_data['feature_names']
        shap_values_list = interpolated_shap_data['shap_values']
        
        for i, feat_name in enumerate(feature_names):
            if i < len(shap_values_list):
                enhanced_res_data['enhanced_shap_values_by_feature'][feat_name] = shap_values_list[i]
        
        enhanced_results[res] = enhanced_res_data
        
        print(f"    âœ… {res}å®Œæ•´ç½‘æ ¼èšç±»åˆ†ææ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"      â€¢ å®Œæ•´ç½‘æ ¼æ•°æ®é‡: {len(interpolated_shap_data['X_sample'])}ä¸ªç½‘æ ¼")
        print(f"      â€¢ æ•°æ®å¢å¼ºå€æ•°: {len(interpolated_shap_data['X_sample'])/len(X_sample):.1f}x")
        print(f"      â€¢ ç‰¹å¾æ•°é‡: {len(enhanced_res_data['enhanced_shap_values_by_feature'])}ä¸ª")
    
    # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨å®Œæ•´ç½‘æ ¼æ•°æ®ï¼Œç¡®ä¿ç©ºé—´è¿ç»­æ€§
    print(f"  ğŸ¯ å¼ºåˆ¶ä½¿ç”¨å®Œæ•´ç½‘æ ¼æ•°æ®è¿›è¡Œèšç±»åˆ†æ...")
    
    # ğŸ¯ ä¿®å¤ï¼šç›´æ¥å¼ºåˆ¶ç”Ÿæˆå®Œæ•´ç½‘æ ¼æ•°æ®ï¼Œç¡®ä¿ä½¿ç”¨æ’å€¼åçš„11ä¸ªä¸»æ•ˆåº”ç‰¹å¾
    print(f"  âš ï¸ å¼ºåˆ¶é‡æ–°ç”Ÿæˆå®Œæ•´ç½‘æ ¼æ•°æ®ï¼Œç¡®ä¿ä½¿ç”¨11ä¸ªä¸»æ•ˆåº”ç‰¹å¾...")
    final_results = {}
    for res in ['res5', 'res6', 'res7']:
        if res in results_by_resolution:
            print(f"\n  ğŸ”„ ä¸º{res}å¼ºåˆ¶ç”Ÿæˆæ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®...")
            final_results[res] = generate_full_grid_data_for_clustering(results_by_resolution[res], res)
    data_source_info = "Full Grid Interpolated"  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä¸æ£€æŸ¥é€»è¾‘åŒ¹é…
    
    processed = {}
    for res, results in final_results.items():
        try:
            # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®è¿›è¡Œèšç±»
            if 'enhanced_shap_values_by_feature' in results and 'enhanced_X_sample' in results:
                shap_values_by_feature = results['enhanced_shap_values_by_feature']
                features = results['enhanced_X_sample']
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                original_sample_count = len(results_by_resolution[res].get('X_sample', []))
                enhanced_sample_count = len(features)
                data_multiplier = enhanced_sample_count / original_sample_count if original_sample_count > 0 else 0
                
                print(f"    {res}: ä½¿ç”¨æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®è¿›è¡Œèšç±»")
                print(f"      â€¢ åŸå§‹é‡‡æ ·: {original_sample_count}ä¸ªç‚¹")
                print(f"      â€¢ å®Œæ•´ç½‘æ ¼: {enhanced_sample_count}ä¸ªç‚¹")
                print(f"      â€¢ æ•°æ®å¢å¼º: {data_multiplier:.1f}å€")
                print(f"      â€¢ æ’å€¼ç‰¹å¾: {len(shap_values_by_feature)}ä¸ª")
                
                # ä»å¢å¼ºçš„ç‰¹å¾æ•°æ®ä¸­æ„å»ºSHAPç‰¹å¾çŸ©é˜µ
                feature_names = []
                shap_matrix_list = []
                
                # ğŸ¯ å®šä¹‰11ä¸ªä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾åˆ—è¡¨ï¼Œä¸ç”Ÿæˆå‡½æ•°ä¿æŒä¸€è‡´ï¼ˆç”¨æˆ·ç¡®è®¤çš„æ­£ç¡®ç»„æˆï¼‰
                environmental_features = {
                    'temperature', 'precipitation',  # 2ä¸ªæ°”å€™ç‰¹å¾
                    'nightlight', 'road_density', 'mining_density', 'population_density',  # 4ä¸ªäººç±»æ´»åŠ¨
                    'elevation', 'slope',  # 2ä¸ªåœ°å½¢ç‰¹å¾
                    'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'  # 3ä¸ªåœŸåœ°è¦†ç›–
                }
                # æ€»è®¡11ä¸ªä¸»æ•ˆåº”ç‰¹å¾
                
                for feat_name, shap_vals in shap_values_by_feature.items():
                    # ğŸ”¥ ä¿®å¤ç‰¹å¾é€‰æ‹©é€»è¾‘ï¼šä¸¥æ ¼åªä¿ç•™ä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾
                    if feat_name.lower() in {f.lower() for f in environmental_features}:
                        feature_names.append(feat_name)
                        shap_matrix_list.append(shap_vals)
                        print(f"    âœ“ å¢å¼ºæ•°æ®é€‰æ‹©ç¯å¢ƒç‰¹å¾: {feat_name}")
                    else:
                        print(f"    âœ— å¢å¼ºæ•°æ®æ’é™¤éç¯å¢ƒç‰¹å¾: {feat_name}")
                
                if shap_matrix_list:
                    shap_values = np.column_stack(shap_matrix_list)
                else:
                    print(f"è­¦å‘Š: {res} æ²¡æœ‰æœ‰æ•ˆçš„SHAPç‰¹å¾")
                    continue
            else:
                # ğŸš¨ å¿…é¡»ä½¿ç”¨å®Œæ•´ç½‘æ ¼æ•°æ®ï¼Œä¸å…è®¸å›é€€åˆ°åŸå§‹ç¨€ç–é‡‡æ ·æ•°æ®
                print(f"    âŒ {res}: ç¼ºå°‘æ’å€¼åçš„å®Œæ•´ç½‘æ ¼æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè¿ç»­ç©ºé—´èšç±»")
                print(f"      â€¢ éœ€è¦: enhanced_shap_values_by_feature å’Œ enhanced_X_sample")
                print(f"      â€¢ è·³è¿‡æ­¤åˆ†è¾¨ç‡ï¼Œå› ä¸ºå¿…é¡»ä½¿ç”¨å®Œæ•´ç½‘æ ¼æ•°æ®æ‰èƒ½å¾—åˆ°è¿ç»­èšç±»æ•ˆæœ")
                continue
                
                # ğŸ”¥ ç¡®ä¿featuresæ˜¯DataFrameæ ¼å¼
                if not isinstance(features, pd.DataFrame):
                    # å°è¯•è½¬æ¢ä¸ºDataFrame
                    if 'feature_names' in results and results['feature_names'] is not None:
                        feature_cols = results['feature_names']
                    else:
                        # ç”Ÿæˆé»˜è®¤åˆ—å
                        feature_cols = [f'feature_{i}' for i in range(features.shape[1] if hasattr(features, 'shape') else len(features[0]))]
                    
                    features = pd.DataFrame(features, columns=feature_cols)
                    print(f"    {res}: è½¬æ¢Xä¸ºDataFrameæ ¼å¼ï¼Œåˆ—æ•°: {len(features.columns)}")
                
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰åæ ‡ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•æ·»åŠ 
                if 'latitude' not in features.columns or 'longitude' not in features.columns:
                    print(f"    {res}: ç‰¹å¾æ•°æ®ç¼ºå°‘åæ ‡ä¿¡æ¯ï¼Œå°è¯•ä»dfè·å–...")
                    
                    # å°è¯•ä»dfå­—æ®µè·å–åæ ‡ä¿¡æ¯
                    if 'df' in results and results['df'] is not None:
                        df_full = results['df']
                        if 'latitude' in df_full.columns and 'longitude' in df_full.columns:
                            # ä½¿ç”¨å‰Nè¡Œçš„åæ ‡ï¼ˆä¸featuresé•¿åº¦åŒ¹é…ï¼‰
                            n_features = len(features)
                            coord_data = df_full[['latitude', 'longitude']].iloc[:n_features].reset_index(drop=True)
                            
                            # æ·»åŠ åæ ‡åˆ—
                            features = features.reset_index(drop=True)
                            features['latitude'] = coord_data['latitude']
                            features['longitude'] = coord_data['longitude']
                            
                            # å¦‚æœæœ‰h3_indexä¹Ÿæ·»åŠ 
                            if 'h3_index' in df_full.columns:
                                features['h3_index'] = df_full['h3_index'].iloc[:n_features].reset_index(drop=True)
                            
                            print(f"    {res}: ä»dfæ·»åŠ åæ ‡ä¿¡æ¯æˆåŠŸ")
                        else:
                            print(f"    {res}: dfä¸­ä¹Ÿç¼ºå°‘åæ ‡ä¿¡æ¯ï¼Œè·³è¿‡æ­¤åˆ†è¾¨ç‡")
                            continue
                    else:
                        print(f"    {res}: æ— æ³•è·å–åæ ‡ä¿¡æ¯ï¼Œè·³è¿‡æ­¤åˆ†è¾¨ç‡")
                        continue
            
            if shap_values is None:
                print(f"è­¦å‘Š: {res} ç¼ºå°‘å¿…è¦çš„SHAPæ•°æ®")
                continue
            
            # è·å–ç›®æ ‡å€¼ - åº”è¯¥å’Œfeaturesçš„é•¿åº¦ä¸€è‡´
            target = None
            if 'enhanced_X_sample' in results:
                # ğŸ”¥ ä¿®å¤ï¼šå¯¹äºå¢å¼ºæ•°æ®ï¼Œä»å®Œæ•´ç½‘æ ¼ä¸­è·å–VHIç›®æ ‡å€¼
                enhanced_X_sample = results['enhanced_X_sample']
                
                # å¦‚æœå¢å¼ºçš„X_sampleä¸­æœ‰VHIåˆ—ï¼Œç›´æ¥ä½¿ç”¨
                if 'VHI' in enhanced_X_sample.columns:
                    target = enhanced_X_sample['VHI'].values
                    print(f"    {res}: ä»å¢å¼ºæ•°æ®ä¸­è·å–VHIç›®æ ‡å€¼ ({len(target)}ä¸ª)")
                
                # å¦åˆ™å°è¯•ä»åŸå§‹æ•°æ®ä¸­è·å–å¹¶æ˜ å°„
                elif 'y' in results_by_resolution[res] or 'y_sample' in results_by_resolution[res]:
                    # è·å–åŸå§‹ç›®æ ‡å€¼ - ğŸ”¥ ä¿®å¤ï¼šé¿å…æ•°ç»„å¸ƒå°”æ¯”è¾ƒé”™è¯¯
                    original_y = results_by_resolution[res].get('y_sample')
                    if original_y is None:
                        original_y = results_by_resolution[res].get('y')
                    
                    original_X = results_by_resolution[res].get('X_sample')
                    if original_X is None:
                        original_X = results_by_resolution[res].get('X')
                    
                    if original_y is not None and original_X is not None and isinstance(original_X, pd.DataFrame):
                        # ç¡®ä¿åŸå§‹æ•°æ®æœ‰åæ ‡ä¿¡æ¯
                        if 'latitude' in original_X.columns and 'longitude' in original_X.columns:
                            # ä½¿ç”¨KNNå°†åŸå§‹VHIå€¼æ˜ å°„åˆ°å¢å¼ºç½‘æ ¼
                            original_coords = original_X[['latitude', 'longitude']].values
                            enhanced_coords = enhanced_X_sample[['latitude', 'longitude']].values
                            
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            min_len = min(len(original_y), len(original_coords))
                            # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æ•°ç»„ç±»å‹
                            if hasattr(original_y, '__getitem__') and not isinstance(original_y, (str, dict)):
                                original_y_aligned = original_y[:min_len]
                            else:
                                original_y_aligned = [original_y] * min_len
                            original_coords_aligned = original_coords[:min_len]
                            
                            try:
                                from sklearn.neighbors import KNeighborsRegressor
                                n_neighbors = min(5, len(original_coords_aligned))
                                knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')
                                knn.fit(original_coords_aligned, original_y_aligned)
                                target = knn.predict(enhanced_coords)
                                print(f"    {res}: é€šè¿‡KNNæ’å€¼è·å–VHIç›®æ ‡å€¼ ({len(target)}ä¸ª)")
                            except Exception as e:
                                print(f"    {res}: KNNæ’å€¼å¤±è´¥: {e}ï¼Œä½¿ç”¨å¹³å‡å€¼å¡«å……")
                                # ä½¿ç”¨åŸå§‹VHIçš„å¹³å‡å€¼å¡«å……
                                mean_vhi = np.mean(original_y_aligned) if len(original_y_aligned) > 0 else 0.5
                                target = np.full(len(enhanced_coords), mean_vhi)
                        else:
                            print(f"    {res}: åŸå§‹æ•°æ®ç¼ºå°‘åæ ‡ä¿¡æ¯ï¼Œæ— æ³•æ˜ å°„VHI")
                            # ä½¿ç”¨é»˜è®¤å€¼
                            target = np.full(len(enhanced_X_sample), 0.5)
                    else:
                        print(f"    {res}: æ— æ³•è·å–åŸå§‹VHIæ•°æ®è¿›è¡Œæ˜ å°„")
                        target = np.full(len(enhanced_X_sample), 0.5)
                else:
                    print(f"    {res}: æ— VHIæ•°æ®å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    target = np.full(len(enhanced_X_sample), 0.5)
                
            elif 'y_sample' in results:
                target = results.get('y_sample')
            elif 'y' in results:
                # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨yæ•°æ®å¹¶ç¡®ä¿é•¿åº¦åŒ¹é…
                y_full = results['y']
                if isinstance(features, pd.DataFrame):
                    if len(y_full) > len(features):
                        # ä½¿ç”¨å‰Nè¡ŒåŒ¹é…featuresçš„é•¿åº¦
                        target = y_full[:len(features)]
                        print(f"    {res}: è°ƒæ•´yé•¿åº¦ä»{len(y_full)}åˆ°{len(features)}")
                    else:
                        target = y_full
                else:
                    target = y_full
            else:
                print(f"    {res}: æ— ç›®æ ‡å€¼æ•°æ®")
                target = None
            
            # ç¡®ä¿featuresæ˜¯DataFrameæ ¼å¼å¹¶åŒ…å«ç»çº¬åº¦ä¿¡æ¯
            if not isinstance(features, pd.DataFrame):
                print(f"è­¦å‘Š: {res} çš„featuresä¸æ˜¯DataFrameæ ¼å¼")
                continue
            
            # æ£€æŸ¥å¿…è¦çš„åæ ‡åˆ—
            required_coords = ['latitude', 'longitude']
            missing_coords = [col for col in required_coords if col not in features.columns]
            if missing_coords:
                print(f"è­¦å‘Š: {res} ç¼ºå°‘åæ ‡åˆ—: {missing_coords}")
                continue
            
            # æ„å»ºåæ ‡DataFrame
            coords_df = features[['latitude', 'longitude']].copy()
            if 'h3_index' in features.columns:
                coords_df['h3_index'] = features['h3_index']
            
            # ç¡®ä¿shap_valuesä¸featuresçš„è¡Œæ•°åŒ¹é…
            if len(shap_values) != len(features):
                print(f"è­¦å‘Š: {res} SHAPå€¼æ•°é‡({len(shap_values)})ä¸ç‰¹å¾æ•°é‡({len(features)})ä¸åŒ¹é…")
                min_len = min(len(shap_values), len(features))
                shap_values = shap_values[:min_len]
                coords_df = coords_df.iloc[:min_len]
                # ğŸ”¥ ä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥targeté•¿åº¦ï¼Œé¿å…å¸ƒå°”æ•°ç»„é”™è¯¯
                if target is not None:
                    try:
                        target_len = len(target) if hasattr(target, '__len__') else 1
                        if target_len > min_len:
                            # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†NumPyæ•°ç»„å’Œæ ‡é‡å€¼
                            if hasattr(target, '__getitem__') and not isinstance(target, (str, dict)):
                                target = target[:min_len]
                            # å¦‚æœtargetæ˜¯æ ‡é‡ï¼Œä¿æŒä¸å˜
                    except (TypeError, ValueError):
                        # å¦‚æœæ— æ³•è·å–é•¿åº¦ï¼Œå‡è®¾æ˜¯æ ‡é‡ï¼Œä¿æŒä¸å˜
                        pass
            
            # è·å–ç‰¹å¾é‡è¦æ€§æ¥ç¡®å®štopç‰¹å¾
            if 'feature_importance' in results:
                feature_importance = results['feature_importance']
                if isinstance(feature_importance, dict):
                    feature_importance = [(k, v) for k, v in feature_importance.items()]
                
                # ğŸ”¥ ä¿®å¤ç‰¹å¾é€‰æ‹©é€»è¾‘ï¼šä¸¥æ ¼ç­›é€‰ä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾
                primary_effects = []
                
                # å®šä¹‰ç¡®å®šçš„11ä¸ªä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾åˆ—è¡¨ï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼ˆç”¨æˆ·ç¡®è®¤çš„æ­£ç¡®ç»„æˆï¼‰
                environmental_features = {
                    'temperature', 'precipitation',  # 2ä¸ªæ°”å€™ç‰¹å¾
                    'nightlight', 'road_density', 'mining_density', 'population_density',  # 4ä¸ªäººç±»æ´»åŠ¨
                    'elevation', 'slope',  # 2ä¸ªåœ°å½¢ç‰¹å¾
                    'forest_area_percent', 'cropland_area_percent', 'impervious_area_percent'  # 3ä¸ªåœŸåœ°è¦†ç›–
                }
                # æ€»è®¡11ä¸ªä¸»æ•ˆåº”ç‰¹å¾
                
                for feat, imp in feature_importance:
                    if isinstance(feat, tuple):
                        feat_name = feat[0]
                    else:
                        feat_name = feat
                    
                    # ğŸ¯ ä¸¥æ ¼ç­›é€‰ï¼šåªä¿ç•™ç¡®å®šçš„ç¯å¢ƒç‰¹å¾
                    if feat_name.lower() in {f.lower() for f in environmental_features}:
                        primary_effects.append((feat_name, imp))
                        print(f"    âœ“ é€‰æ‹©ç¯å¢ƒç‰¹å¾: {feat_name} (é‡è¦æ€§: {imp:.4f})")
                    else:
                        print(f"    âœ— æ’é™¤éç¯å¢ƒç‰¹å¾: {feat_name}")
                
                print(f"    ğŸ“Š å…±é€‰æ‹© {len(primary_effects)} ä¸ªä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾")
                
                # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©top_n
                primary_effects.sort(key=lambda x: x[1], reverse=True)
                if top_n:
                    top_features = [f for f, _ in primary_effects[:top_n]]
                else:
                    top_features = [f for f, _ in primary_effects]
                    
                print(f"    {res}: é€‰æ‹©ä¸»æ•ˆåº”ç¯å¢ƒç‰¹å¾ {', '.join(top_features[:3])}")
            else:
                # å¦‚æœæ²¡æœ‰ç‰¹å¾é‡è¦æ€§ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨ç‰¹å¾
                if 'enhanced_shap_values_by_feature' in results:
                    top_features = list(shap_values_by_feature.keys())
                else:
                    feature_cols = [col for col in features.columns 
                                  if col not in ['latitude', 'longitude', 'h3_index']]
                    top_features = feature_cols[:top_n] if top_n else feature_cols
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            processed[res] = {
                'shap_features': shap_values,
                'coords_df': coords_df, 
                'top_features': top_features,
                'target_values': target,
                'data_source': data_source_info if 'enhanced_shap_values_by_feature' in results else "Sampled Data"
            }
            
            print(f"  âœ“ {res}: æˆåŠŸé¢„å¤„ç†ï¼ŒSHAPç‰¹å¾ç»´åº¦={shap_values.shape}, åæ ‡æ•°={len(coords_df)}")
            
        except Exception as e:
            print(f"  âœ— {res}: é¢„å¤„ç†å¤±è´¥: {e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            continue
    
    if processed:
        # ğŸ”¥ ä¿®å¤ï¼šè¯¦ç»†åˆ†ææ•°æ®æºç±»å‹å¹¶æ­£ç¡®æŠ¥å‘Š
        enhanced_count = sum(1 for data in processed.values() 
                           if data.get('data_source') == "Full Grid Interpolated")
        total_count = len(processed)
        
        print(f"\nğŸ“Š èšç±»é¢„å¤„ç†æ•°æ®æºåˆ†æ:")
        for res, data in processed.items():
            data_source = data.get('data_source', 'æœªçŸ¥')
            coords_count = len(data['coords_df'])
            print(f"  {res}: {data_source} ({coords_count}ä¸ªç½‘æ ¼)")
        
        if enhanced_count > 0:
            print(f"  âœ… èšç±»é¢„å¤„ç†å®Œæˆï¼š{enhanced_count}/{total_count}ä¸ªåˆ†è¾¨ç‡ä½¿ç”¨äº†å®Œæ•´ç½‘æ ¼æ’å€¼æ•°æ®")
        else:
            print(f"  âš ï¸ èšç±»é¢„å¤„ç†å®Œæˆï¼š{total_count}ä¸ªåˆ†è¾¨ç‡ä½¿ç”¨åŸå§‹é‡‡æ ·æ•°æ®")
    
    return processed

__all__ = ['preprocess_data_for_clustering'] 