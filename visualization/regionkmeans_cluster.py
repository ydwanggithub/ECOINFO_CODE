#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŒºåŸŸKMeansèšç±»ç®—æ³•å®ç°
"""

# è®¾ç½®OMP_NUM_THREADSç¯å¢ƒå˜é‡ï¼Œé¿å…Windowsä¸ŠKMeanså†…å­˜æ³„æ¼é—®é¢˜
# è¿™å¿…é¡»åœ¨å¯¼å…¥sklearnä¹‹å‰è¿›è¡Œè®¾ç½®
import os
import sys
import platform
if platform.system() == 'Windows' and 'OMP_NUM_THREADS' not in os.environ:
    os.environ['OMP_NUM_THREADS'] = '1'
    print(f"åœ¨regionkmeans_cluster.pyä¸­è®¾ç½®OMP_NUM_THREADS=1ï¼Œé¿å…Windowsä¸ŠKMeanså†…å­˜æ³„æ¼é—®é¢˜")

import numpy as np
import pandas as pd
import libpysal
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import connected_components
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering, KMeans
import time
import threading
import logging

# å°è¯•å¯¼å…¥h3åº“ï¼Œæ”¯æŒå¤šç§ç‰ˆæœ¬
try:
    import h3
    H3_AVAILABLE = True
    print("clusteræ¨¡å—: æˆåŠŸå¯¼å…¥h3åº“")
except ImportError:
    try:
        # å°è¯•ä½¿ç”¨h3ronpyä½œä¸ºæ›¿ä»£
        from h3ronpy import h3
        H3_AVAILABLE = True
        print("clusteræ¨¡å—: ä½¿ç”¨h3ronpyä½œä¸ºh3åº“æ›¿ä»£")
    except ImportError:
        H3_AVAILABLE = False
        print("clusteræ¨¡å—: æœªèƒ½å¯¼å…¥h3åº“ï¼Œéƒ¨åˆ†H3åŠŸèƒ½å°†ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ç»™ PySAL W ç±»æ·»åŠ  to_adjlist æ–¹æ³•ï¼Œç¡®ä¿ RegionKMeans æ­£å¸¸è°ƒç”¨
if not hasattr(libpysal.weights.W, 'to_adjlist'):
    def w_to_adjlist(self):
        # ä»ç¨€ç–ˆçŸ©é˜µè·å–é‚»æ¥å¯¹
        rows, cols = self.sparse.nonzero()
        return pd.DataFrame({'focal': rows, 'neighbor': cols})
    libpysal.weights.W.to_adjlist = w_to_adjlist


def _solve_with_timeout(model, timeout_seconds=300):
    """
    åœ¨æŒ‡å®šçš„è¶…æ—¶æ—¶é—´å†…æ‰§è¡Œèšç±»æ±‚è§£ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å®ç°è¶…æ—¶æ£€æµ‹
    è¿”å›: (æˆåŠŸæ ‡å¿—, é”™è¯¯ä¿¡æ¯)
    """
    result = {"success": False, "error": None, "completed": False}
    
    def solve_func():
        try:
            model.solve()
            result["success"] = True
        except Exception as e:
            result["error"] = str(e)
        finally:
            result["completed"] = True
    
    thread = threading.Thread(target=solve_func)
    thread.daemon = True
    thread.start()
    
    start_time = time.time()
    print(f"RegionKMeans æ±‚è§£å¼€å§‹ (è¶…æ—¶ {timeout_seconds}s)")
    
    while not result["completed"] and (time.time() - start_time) < timeout_seconds:
        time.sleep(1.0)  # ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶ï¼Œå¿½ç•¥ä¸­é—´æ—¥å¿—
    
    if not result["completed"]:
        print("âœ— RegionKMeans æ±‚è§£è¶…æ—¶")
        return (False, result.get("error") or "è¶…æ—¶")
    
    if result["success"]:
        print("âœ“ RegionKMeans æ±‚è§£å®Œæˆ")
    else:
        print(f"âœ— RegionKMeans æ±‚è§£å¤±è´¥: {result.get('error')}")
    return (result["success"], result.get("error"))


def create_optimized_spatial_weights(coords_array, k, h3_indices=None):
    """
    åˆ›å»ºä¼˜åŒ–çš„ç©ºé—´æƒé‡çŸ©é˜µï¼Œæ”¯æŒH3 grid_diskæˆ–KNN
    
    å‚æ•°:
    - coords_array: åæ ‡æ•°ç»„
    - k: KNNé‚»å±…æ•°é‡æˆ–H3 grid_diskåŠå¾„
    - h3_indices: å¯é€‰çš„H3ç´¢å¼•ï¼Œç”¨äºåŸºäºH3çš„é‚»æ¥å…³ç³»
    
    è¿”å›:
    - w: libpysalç©ºé—´æƒé‡å¯¹è±¡ï¼ˆè¡Œæ ‡å‡†åŒ–ï¼‰
    """
    import warnings
    import sys
    from io import StringIO
    
    # æŠ‘åˆ¶libpysalçš„å­¤ç«‹ç‚¹è­¦å‘Šä¿¡æ¯
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    
    try:
        # é‡å®šå‘æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºï¼ŒæŠ‘åˆ¶å­¤ç«‹ç‚¹è­¦å‘Š
        sys.stdout = StringIO()
        sys.stderr = StringIO()
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            data_size = len(coords_array)
            if (h3_indices is not None) and (len(h3_indices) == data_size) and H3_AVAILABLE:
                try:
                    # ç¡®è®¤h3åº“ä¸­å­˜åœ¨grid_diskæ–¹æ³•
                    if hasattr(h3, 'grid_disk') or hasattr(h3, 'k_ring'):
                        grid_disk_func = None
                        if hasattr(h3, 'grid_disk'):
                            grid_disk_func = lambda idx, k: h3.grid_disk(idx, k)
                        elif hasattr(h3, 'k_ring'):
                            grid_disk_func = lambda idx, k: h3.k_ring(idx, k)
                        
                        if grid_disk_func:
                            n = len(h3_indices)
                            adj_matrix = np.zeros((n, n), dtype=int)
                            for i, h3_idx in enumerate(h3_indices):
                                try:
                                    neighbors = grid_disk_func(h3_idx, k)
                                    for neighbor in neighbors:
                                        neighbor_indices = np.where(h3_indices == neighbor)[0]
                                        if len(neighbor_indices) > 0:
                                            for j in neighbor_indices:
                                                adj_matrix[i, j] = 1
                                except:
                                    pass  # é™é»˜å¤„ç†å¤±è´¥
                            
                            knn_graph = csr_matrix(adj_matrix)
                        else:
                            n_neighbors = min(max(2, min(k, data_size-1)), data_size-1)
                            knn_graph = kneighbors_graph(coords_array, n_neighbors=n_neighbors, mode='connectivity', n_jobs=-1)
                    else:
                        n_neighbors = min(max(2, min(k, data_size-1)), data_size-1)
                        knn_graph = kneighbors_graph(coords_array, n_neighbors=n_neighbors, mode='connectivity', n_jobs=-1)
                except:
                    n_neighbors = min(max(2, min(k, data_size-1)), data_size-1)
                    knn_graph = kneighbors_graph(coords_array, n_neighbors=n_neighbors, mode='connectivity', n_jobs=-1)
            else:
                # ä½¿ç”¨KNNæ„å»ºé‚»æ¥å…³ç³»
                n_neighbors = min(max(2, min(k, data_size-1)), data_size-1)
                knn_graph = kneighbors_graph(coords_array, n_neighbors=n_neighbors, mode='connectivity', n_jobs=-1)

            # æ„å»ºé‚»å±…å­—å…¸å’Œæƒé‡å­—å…¸
            try:
                neighbors = {i: list(knn_graph.indices[knn_graph.indptr[i]:knn_graph.indptr[i+1]]) 
                             for i in range(knn_graph.shape[0])}
                weights = {i: {j: 1.0 for j in neighbors[i]} for i in range(knn_graph.shape[0])}

                w = libpysal.weights.W(weights)
                w.transform = 'r'
            except:
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„é‚»æ¥çŸ©é˜µä½œä¸ºå¤‡é€‰
                n = data_size
                simple_neighbors = {i: [j for j in range(n) if j != i and np.random.random() < 3.0/n] for i in range(n)}
                for i in range(n):
                    if len(simple_neighbors[i]) == 0:
                        j = (i + 1) % n
                        simple_neighbors[i].append(j)
                        simple_neighbors[j].append(i)
                simple_weights = {i: {j: 1.0 for j in simple_neighbors[i]} for i in range(n)}
                w = libpysal.weights.W(simple_weights)
                w.transform = 'r'

            # å¤„ç†å­¤ç«‹ç‚¹ï¼ˆé™é»˜å¤„ç†ï¼‰
            if hasattr(w, 'islands') and w.islands:
                for island in w.islands:
                    try:
                        distances = []
                        for j in range(len(coords_array)):
                            if j != island:
                                dist = np.sum((coords_array[island] - coords_array[j])**2)
                                distances.append((j, dist))
                        distances.sort(key=lambda x: x[1])
                        nearest = distances[:2]
                        neighbors[island] = [n[0] for n in nearest]
                        weights[island] = {n[0]: 1.0 for n in nearest}
                        for n in nearest:
                            if n[0] not in neighbors:
                                neighbors[n[0]] = [island]
                                weights[n[0]] = {island: 1.0}
                            else:
                                neighbors[n[0]].append(island)
                                weights[n[0]][island] = 1.0
                    except:
                        pass  # é™é»˜å¤„ç†å¤±è´¥
                
                try:
                    w = libpysal.weights.W(weights)
                    w.transform = 'r'
                except:
                    pass  # é™é»˜å¤„ç†å¤±è´¥

            # æ£€æŸ¥è¿é€šæ€§ï¼ˆé™é»˜ï¼‰
            try:
                n_components, _ = connected_components(w.sparse, directed=False)
                if n_components > 1:
                    # è‡ªé€‚åº”é‚»å±…æ•°
                    if data_size < 500:
                        k_used = min(10, data_size - 1)
                    else:
                        k_used = min(6, data_size - 1)
                    try:
                        knn_graph_new = kneighbors_graph(coords_array, n_neighbors=k_used, mode='connectivity', n_jobs=-1)
                        neighbors_new = {i: list(knn_graph_new.indices[knn_graph_new.indptr[i]:knn_graph_new.indptr[i+1]])
                                        for i in range(knn_graph_new.shape[0])}
                        weights_new = {i: {j: 1.0 for j in neighbors_new[i]} for i in range(knn_graph_new.shape[0])}
                        w = libpysal.weights.W(weights_new)
                        w.transform = 'r'
                    except:
                        pass  # é™é»˜å¤„ç†å¤±è´¥
            except:
                pass  # é™é»˜å¤„ç†å¤±è´¥

            # Monkey-patch to_adjlist æ–¹æ³•
            if not hasattr(w, 'to_adjlist'):
                def to_adjlist():
                    adj = []
                    for i, neis in w.neighbors.items():
                        for j in neis:
                            adj.append((i, j))
                    return pd.DataFrame(adj, columns=['focal', 'neighbor'])
                w.to_adjlist = to_adjlist

    finally:
        # æ¢å¤æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
        sys.stdout = old_stdout
        sys.stderr = old_stderr

    print("âœ“ ç©ºé—´æƒé‡çŸ©é˜µæ„å»ºå®Œæˆ")
    return w


def create_subgraph(w, indices):
    """
    æ‰‹åŠ¨åˆ›å»ºç©ºé—´æƒé‡çŸ©é˜µçš„å­å›¾
    
    å‚æ•°:
    - w: åŸå§‹ç©ºé—´æƒé‡çŸ©é˜µ (libpysal.weights.Wå¯¹è±¡)
    - indices: å­å›¾åŒ…å«çš„ç‚¹çš„ç´¢å¼•
    
    è¿”å›:
    - subw: å­å›¾çš„ç©ºé—´æƒé‡çŸ©é˜µ
    """
    # åˆ›å»ºç´¢å¼•æ˜ å°„ï¼Œä»åŸå§‹ç´¢å¼•åˆ°å­å›¾ç´¢å¼•
    idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(indices)}
    
    # åˆ›å»ºæ–°çš„é‚»å±…å­—å…¸å’Œæƒé‡å­—å…¸
    new_neighbors = {}
    new_weights = {}
    
    for i, idx in enumerate(indices):
        # è·å–å½“å‰ç‚¹çš„é‚»å±…
        neighbors_in_subgraph = []
        weights_in_subgraph = {}
        
        for neighbor in w.neighbors[idx]:
            # åªä¿ç•™ä¹Ÿåœ¨å­å›¾ä¸­çš„é‚»å±…
            if neighbor in idx_map:
                neighbors_in_subgraph.append(idx_map[neighbor])
                weights_in_subgraph[idx_map[neighbor]] = w.weights[idx][neighbor]
        
        new_neighbors[i] = neighbors_in_subgraph
        new_weights[i] = weights_in_subgraph
    
    # åˆ›å»ºæ–°çš„Wå¯¹è±¡
    subw = libpysal.weights.W(new_weights, id_order=range(len(indices)))
    
    # å¦‚æœåŸå§‹Wæœ‰sparseå±æ€§ï¼Œä¸ºå­å›¾ä¹Ÿåˆ›å»ºsparse
    if hasattr(w, 'sparse') and hasattr(w.sparse, 'toarray'):
        import scipy.sparse as sp
        n = len(indices)
        data = []
        row = []
        col = []
        
        for i in range(n):
            for j in new_neighbors[i]:
                row.append(i)
                col.append(j)
                data.append(new_weights[i][j])
        
        subw.sparse = sp.csr_matrix((data, (row, col)), shape=(n, n))
    
    return subw


def spatial_constrained_postprocessing(initial_labels, w, data, target_clusters):
    """
    å¯¹æ ‡å‡†KMeansç»“æœè¿›è¡Œç©ºé—´åå¤„ç†ï¼Œåˆå¹¶ç©ºé—´ç›¸é‚»ä¸”å±æ€§ç›¸ä¼¼çš„èšç±»
    
    å‚æ•°:
    - initial_labels: åˆå§‹èšç±»æ ‡ç­¾
    - w: ç©ºé—´æƒé‡çŸ©é˜µ
    - data: æ•°æ®çŸ©é˜µ
    - target_clusters: ç›®æ ‡èšç±»æ•°é‡
    
    è¿”å›:
    - final_labels: å¤„ç†åçš„èšç±»æ ‡ç­¾
    """
    unique_labels = np.unique(initial_labels)
    if len(unique_labels) <= target_clusters:
        return initial_labels
    
    centers = {}
    for label in unique_labels:
        mask = (initial_labels == label)
        if np.any(mask):
            centers[label] = np.mean(data[mask], axis=0)
    
    cluster_adjacency = {}
    for i in range(len(initial_labels)):
        label_i = initial_labels[i]
        neighbors = w.neighbors[i]
        for j in neighbors:
            if j < len(initial_labels):
                label_j = initial_labels[j]
                if label_i != label_j:
                    if label_i not in cluster_adjacency:
                        cluster_adjacency[label_i] = set()
                    cluster_adjacency[label_i].add(label_j)
    
    for label in cluster_adjacency:
        cluster_adjacency[label] = list(cluster_adjacency[label])
    
    current_labels = initial_labels.copy()
    current_unique = np.unique(current_labels)
    max_iterations = 100
    iteration = 0
    
    while len(current_unique) > target_clusters and iteration < max_iterations:
        iteration += 1
        # æ›´å¤æ‚çš„åˆå¹¶ç­–ç•¥ï¼ŒåŒæ—¶è€ƒè™‘ç‰¹å¾è·ç¦»å’Œè¿é€šæ€§
        best_pair = None
        min_distance = float('inf')
        
        for label_i in current_unique:
            if label_i in cluster_adjacency:
                neighbors = [n for n in cluster_adjacency[label_i] if n in current_unique]
                # å¦‚æœé‚»å±…åˆ—è¡¨ä¸ºç©ºä½†ä»éœ€åˆå¹¶
                if not neighbors and len(current_unique) > target_clusters:
                    # æ‰¾åˆ°ç‰¹å¾ç©ºé—´æœ€è¿‘çš„ç°‡
                    min_feat_dist = float('inf')
                    closest_label = None
                    for label_j in current_unique:
                        if label_i != label_j and label_i in centers and label_j in centers:
                            dist = np.linalg.norm(centers[label_i] - centers[label_j])
                            if dist < min_feat_dist:
                                min_feat_dist = dist
                                closest_label = label_j
                    if closest_label is not None:
                        # æ·»åŠ ä¸ºè™šæ‹Ÿé‚»å±…
                        neighbors.append(closest_label)
                        print(f"æ·»åŠ ç‰¹å¾ç©ºé—´æœ€è¿‘çš„ç°‡ {closest_label} ä½œä¸ºç°‡ {label_i} çš„è™šæ‹Ÿé‚»å±…")

                for label_j in neighbors:
                    if label_i in centers and label_j in centers:
                        # å¼•å…¥ç©ºé—´è¿‘ä¼¼åº¦åŠ æƒçš„ç‰¹å¾è·ç¦»åº¦é‡
                        feat_dist = np.linalg.norm(centers[label_i] - centers[label_j])
                        
                        # ä¼°è®¡ç©ºé—´è¿‘ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        space_weight = 1.0  # é»˜è®¤æƒé‡
                        # å¯¹äºå°ç°‡èµ‹äºˆæ›´é«˜æƒé‡ï¼Œé¿å…å­¤ç«‹ç‚¹æˆä¸ºç‹¬ç«‹ç°‡
                        mask_i = (current_labels == label_i)
                        mask_j = (current_labels == label_j)
                        size_i = np.sum(mask_i)
                        size_j = np.sum(mask_j)
                        # å°ç°‡çš„åˆå¹¶ä¼˜å…ˆçº§æ›´é«˜
                        if size_i < 5 or size_j < 5:
                            space_weight = 0.7  # æé«˜åˆå¹¶æ¦‚ç‡
                        
                        # è®¡ç®—åŠ æƒè·ç¦»
                        dist = feat_dist * space_weight
                        
                        if dist < min_distance:
                            min_distance = dist
                            best_pair = (label_i, label_j)
        
        if best_pair is None:
            if len(current_unique) >= 2:
                updated_adjacency = False
                for i, label_i in enumerate(current_unique):
                    for j, label_j in enumerate(current_unique):
                        if i != j:
                            mask_i = (current_labels == label_i)
                            mask_j = (current_labels == label_j)
                            indices_i = np.where(mask_i)[0]
                            indices_j = np.where(mask_j)[0]
                            if len(indices_i) > 0 and len(indices_j) > 0:
                                max_checks = min(100, len(indices_i) * len(indices_j))
                                check_count = 0
                                for idx_i in indices_i:
                                    for idx_j in indices_j:
                                        check_count += 1
                                        if check_count > max_checks or updated_adjacency:
                                            break
                                    if check_count > max_checks or updated_adjacency:
                                        break
                if updated_adjacency:
                    continue
                min_feature_dist = float('inf')
                closest_pair = None
                for i, label_i in enumerate(current_unique):
                    for j, label_j in enumerate(current_unique):
                        if i < j and label_i in centers and label_j in centers:
                            dist = np.linalg.norm(centers[label_i] - centers[label_j])
                            if dist < min_feature_dist:
                                min_feature_dist = dist
                                closest_pair = (label_i, label_j)
                if closest_pair:
                    best_pair = closest_pair
                else:
                    random_indices = np.random.choice(len(current_unique), 2, replace=False)
                    best_pair = (current_unique[random_indices[0]], current_unique[random_indices[1]])
            else:
                break
        
        label_keep, label_merge = best_pair
        mask_merge = (current_labels == label_merge)
        current_labels[mask_merge] = label_keep
        
        if label_keep in centers and label_merge in centers:
            mask_keep = (current_labels == label_keep)
            centers[label_keep] = np.mean(data[mask_keep], axis=0)
            if label_merge in centers:
                del centers[label_merge]
        
        current_unique = np.unique(current_labels)
    
    if len(current_unique) > target_clusters:
        print(f"è­¦å‘Š: ç©ºé—´åå¤„ç†æœªèƒ½è¾¾åˆ°ç›®æ ‡èšç±»æ•° {target_clusters}ï¼Œå½“å‰èšç±»æ•° {len(current_unique)}")
    
    final_labels = np.zeros_like(current_labels)
    for i, label in enumerate(np.unique(current_labels)):
        final_labels[current_labels == label] = i
    
    return final_labels


def _perform_clustering(shap_features, n_clusters=3):
    """
    å¯¹SHAPç‰¹å¾æ‰§è¡Œèšç±»å¹¶è®¡ç®—ç›¸å…³ç»Ÿè®¡ä¿¡æ¯
    
    å‚æ•°:
    - shap_features: SHAPç‰¹å¾DataFrame
    - n_clusters: èšç±»æ•°é‡
    
    è¿”å›:
    - clusters: èšç±»æ ‡ç­¾
    - centers: èšç±»ä¸­å¿ƒ
    - standardized_features: æ ‡å‡†åŒ–åçš„ç‰¹å¾
    """
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    standardized_features = scaler.fit_transform(shap_features)
    
    # æ‰§è¡ŒK-meansèšç±»ï¼Œè°ƒæ•´å‚æ•°ä»¥æé«˜èšç±»ç¨³å®šæ€§
    max_iter = 1000  # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
    n_init = 30     # å¢åŠ å¯åŠ¨æ¬¡æ•°
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=n_init, max_iter=max_iter)
    clusters = kmeans.fit_predict(standardized_features)
    centers = kmeans.cluster_centers_
    
    # è®¡ç®—æ¯ä¸ªèšç±»çš„SHAPæ€»å½±å“
    abs_shap_sum = np.abs(shap_features).sum(axis=1)
    cluster_impacts = []
    
    for cluster in range(n_clusters):
        mask = (clusters == cluster)
        if np.any(mask):  # ç¡®ä¿æœ‰ç‚¹å±äºè¿™ä¸ªèšç±»
            impact = np.mean(abs_shap_sum[mask])
            cluster_impacts.append((cluster, impact))
    
    # æŒ‰å½±å“åŠ›æ’åºï¼ˆé«˜->ä½ï¼‰
    cluster_impacts.sort(key=lambda x: x[1], reverse=True)
    
    # åˆ›å»ºé‡æ˜ å°„ï¼šé«˜å½±å“->0, ä¸­ç­‰å½±å“->1, ä½å½±å“->2
    remapping = {old_idx: new_idx for new_idx, (old_idx, _) in enumerate(cluster_impacts)}
    
    # åº”ç”¨é‡æ˜ å°„
    remapped_clusters = np.array([remapping[c] for c in clusters])
    
    return remapped_clusters, centers, standardized_features


def force_large_continuous_regions(labels, coords, w, X, target_clusters):
    """
    å¼ºåˆ¶å½¢æˆå¤§å—è¿ç»­çš„èšç±»åŒºåŸŸï¼Œè¿›ä¸€æ­¥åˆå¹¶å°èšç±»å¹¶ä¼˜åŒ–ç©ºé—´è¿ç»­æ€§
    
    å‚æ•°:
    - labels: èšç±»æ ‡ç­¾
    - coords: åæ ‡æ•°ç»„
    - w: ç©ºé—´æƒé‡çŸ©é˜µ
    - X: ç‰¹å¾çŸ©é˜µ
    - target_clusters: ç›®æ ‡èšç±»æ•°é‡
    
    è¿”å›:
    - enhanced_labels: å¼ºåŒ–åçš„èšç±»æ ‡ç­¾
    """
    print("  ğŸ”§ å¼ºåˆ¶å½¢æˆå¤§å—è¿ç»­åŒºåŸŸ...")
    
    enhanced_labels = labels.copy()
    unique_labels, counts = np.unique(enhanced_labels, return_counts=True)
    
    # è®¡ç®—æ¯ä¸ªèšç±»çš„ç‰¹å¾ä¸­å¿ƒ
    cluster_centers = {}
    for label in unique_labels:
        mask = (enhanced_labels == label)
        if np.any(mask):
            cluster_centers[label] = np.mean(X[mask], axis=0)
    
    # ğŸ”¥ é€‚åº¦çš„æœ€å°èšç±»å¤§å°ï¼šæ¯ä¸ªèšç±»å åˆç†æ¯”ä¾‹
    min_region_size = len(enhanced_labels) // (target_clusters * 5)  # æ¯ä¸ªèšç±»è‡³å°‘å 1/(5*n_clusters)
    print(f"    é€‚åº¦æœ€å°åŒºåŸŸå¤§å°: {min_region_size}")
    
    # è¿­ä»£åˆå¹¶å°èšç±»åˆ°æœ€è¿‘çš„å¤§èšç±»
    max_iterations = 50
    iteration = 0
    
    while iteration < max_iterations:
        iteration += 1
        unique_labels, counts = np.unique(enhanced_labels, return_counts=True)
        
        # æ‰¾åˆ°æœ€å°çš„èšç±»
        small_clusters = [label for label, count in zip(unique_labels, counts) if count < min_region_size]
        
        if not small_clusters:
            break
            
        print(f"    è¿­ä»£{iteration}: å‘ç°{len(small_clusters)}ä¸ªå°èšç±»éœ€è¦åˆå¹¶")
        
        # é€ä¸ªåˆå¹¶æœ€å°çš„èšç±»
        for small_label in small_clusters:
            small_mask = (enhanced_labels == small_label)
            small_indices = np.where(small_mask)[0]
            
            if len(small_indices) == 0:
                continue
                
            # æ‰¾åˆ°è¿™ä¸ªå°èšç±»çš„ç©ºé—´é‚»å±…
            neighbor_labels = set()
            for idx in small_indices:
                if idx in w.neighbors:
                    for neighbor_idx in w.neighbors[idx]:
                        if neighbor_idx < len(enhanced_labels):
                            neighbor_label = enhanced_labels[neighbor_idx]
                            if neighbor_label != small_label:
                                neighbor_labels.add(neighbor_label)
            
            # åœ¨é‚»å±…ä¸­æ‰¾åˆ°æœ€å¤§çš„èšç±»
            best_neighbor = None
            max_neighbor_size = 0
            min_feature_distance = float('inf')
            
            for neighbor_label in neighbor_labels:
                neighbor_size = np.sum(enhanced_labels == neighbor_label)
                if neighbor_size > max_neighbor_size:
                    # è¿˜è¦æ£€æŸ¥ç‰¹å¾ç›¸ä¼¼æ€§
                    if (small_label in cluster_centers and 
                        neighbor_label in cluster_centers):
                        feature_dist = np.linalg.norm(
                            cluster_centers[small_label] - cluster_centers[neighbor_label]
                        )
                        if neighbor_size > max_neighbor_size or feature_dist < min_feature_distance:
                            max_neighbor_size = neighbor_size
                            min_feature_distance = feature_dist
                            best_neighbor = neighbor_label
            
            # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„é‚»å±…ï¼Œæ‰¾ç‰¹å¾æœ€ç›¸ä¼¼çš„èšç±»
            if best_neighbor is None and small_label in cluster_centers:
                min_dist = float('inf')
                for other_label in unique_labels:
                    if (other_label != small_label and 
                        other_label in cluster_centers and
                        np.sum(enhanced_labels == other_label) >= min_region_size):
                        dist = np.linalg.norm(
                            cluster_centers[small_label] - cluster_centers[other_label]
                        )
                        if dist < min_dist:
                            min_dist = dist
                            best_neighbor = other_label
            
            # æ‰§è¡Œåˆå¹¶
            if best_neighbor is not None:
                enhanced_labels[small_mask] = best_neighbor
                print(f"      åˆå¹¶èšç±»{small_label}({np.sum(small_mask)}ä¸ªç‚¹) â†’ èšç±»{best_neighbor}")
                
                # æ›´æ–°èšç±»ä¸­å¿ƒ
                if best_neighbor in cluster_centers:
                    new_mask = (enhanced_labels == best_neighbor)
                    cluster_centers[best_neighbor] = np.mean(X[new_mask], axis=0)
                
                # åˆ é™¤è¢«åˆå¹¶çš„èšç±»ä¸­å¿ƒ
                if small_label in cluster_centers:
                    del cluster_centers[small_label]
    
    # é‡æ–°ç¼–å·ä»¥ç¡®ä¿è¿ç»­æ€§
    final_unique = np.unique(enhanced_labels)
    label_mapping = {old: new for new, old in enumerate(final_unique)}
    final_labels = np.array([label_mapping[label] for label in enhanced_labels])
    
    print(f"    âœ… å¤§å—åŒºåŸŸå¼ºåŒ–å®Œæˆï¼Œæœ€ç»ˆèšç±»æ•°: {len(np.unique(final_labels))}")
    print(f"    æœ€ç»ˆèšç±»å¤§å°åˆ†å¸ƒ: {dict(zip(*np.unique(final_labels, return_counts=True)))}")
    
    return final_labels


def fix_spatial_discontinuity(labels, coords, w, X):
    """
    ä¿®å¤ç©ºé—´èšç±»ä¸­çš„ä¸è¿ç»­æ€§é—®é¢˜ï¼Œç¡®ä¿èšç±»ç»“æœå…·æœ‰æ›´å¥½çš„ç©ºé—´è¿ç»­æ€§
    
    å‚æ•°:
    - labels: åŸå§‹èšç±»æ ‡ç­¾
    - coords: åæ ‡æ•°ç»„
    - w: ç©ºé—´æƒé‡çŸ©é˜µ
    - X: ç‰¹å¾çŸ©é˜µ
    
    è¿”å›:
    - fixed_labels: ä¿®å¤åçš„èšç±»æ ‡ç­¾
    """
    print("  ğŸ”§ ä¿®å¤ç©ºé—´ä¸è¿ç»­æ€§...")
    
    fixed_labels = labels.copy()
    n_clusters = len(np.unique(labels))
    
    # æ£€æµ‹å¹¶ä¿®å¤å­¤ç«‹ç‚¹
    isolated_count = 0
    for i in range(len(labels)):
        current_label = labels[i]
        neighbors = w.neighbors[i] if i in w.neighbors else []
        
        # è®¡ç®—é‚»å±…çš„æ ‡ç­¾åˆ†å¸ƒ
        neighbor_labels = [labels[j] for j in neighbors if j < len(labels)]
        
        if neighbor_labels:
            # å¦‚æœå½“å‰ç‚¹çš„æ ‡ç­¾ä¸æ‰€æœ‰é‚»å±…éƒ½ä¸åŒï¼Œåˆ™ä¸ºå­¤ç«‹ç‚¹
            if current_label not in neighbor_labels:
                # æ‰¾åˆ°é‚»å±…ä¸­æœ€é¢‘ç¹çš„æ ‡ç­¾
                from collections import Counter
                most_common_label = Counter(neighbor_labels).most_common(1)[0][0]
                
                # è®¡ç®—ä¸æœ€å¸¸è§é‚»å±…æ ‡ç­¾çš„ç‰¹å¾ç›¸ä¼¼æ€§
                same_label_indices = [j for j in range(len(labels)) if labels[j] == most_common_label]
                if same_label_indices:
                    # è®¡ç®—ç‰¹å¾è·ç¦»
                    point_feature = X[i]
                    cluster_center = np.mean(X[same_label_indices], axis=0)
                    feature_distance = np.linalg.norm(point_feature - cluster_center)
                    
                    # å¦‚æœç‰¹å¾è·ç¦»ä¸å¤ªå¤§ï¼Œåˆ™é‡æ–°åˆ†é…æ ‡ç­¾
                    threshold = np.std(X) * 2  # 2å€æ ‡å‡†å·®ä½œä¸ºé˜ˆå€¼
                    if feature_distance < threshold:
                        fixed_labels[i] = most_common_label
                        isolated_count += 1
    
    # ğŸ”¥ ä»…å¤„ç†çœŸæ­£çš„å­¤ç«‹ç‚¹ï¼ˆ1-2ä¸ªç‚¹çš„èšç±»ï¼‰ï¼Œä¿æŒSHAPå€¼åˆ†å¸ƒçš„è‡ªç„¶æ€§
    unique_labels, counts = np.unique(fixed_labels, return_counts=True)
    # åªåˆå¹¶çœŸæ­£çš„å­¤ç«‹ç‚¹
    min_cluster_size = 3  # åªåˆå¹¶å°äº3ä¸ªç‚¹çš„èšç±»
    print(f"  ä»…å¤„ç†çœŸæ­£çš„å­¤ç«‹ç‚¹ï¼Œæœ€å°å¤§å°: {min_cluster_size}")
    print(f"  å½“å‰èšç±»å¤§å°åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")
    
    for label, count in zip(unique_labels, counts):
        if count < min_cluster_size:
            # æ‰¾åˆ°ç‰¹å¾æœ€ç›¸ä¼¼çš„èšç±»è¿›è¡Œåˆå¹¶
            label_indices = np.where(fixed_labels == label)[0]
            if len(label_indices) == 0:
                continue
                
            # è®¡ç®—å½“å‰å°èšç±»çš„å¹³å‡ç‰¹å¾
            small_cluster_features = np.mean(X[label_indices], axis=0)
            
            min_feature_distance = float('inf')
            closest_label = None
            
            for other_label in unique_labels:
                if other_label != label and np.sum(fixed_labels == other_label) >= min_cluster_size:
                    other_indices = np.where(fixed_labels == other_label)[0]
                    other_features = np.mean(X[other_indices], axis=0)
                    
                    # ä½¿ç”¨ç‰¹å¾ç›¸ä¼¼æ€§è€Œä¸æ˜¯ç©ºé—´è·ç¦»
                    feature_distance = np.linalg.norm(small_cluster_features - other_features)
                    if feature_distance < min_feature_distance:
                        min_feature_distance = feature_distance
                        closest_label = other_label
            
            if closest_label is not None:
                fixed_labels[fixed_labels == label] = closest_label
                print(f"    åˆå¹¶å­¤ç«‹ç‚¹èšç±» {label} åˆ°ç‰¹å¾ç›¸ä¼¼çš„èšç±» {closest_label} (å¤§å°: {count})")
    
    # é‡æ–°ç¼–å·æ ‡ç­¾ä»¥ç¡®ä¿è¿ç»­æ€§
    unique_labels = np.unique(fixed_labels)
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
    final_labels = np.array([label_mapping[label] for label in fixed_labels])
    
    if isolated_count > 0:
        print(f"  âœ“ ä¿®å¤äº† {isolated_count} ä¸ªå­¤ç«‹ç‚¹ï¼Œæœ€ç»ˆèšç±»æ•°: {len(np.unique(final_labels))}")
    
    return final_labels


def perform_spatial_clustering(shap_features, coords_df, n_clusters=3, grid_disk_k=1):
    """
    ä½¿ç”¨AgglomerativeClusteringæ‰§è¡Œç©ºé—´çº¦æŸèšç±»
    
    å‚æ•°:
    - shap_features: DataFrameæ ¼å¼çš„SHAPç‰¹å¾
    - coords_df: åŒ…å«latitude, longitudeå’Œå¯é€‰h3_indexçš„DataFrame
    - n_clusters: èšç±»æ•°é‡
    - grid_disk_k: H3 grid_diskåŠå¾„ï¼ˆé‚»å±…æ•°ï¼‰
    
    è¿”å›:
    - labels: èšç±»æ ‡ç­¾æ•°ç»„
    - X: æ ‡å‡†åŒ–å¹¶åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
    """
    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨KMeans
    import os
    force_kmeans = os.getenv('FORCE_KMEANS_CLUSTERING', 'false').lower() == 'true'
    
    if force_kmeans:
        print(f"ğŸ¯ å¼ºåˆ¶ä½¿ç”¨æ™®é€šKMeansèšç±»(n_clusters={n_clusters})ï¼Œè·³è¿‡ç©ºé—´çº¦æŸ")
    else:
        print(f"å¯¹SHAPç‰¹å¾è¿›è¡Œç©ºé—´çº¦æŸèšç±»(n_clusters={n_clusters}, grid_disk_k={grid_disk_k})")
    
    try:
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿shap_featuresæ˜¯DataFrame
        if isinstance(shap_features, np.ndarray):
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºDataFrame
            if hasattr(coords_df, 'columns') and len(coords_df.columns) > 3:
                # å°è¯•ä»coords_dfæ¨æ–­ç‰¹å¾å
                feature_cols = [col for col in coords_df.columns if col not in ['h3_index', 'latitude', 'longitude']]
                if len(feature_cols) == shap_features.shape[1]:
                    shap_features = pd.DataFrame(shap_features, columns=feature_cols)
                else:
                    shap_features = pd.DataFrame(shap_features, columns=[f'feature_{i}' for i in range(shap_features.shape[1])])
            else:
                shap_features = pd.DataFrame(shap_features, columns=[f'feature_{i}' for i in range(shap_features.shape[1])])
        
        # æ ‡å‡†åŒ–SHAPç‰¹å¾
        scaler = StandardScaler()
        X = scaler.fit_transform(shap_features)
        
        # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®å¹³å‡ç»å¯¹SHAPå€¼è®¡ç®—ç‰¹å¾é‡è¦æ€§æƒé‡
        if hasattr(shap_features, 'values'):
            # pandas DataFrame
            importance = np.abs(shap_features).mean(axis=0).values
        else:
            # numpy array
            importance = np.abs(shap_features).mean(axis=0)
        
        # å½’ä¸€åŒ–æœ€å¤§å€¼ä¸º1
        if importance.max() > 0:
            importance_norm = importance / importance.max()
        else:
            importance_norm = np.ones_like(importance)
        
        # å¯¹æ ‡å‡†åŒ–åçš„ç‰¹å¾æŒ‰åˆ—åŠ æƒï¼Œä½¿å¹³å‡SHAPå€¼æ›´é«˜çš„ç‰¹å¾åœ¨è·ç¦»è®¡ç®—ä¸­æ›´é‡è¦
        X = X * importance_norm
        
        # ğŸ”¥ å¦‚æœå¼ºåˆ¶ä½¿ç”¨KMeansï¼Œç›´æ¥è·³åˆ°KMeansåˆ†æ”¯
        if force_kmeans:
            print("ğŸš€ ç›´æ¥ä½¿ç”¨KMeansèšç±»ï¼ŒåŸºäºSHAPå€¼è‡ªç„¶åˆ†å¸ƒ")
            try:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = kmeans.fit_predict(X)
                print(f"âœ… KMeansèšç±»å®Œæˆï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
                
                # è¾“å‡ºèšç±»åˆ†å¸ƒç»Ÿè®¡
                unique_labels, counts = np.unique(labels, return_counts=True)
                for i, (label, count) in enumerate(zip(unique_labels, counts)):
                    percentage = count / len(labels) * 100
                    print(f"  èšç±»{label}: {count}ä¸ªç½‘æ ¼ ({percentage:.1f}%)")
                
                return labels, X
                
            except Exception as e:
                print(f"âŒ KMeansèšç±»å¤±è´¥: {e}")
                # ç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ä»½
                pass
        
        # æ„å»ºç©ºé—´æƒé‡ï¼ŒåŸºäºåæ ‡
        coords = coords_df[['latitude', 'longitude']].values
        h3_indices = coords_df['h3_index'].values if 'h3_index' in coords_df.columns else None
        
        try:
            # ğŸ”¥ æ ¹æ®grid_disk_kç›´æ¥è®¾ç½®é‚»å±…æ•°é‡ï¼Œä¸å¼ºåˆ¶æœ€å°å€¼
            if grid_disk_k == 1:
                enhanced_k = 6   # res7: æœ€å°ç©ºé—´çº¦æŸ
            elif grid_disk_k == 2:
                enhanced_k = 12  # res6: ä¸­ç­‰ç©ºé—´çº¦æŸ
            else:  # grid_disk_k == 3
                enhanced_k = 18  # res5: è¾ƒå¼ºç©ºé—´çº¦æŸ
            print(f"  æ ¹æ®grid_disk_k={grid_disk_k}ä½¿ç”¨ç©ºé—´é‚»å±…æ•°ï¼š{enhanced_k}")
            w = create_optimized_spatial_weights(coords, k=enhanced_k, h3_indices=h3_indices)
            conn = w.sparse
            print(f"  åˆ›å»ºç©ºé—´æƒé‡æˆåŠŸï¼Œå¹³å‡é‚»å±…æ•°: {w.mean_neighbors:.1f}")
        except Exception as e:
            print(f"åˆ›å»ºç©ºé—´æƒé‡å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ— çº¦æŸèšç±»")
            conn = None
        
        # ğŸ”§ ä½¿ç”¨æ ‡å‡†ç©ºé—´çº¦æŸèšç±»
        print("å¼€å§‹æ‰§è¡Œæ ‡å‡†ç©ºé—´çº¦æŸå±‚æ¬¡èšç±»...")
        if conn is not None:
            try:
                # ğŸ¯ ä½¿ç”¨æ ‡å‡†ward linkage
                model = AgglomerativeClustering(
                    n_clusters=n_clusters, 
                    connectivity=conn, 
                    linkage='ward'  # ä½¿ç”¨æ ‡å‡†ward linkage
                )
                labels = model.fit_predict(X)
                print(f"æ ‡å‡†ç©ºé—´çº¦æŸèšç±»å®Œæˆï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
                
                # ğŸ”¥ ä»…ä¿®å¤æ˜æ˜¾çš„å­¤ç«‹ç‚¹ï¼Œä¿æŒè‡ªç„¶åˆ†å¸ƒ
                labels = fix_spatial_discontinuity(labels, coords, w, X)
                
            except Exception as e:
                print(f"ç©ºé—´çº¦æŸèšç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨æ— çº¦æŸèšç±»")
                try:
                    # å›é€€åˆ°æ— çº¦æŸèšç±»
                    model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                    labels = model.fit_predict(X)
                    print(f"æ— çº¦æŸèšç±»å®Œæˆï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
                    
                    # ğŸ”¥ ä»…ä¿®å¤æ˜æ˜¾çš„å­¤ç«‹ç‚¹
                    if w is not None:
                        labels = fix_spatial_discontinuity(labels, coords, w, X)
                    
                except Exception as e2:
                    print(f"æ— çº¦æŸèšç±»ä¹Ÿå¤±è´¥: {e2}ï¼Œä½¿ç”¨KMeans")
                    model = KMeans(n_clusters=n_clusters, random_state=42)
                    labels = model.fit_predict(X)
                    print(f"KMeansèšç±»å®Œæˆï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
        else:
            model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
            labels = model.fit_predict(X)
            print(f"æ— ç©ºé—´çº¦æŸèšç±»å®Œæˆï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
            
            # ğŸ”¥ ä»…åœ¨æœ‰ç©ºé—´æƒé‡æ—¶è½»åº¦ä¿®å¤å­¤ç«‹ç‚¹
            if 'w' in locals() and w is not None:
                labels = fix_spatial_discontinuity(labels, coords, w, X)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºèšç±»
        unique_labels = np.unique(labels)
        if len(unique_labels) < n_clusters:
            print(f"è­¦å‘Š: èšç±»æ•°é‡({len(unique_labels)})å°äºç›®æ ‡æ•°é‡({n_clusters})ï¼Œå°†å°è¯•é‡æ–°å¹³è¡¡èšç±»")
            # é‡æ–°åˆ†é…æœ€å¤§èšç±»çš„éƒ¨åˆ†ç‚¹ä»¥è¾¾åˆ°ç›®æ ‡èšç±»æ•°
            counts = np.bincount(labels)
            largest_cluster = np.argmax(counts)
            largest_indices = np.where(labels == largest_cluster)[0]
            
            if len(largest_indices) > n_clusters:
                # ä»å¤§ç°‡ä¸­æ‹†åˆ†å‡ºç‚¹åˆ›å»ºæ–°ç°‡
                missing_clusters = n_clusters - len(unique_labels)
                points_per_new_cluster = len(largest_indices) // (missing_clusters + 1)
                
                # ä½¿ç”¨KMeansåœ¨å¤§ç°‡å†…éƒ¨å†æ¬¡èšç±»
                sub_X = X[largest_indices]
                kmeans = KMeans(n_clusters=missing_clusters+1, random_state=42)
                sub_labels = kmeans.fit_predict(sub_X)
                
                # é‡æ–°åˆ†é…æ ‡ç­¾
                new_labels = labels.copy()
                for i, new_cluster_id in enumerate(range(len(unique_labels), n_clusters)):
                    new_labels[largest_indices[sub_labels == i]] = new_cluster_id
                    
                labels = new_labels
                print(f"èšç±»é‡æ–°å¹³è¡¡å®Œæˆï¼Œç°æœ‰{len(np.unique(labels))}ä¸ªèšç±»")
    
    except Exception as e:
        print(f"ç©ºé—´èšç±»è¿‡ç¨‹ä¸­å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨ç®€å•KMeans")
        # ä½¿ç”¨ç®€å•KMeansä½œä¸ºåå¤‡æ–¹æ¡ˆ
        try:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹å…¼å®¹
            if isinstance(shap_features, np.ndarray):
                X = StandardScaler().fit_transform(shap_features)
            else:
                X = StandardScaler().fit_transform(shap_features.values if hasattr(shap_features, 'values') else shap_features)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            labels = kmeans.fit_predict(X)
            print(f"ä½¿ç”¨KMeanså®Œæˆèšç±»ï¼Œå…±ç”Ÿæˆ{len(np.unique(labels))}ä¸ªèšç±»")
            
            # ğŸ”¥ ä¸ºKMeansèšç±»åˆ›å»ºç©ºé—´æƒé‡å¹¶è½»åº¦ä¿®å¤å­¤ç«‹ç‚¹
            try:
                w_backup = create_optimized_spatial_weights(coords, k=8, h3_indices=h3_indices if 'h3_indices' in locals() else None)
                labels = fix_spatial_discontinuity(labels, coords, w_backup, X)
                print("KMeansèšç±»è½»åº¦ä¿®å¤å®Œæˆ")
            except Exception as e3:
                print(f"KMeansåå¤„ç†å¤±è´¥: {e3}")
        except Exception as e2:
            print(f"KMeansèšç±»ä¹Ÿå¤±è´¥: {e2}ï¼Œä½¿ç”¨éšæœºåˆ†é…")
            # æœ€åçš„åå¤‡æ–¹æ¡ˆï¼šéšæœºåˆ†é…æ ‡ç­¾
            labels = np.random.randint(0, n_clusters, size=len(shap_features))
            X = StandardScaler().fit_transform(shap_features.values if hasattr(shap_features, 'values') else shap_features)
            print("ä½¿ç”¨éšæœºåˆ†é…å®Œæˆèšç±»")
            
    return labels, X


__all__ = [
    'create_optimized_spatial_weights',
    'create_subgraph', 
    'spatial_constrained_postprocessing',
    '_solve_with_timeout',
    '_perform_clustering',
    'perform_spatial_clustering',
    'force_large_continuous_regions',
    'fix_spatial_discontinuity'
] 